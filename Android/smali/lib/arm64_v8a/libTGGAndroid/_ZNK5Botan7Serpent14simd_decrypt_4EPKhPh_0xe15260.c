// 函数: _ZNK5Botan7Serpent14simd_decrypt_4EPKhPh
// 地址: 0xe15260
// 来自: E:\torrent\Cursor\Dominion_1.0.3315\split_config.arm64_v8a\lib\arm64-v8a\libTGGAndroid.so

int32_t* x8_1 = *(arg1 + 8)
uint128_t v2 = *(arg2 + 0x10)
uint128_t v0 = *(arg2 + 0x20)
uint128_t v3 = *(arg2 + 0x30)
uint128_t v1 = *arg2
uint128_t v5 = vzip1q_s32(v2, v3)
uint128_t v2_1 = vzip2q_s32(v2, v3)
int32_t temp0 = x8_1[0x80]
v3.d = temp0
v3:4.d = temp0
v3:8.d = temp0
v3:0xc.d = temp0
int32_t temp0_1 = x8_1[0x81]
int128_t v6
v6.d = temp0_1
v6:4.d = temp0_1
v6:8.d = temp0_1
v6:0xc.d = temp0_1
uint128_t v4 = vzip1q_s32(v1, v0)
uint128_t v0_1 = vzip2q_s32(v1, v0)
int32_t temp0_2 = x8_1[0x82]
v1.d = temp0_2
v1:4.d = temp0_2
v1:8.d = temp0_2
v1:0xc.d = temp0_2
int32_t temp0_3 = x8_1[0x83]
int128_t v7
v7.d = temp0_3
v7:4.d = temp0_3
v7:8.d = temp0_3
v7:0xc.d = temp0_3
uint128_t v16 = vzip1q_s32(v4, v5)
uint128_t v4_1 = vzip2q_s32(v4, v5)
uint128_t v5_1 = vzip1q_s32(v0_1, v2_1)
uint128_t v2_2 = vzip2q_s32(v0_1, v2_1)
int32_t temp0_4 = x8_1[0x7c]
int128_t v17
v17.d = temp0_4
v17:4.d = temp0_4
v17:8.d = temp0_4
v17:0xc.d = temp0_4
uint128_t v3_1 = v3 ^ v16
uint128_t v4_2 = v6 ^ v4_1
int32_t temp0_5 = x8_1[0x7e]
v6.d = temp0_5
v6:4.d = temp0_5
v6:8.d = temp0_5
v6:0xc.d = temp0_5
uint128_t v1_1 = v1 ^ v5_1
uint128_t v2_3 = v7 ^ v2_2
int32_t temp0_6 = x8_1[0x7d]
v0_1.d = temp0_6
v0_1:4.d = temp0_6
v0_1:8.d = temp0_6
v0_1:0xc.d = temp0_6
uint128_t v5_2 = v2_3 & v3_1
uint128_t v7_1 = vorrq_s8(v2_3, v1_1)
uint128_t v1_2 = v3_1 ^ v1_1
uint128_t v4_3 = vorrq_s8(v5_2, v4_2)
uint128_t v1_3 = v7_1 & not.o(v1_2)
int32_t temp0_7 = x8_1[0x7f]
v16.d = temp0_7
v16:4.d = temp0_7
v16:8.d = temp0_7
v16:0xc.d = temp0_7
uint128_t v3_3 = v5_2 ^ not.o(v1_2)
uint128_t v4_4 = v1_3 ^ v4_3
uint128_t v2_5 = (v2_3 ^ v4_2) & v7_1
uint128_t v5_3 = v3_3 ^ v1_3
uint128_t v7_2 = v4_4 ^ v7_1
uint128_t v4_5 = v4_4 ^ v6
uint128_t v17_1 = vorrq_s8(v3_3, v1_3) ^ v2_5
uint128_t v6_3 = v2_5 ^ v17 ^ v5_3 ^ v7_2
uint128_t v7_3 = vorrq_s8(v17_1, v7_2)
uint128_t v16_1 = v17_1 ^ v0_1
v0_1.d = v4_5.d u>> 0x16
v0_1:4.d = v4_5:4.d u>> 0x16
v0_1:8.d = v4_5:8.d u>> 0x16
v0_1:0xc.d = v4_5:0xc.d u>> 0x16
v4_5.d <<= 0xa
v4_5:4.d <<= 0xa
v4_5:8.d <<= 0xa
v4_5:0xc.d <<= 0xa
uint128_t v0_2 = vorrq_s8(v4_5, v0_1)
uint128_t v4_6 = v16 ^ v5_3 ^ v7_3
uint128_t v5_4
v5_4.d = v6_3.d u>> 5
v5_4:4.d = v6_3:4.d u>> 5
v5_4:8.d = v6_3:8.d u>> 5
v5_4:0xc.d = v6_3:0xc.d u>> 5
v6_3.d <<= 0x1b
v6_3:4.d <<= 0x1b
v6_3:8.d <<= 0x1b
v6_3:0xc.d <<= 0x1b
v7_3.d = v16_1.d << 7
v7_3:4.d = v16_1:4.d << 7
v7_3:8.d = v16_1:8.d << 7
v7_3:0xc.d = v16_1:0xc.d << 7
uint128_t v5_5 = vorrq_s8(v6_3, v5_4)
v6_3.d = v16_1.d u>> 1
v6_3:4.d = v16_1:4.d u>> 1
v6_3:8.d = v16_1:8.d u>> 1
v6_3:0xc.d = v16_1:0xc.d u>> 1
uint128_t v7_4 = v7_3 ^ v0_2
v0_2.d = v16_1.d << 0x1f
v0_2:4.d = v16_1:4.d << 0x1f
v0_2:8.d = v16_1:8.d << 0x1f
v0_2:0xc.d = v16_1:0xc.d << 0x1f
uint128_t v5_6 = v4_6 ^ v16_1 ^ v5_5
uint128_t v16_2
v16_2.d = v4_6.d u>> 7
v16_2:4.d = v4_6:4.d u>> 7
v16_2:8.d = v4_6:8.d u>> 7
v16_2:0xc.d = v4_6:0xc.d u>> 7
uint128_t v7_5 = v7_4 ^ v4_6
v4_6.d <<= 0x19
v4_6:4.d <<= 0x19
v4_6:8.d <<= 0x19
v4_6:0xc.d <<= 0x19
uint128_t v6_4 = vorrq_s8(v0_2, v6_3)
uint128_t v4_7 = vorrq_s8(v4_6, v16_2)
v16_2.d = v5_6.d << 3
v16_2:4.d = v5_6:4.d << 3
v16_2:8.d = v5_6:8.d << 3
v16_2:0xc.d = v5_6:0xc.d << 3
int32_t temp0_8 = x8_1[0x79]
v1_3.d = temp0_8
v1_3:4.d = temp0_8
v1_3:8.d = temp0_8
v1_3:0xc.d = temp0_8
uint128_t v4_9 = v7_5 ^ v4_7 ^ v16_2
v16_2.d = v7_5.d u>> 3
v16_2:4.d = v7_5:4.d u>> 3
v16_2:8.d = v7_5:8.d u>> 3
v16_2:0xc.d = v7_5:0xc.d u>> 3
uint128_t v6_6 = v5_6 ^ v6_4 ^ v7_5
v7_5.d <<= 0x1d
v7_5:4.d <<= 0x1d
v7_5:8.d <<= 0x1d
v7_5:0xc.d <<= 0x1d
int32_t temp0_9 = x8_1[0x7a]
uint128_t v3_4
v3_4.d = temp0_9
v3_4:4.d = temp0_9
v3_4:8.d = temp0_9
v3_4:0xc.d = temp0_9
uint128_t v7_6 = vorrq_s8(v7_5, v16_2)
v16_2.d = v5_6.d u>> 0xd
v16_2:4.d = v5_6:4.d u>> 0xd
v16_2:8.d = v5_6:8.d u>> 0xd
v16_2:0xc.d = v5_6:0xc.d u>> 0xd
v5_6.d <<= 0x13
v5_6:4.d <<= 0x13
v5_6:8.d <<= 0x13
v5_6:0xc.d <<= 0x13
uint128_t v5_8 = v7_6 ^ vorrq_s8(v5_6, v16_2)
uint128_t v4_10 = v4_9 ^ v6_6
uint128_t v16_4 = vorrq_s8(v4_9 ^ v7_6, v5_8)
uint128_t v7_8 = (v5_8 & v7_6) ^ v4_10
int32_t temp0_10 = x8_1[0x7b]
v2_5.d = temp0_10
v2_5:4.d = temp0_10
v2_5:8.d = temp0_10
v2_5:0xc.d = temp0_10
uint128_t v4_11 = v16_4 ^ v4_10
uint128_t v16_5 = not.o(v7_8)
uint128_t v5_9 = v5_8 ^ v16_5
uint128_t v1_4 = v1_3 ^ v16_5
int32_t temp0_11 = x8_1[0x78]
v16_5.d = temp0_11
v16_5:4.d = temp0_11
v16_5:8.d = temp0_11
v16_5:0xc.d = temp0_11
uint128_t v6_8 = (v4_11 & v6_6) ^ v5_9
uint128_t v4_13 = (v4_11 ^ v5_9) | not.o(v7_8)
v7_8.d = v1_4.d u>> 1
v7_8:4.d = v1_4:4.d u>> 1
v7_8:8.d = v1_4:8.d u>> 1
v7_8:0xc.d = v1_4:0xc.d u>> 1
uint128_t v16_6 = v6_8 ^ v16_5
uint128_t v2_7 = v4_11 ^ v2_5 ^ v6_8
v6_8.d = v1_4.d << 0x1f
v6_8:4.d = v1_4:4.d << 0x1f
v6_8:8.d = v1_4:8.d << 0x1f
v6_8:0xc.d = v1_4:0xc.d << 0x1f
uint128_t v6_9 = vorrq_s8(v6_8, v7_8)
v7_8.d = v1_4.d << 7
v7_8:4.d = v1_4:4.d << 7
v7_8:8.d = v1_4:8.d << 7
v7_8:0xc.d = v1_4:0xc.d << 7
uint128_t v3_7 = v6_6 ^ v3_4 ^ v16_4 ^ v4_13
v4_13.d = v16_6.d u>> 5
v4_13:4.d = v16_6:4.d u>> 5
v4_13:8.d = v16_6:8.d u>> 5
v4_13:0xc.d = v16_6:0xc.d u>> 5
v16_6.d <<= 0x1b
v16_6:4.d <<= 0x1b
v16_6:8.d <<= 0x1b
v16_6:0xc.d <<= 0x1b
uint128_t v7_9 = v2_7 ^ v7_8
uint128_t v1_5 = v2_7 ^ v1_4
uint128_t v4_14 = vorrq_s8(v16_6, v4_13)
v16_6.d = v2_7.d u>> 7
v16_6:4.d = v2_7:4.d u>> 7
v16_6:8.d = v2_7:8.d u>> 7
v16_6:0xc.d = v2_7:0xc.d u>> 7
v2_7.d <<= 0x19
v2_7:4.d <<= 0x19
v2_7:8.d <<= 0x19
v2_7:0xc.d <<= 0x19
uint128_t v2_8 = vorrq_s8(v2_7, v16_6)
v16_6.d = v3_7.d u>> 0x16
v16_6:4.d = v3_7:4.d u>> 0x16
v16_6:8.d = v3_7:8.d u>> 0x16
v16_6:0xc.d = v3_7:0xc.d u>> 0x16
v3_7.d <<= 0xa
v3_7:4.d <<= 0xa
v3_7:8.d <<= 0xa
v3_7:0xc.d <<= 0xa
uint128_t v1_6 = v1_5 ^ v4_14
uint128_t v3_9 = v7_9 ^ vorrq_s8(v3_7, v16_6)
v4_14.d = v1_6.d << 3
v4_14:4.d = v1_6:4.d << 3
v4_14:8.d = v1_6:8.d << 3
v4_14:0xc.d = v1_6:0xc.d << 3
v7_9.d = v1_6.d u>> 0xd
v7_9:4.d = v1_6:4.d u>> 0xd
v7_9:8.d = v1_6:8.d u>> 0xd
v7_9:0xc.d = v1_6:0xc.d u>> 0xd
v16_6.d = v1_6.d << 0x13
v16_6:4.d = v1_6:4.d << 0x13
v16_6:8.d = v1_6:8.d << 0x13
v16_6:0xc.d = v1_6:0xc.d << 0x13
uint128_t v1_7 = v6_9 ^ v1_6
v6_9.d = v3_9.d u>> 3
v6_9:4.d = v3_9:4.d u>> 3
v6_9:8.d = v3_9:8.d u>> 3
v6_9:0xc.d = v3_9:0xc.d u>> 3
uint128_t v7_10 = vorrq_s8(v16_6, v7_9)
v16_6.d = v3_9.d << 0x1d
v16_6:4.d = v3_9:4.d << 0x1d
v16_6:8.d = v3_9:8.d << 0x1d
v16_6:0xc.d = v3_9:0xc.d << 0x1d
uint128_t v1_8 = v1_7 ^ v3_9
uint128_t v2_10 = v3_9 ^ v2_8 ^ v4_14
uint128_t v3_10 = vorrq_s8(v16_6, v6_9)
uint128_t v4_15 = not.o(v1_8)
uint128_t v16_7 = vorrq_s8(v2_10, v7_10)
int32_t temp0_12 = x8_1[0x76]
v5_9.d = temp0_12
v5_9:4.d = temp0_12
v5_9:8.d = temp0_12
v5_9:0xc.d = temp0_12
uint128_t v6_11 = v3_10 ^ v4_15 ^ v16_7
v0_2.d = x8_1[0x75]
uint128_t v2_11 = v6_11 ^ v2_10
int32_t temp0_13 = x8_1[0x74]
v16_7.d = temp0_13
v16_7:4.d = temp0_13
v16_7:8.d = temp0_13
v16_7:0xc.d = temp0_13
int128_t v3_13 = v2_11 ^ ((v3_10 | not.o(v1_8)) & v7_10)
uint128_t v2_13 = vorrq_s8(v2_11, v7_10) ^ v4_15
uint128_t v5_10 = v7_10 ^ v5_9
int32_t temp0_14 = x8_1[0x77]
v7_10.d = temp0_14
v7_10:4.d = temp0_14
v7_10:8.d = temp0_14
v7_10:0xc.d = temp0_14
int128_t v2_14 = v2_13 ^ v3_13
int128_t v1_10 = (v3_13 & not.o(v1_8)) ^ v6_11
int128_t v2_15 = v1_10 ^ v2_14
uint128_t v1_11 = v1_10 ^ v16_7
int128_t v0_5 = v2_15 ^ vdupq_laneq_s32(not.o(v0_2), 0)
int128_t v2_16 = v5_10 ^ (v2_14 & v6_11) ^ v2_15
int128_t v5_11
v5_11.d = v1_11.d u>> 5
v5_11:4.d = v1_11:4.d u>> 5
v5_11:8.d = v1_11:8.d u>> 5
v5_11:0xc.d = v1_11:0xc.d u>> 5
v1_11.d <<= 0x1b
v1_11:4.d <<= 0x1b
v1_11:8.d <<= 0x1b
v1_11:0xc.d <<= 0x1b
int128_t v7_11 = v3_13 ^ v7_10
uint128_t v1_12 = vorrq_s8(v1_11, v5_11)
v5_11.d = v0_5.d << 7
v5_11:4.d = v0_5:4.d << 7
v5_11:8.d = v0_5:8.d << 7
v5_11:0xc.d = v0_5:0xc.d << 7
v3_13.d = v7_11.d u>> 7
v3_13:4.d = v7_11:4.d u>> 7
v3_13:8.d = v7_11:8.d u>> 7
v3_13:0xc.d = v7_11:0xc.d u>> 7
v16_7.d = v7_11.d << 0x19
v16_7:4.d = v7_11:4.d << 0x19
v16_7:8.d = v7_11:8.d << 0x19
v16_7:0xc.d = v7_11:0xc.d << 0x19
uint128_t v1_13 = v0_5 ^ v7_11 ^ v1_12
uint128_t v7_12
v7_12.d = v0_5.d u>> 1
v7_12:4.d = v0_5:4.d u>> 1
v7_12:8.d = v0_5:8.d u>> 1
v7_12:0xc.d = v0_5:0xc.d u>> 1
v0_5.d <<= 0x1f
v0_5:4.d <<= 0x1f
v0_5:8.d <<= 0x1f
v0_5:0xc.d <<= 0x1f
uint128_t v0_6 = vorrq_s8(v0_5, v7_12)
v7_12.d = v2_16.d u>> 0x16
v7_12:4.d = v2_16:4.d u>> 0x16
v7_12:8.d = v2_16:8.d u>> 0x16
v7_12:0xc.d = v2_16:0xc.d u>> 0x16
v2_16.d <<= 0xa
v2_16:4.d <<= 0xa
v2_16:8.d <<= 0xa
v2_16:0xc.d <<= 0xa
uint128_t v3_14 = vorrq_s8(v16_7, v3_13)
uint128_t v2_17 = vorrq_s8(v2_16, v7_12)
v7_12.d = v1_13.d << 3
v7_12:4.d = v1_13:4.d << 3
v7_12:8.d = v1_13:8.d << 3
v7_12:0xc.d = v1_13:0xc.d << 3
uint128_t v0_7 = v1_13 ^ v0_6
uint128_t v2_18 = v5_11 ^ v7_11 ^ v2_17
int128_t v5_12
v5_12.d = v1_13.d u>> 0xd
v5_12:4.d = v1_13:4.d u>> 0xd
v5_12:8.d = v1_13:8.d u>> 0xd
v5_12:0xc.d = v1_13:0xc.d u>> 0xd
v1_13.d <<= 0x13
v1_13:4.d <<= 0x13
v1_13:8.d <<= 0x13
v1_13:0xc.d <<= 0x13
uint128_t v1_14 = vorrq_s8(v1_13, v5_12)
uint128_t v3_16 = v7_12 ^ v3_14 ^ v2_18
uint128_t v0_8 = v0_7 ^ v2_18
v5_12.d = v2_18.d u>> 3
v5_12:4.d = v2_18:4.d u>> 3
v5_12:8.d = v2_18:8.d u>> 3
v5_12:0xc.d = v2_18:0xc.d u>> 3
v2_18.d <<= 0x1d
v2_18:4.d <<= 0x1d
v2_18:8.d <<= 0x1d
v2_18:0xc.d <<= 0x1d
uint128_t v2_19 = vorrq_s8(v2_18, v5_12)
int32_t temp0_15 = x8_1[0x70]
v4_15.d = temp0_15
v4_15:4.d = temp0_15
v4_15:8.d = temp0_15
v4_15:0xc.d = temp0_15
int32_t temp0_16 = x8_1[0x72]
v16_7.d = temp0_16
v16_7:4.d = temp0_16
v16_7:8.d = temp0_16
v16_7:0xc.d = temp0_16
uint128_t v5_14 = (v3_16 & v2_19) ^ v0_8
uint128_t v0_9 = vorrq_s8(v3_16, v0_8)
int32_t temp0_17 = x8_1[0x73]
int128_t v6_12
v6_12.d = temp0_17
v6_12:4.d = temp0_17
v6_12:8.d = temp0_17
v6_12:0xc.d = temp0_17
uint128_t v0_12 = (v0_9 & v1_14) ^ v2_19 ^ v5_14
uint128_t v3_17 = v0_12 ^ v3_16
int128_t v6_13 = v0_12 ^ v6_12
uint128_t v2_21 = v3_17 ^ (v5_14 & v1_14)
uint128_t v1_15 = v3_17 & not.o(v1_14)
v3_17.d = v6_13.d u>> 7
v3_17:4.d = v6_13:4.d u>> 7
v3_17:8.d = v6_13:8.d u>> 7
v3_17:0xc.d = v6_13:0xc.d u>> 7
uint128_t v7_14 = v2_21 ^ not.o(v1_14)
uint128_t v2_22 = v2_21 ^ v16_7
v16_7.d = v6_13.d << 0x19
v16_7:4.d = v6_13:4.d << 0x19
v16_7:8.d = v6_13:8.d << 0x19
v16_7:0xc.d = v6_13:0xc.d << 0x19
uint128_t v3_18 = vorrq_s8(v16_7, v3_17)
int32_t temp0_18 = x8_1[0x71]
v16_7.d = temp0_18
v16_7:4.d = temp0_18
v16_7:8.d = temp0_18
v16_7:0xc.d = temp0_18
uint128_t v1_16 = v1_15 ^ v5_14
uint128_t v4_16 = v7_14 ^ v4_15
uint128_t v1_17 = v1_16 ^ v16_7
uint128_t v5_15
v5_15.d = v4_16.d u>> 5
v5_15:4.d = v4_16:4.d u>> 5
v5_15:8.d = v4_16:8.d u>> 5
v5_15:0xc.d = v4_16:0xc.d u>> 5
v4_16.d <<= 0x1b
v4_16:4.d <<= 0x1b
v4_16:8.d <<= 0x1b
v4_16:0xc.d <<= 0x1b
uint128_t v7_15
v7_15.d = v1_17.d << 7
v7_15:4.d = v1_17:4.d << 7
v7_15:8.d = v1_17:8.d << 7
v7_15:0xc.d = v1_17:0xc.d << 7
uint128_t v5_16 = v1_17 ^ v6_13
uint128_t v0_15 = v2_22 ^ vorrq_s8((v7_14 & v5_14) ^ v0_12, v7_14 ^ v1_16)
int128_t v6_14 = v7_15 ^ v6_13
v7_15.d = v1_17.d u>> 1
v7_15:4.d = v1_17:4.d u>> 1
v7_15:8.d = v1_17:8.d u>> 1
v7_15:0xc.d = v1_17:0xc.d u>> 1
v1_17.d <<= 0x1f
v1_17:4.d <<= 0x1f
v1_17:8.d <<= 0x1f
v1_17:0xc.d <<= 0x1f
uint128_t v4_18 = v5_16 ^ vorrq_s8(v4_16, v5_15)
v5_16.d = v0_15.d u>> 0x16
v5_16:4.d = v0_15:4.d u>> 0x16
v5_16:8.d = v0_15:8.d u>> 0x16
v5_16:0xc.d = v0_15:0xc.d u>> 0x16
v0_15.d <<= 0xa
v0_15:4.d <<= 0xa
v0_15:8.d <<= 0xa
v0_15:0xc.d <<= 0xa
uint128_t v1_18 = vorrq_s8(v1_17, v7_15)
uint128_t v0_16 = vorrq_s8(v0_15, v5_16)
v5_16.d = v4_18.d << 3
v5_16:4.d = v4_18:4.d << 3
v5_16:8.d = v4_18:8.d << 3
v5_16:0xc.d = v4_18:0xc.d << 3
uint128_t v1_19 = v4_18 ^ v1_18
uint128_t v3_19 = v5_16 ^ v3_18
v5_16.d = v4_18.d u>> 0xd
v5_16:4.d = v4_18:4.d u>> 0xd
v5_16:8.d = v4_18:8.d u>> 0xd
v5_16:0xc.d = v4_18:0xc.d u>> 0xd
v4_18.d <<= 0x13
v4_18:4.d <<= 0x13
v4_18:8.d <<= 0x13
v4_18:0xc.d <<= 0x13
uint128_t v0_17 = v6_14 ^ v0_16
uint128_t v4_19 = vorrq_s8(v4_18, v5_16)
uint128_t v3_20 = v3_19 ^ v0_17
uint128_t v1_20 = v1_19 ^ v0_17
v6_14.d = v0_17.d u>> 3
v6_14:4.d = v0_17:4.d u>> 3
v6_14:8.d = v0_17:8.d u>> 3
v6_14:0xc.d = v0_17:0xc.d u>> 3
v0_17.d <<= 0x1d
v0_17:4.d <<= 0x1d
v0_17:8.d <<= 0x1d
v0_17:0xc.d <<= 0x1d
uint128_t v0_18 = vorrq_s8(v0_17, v6_14)
uint128_t v6_15 = v1_20 ^ v0_18
int32_t temp0_19 = x8_1[0x6d]
v7_15.d = temp0_19
v7_15:4.d = temp0_19
v7_15:8.d = temp0_19
v7_15:0xc.d = temp0_19
uint128_t v4_20 = v6_15 ^ v4_19
uint128_t v17_2 = (v6_15 & v0_18) ^ v4_20
uint128_t v0_20 = v4_20 & v1_20
int32_t temp0_20 = x8_1[0x6f]
v1_20.d = temp0_20
v1_20:4.d = temp0_20
v1_20:8.d = temp0_20
v1_20:0xc.d = temp0_20
int32_t temp0_21 = x8_1[0x6c]
v16_7.d = temp0_21
v16_7:4.d = temp0_21
v16_7:8.d = temp0_21
v16_7:0xc.d = temp0_21
int32_t temp0_22 = x8_1[0x6e]
v2_22.d = temp0_22
v2_22:4.d = temp0_22
v2_22:8.d = temp0_22
v2_22:0xc.d = temp0_22
uint128_t v3_21 = vorrq_s8(v17_2, v3_20)
uint128_t v4_21 = v3_19 ^ v1_19 ^ v17_2
uint128_t v5_18 = v3_21 ^ v6_15
uint128_t v0_21 = v3_21 ^ v0_20
uint128_t v3_23 = (v5_18 & v3_21) ^ v4_21
uint128_t v6_16 = v5_18 ^ v16_7
uint128_t v5_20 = vorrq_s8(v0_21 ^ v4_21, v5_18) ^ v17_2
v16_7.d = v6_16.d u>> 5
v16_7:4.d = v6_16:4.d u>> 5
v16_7:8.d = v6_16:8.d u>> 5
v16_7:0xc.d = v6_16:0xc.d u>> 5
v6_16.d <<= 0x1b
v6_16:4.d <<= 0x1b
v6_16:8.d <<= 0x1b
v6_16:0xc.d <<= 0x1b
uint128_t v2_23 = v3_23 ^ v2_22
uint128_t v3_24 = vorrq_s8(v6_16, v16_7)
uint128_t v6_17 = v5_20 ^ v7_15
uint128_t v1_23 = v0_21 ^ v1_20 ^ v3_23 ^ v5_20
v5_20.d = v2_23.d u>> 0x16
v5_20:4.d = v2_23:4.d u>> 0x16
v5_20:8.d = v2_23:8.d u>> 0x16
v5_20:0xc.d = v2_23:0xc.d u>> 0x16
v2_23.d <<= 0xa
v2_23:4.d <<= 0xa
v2_23:8.d <<= 0xa
v2_23:0xc.d <<= 0xa
uint128_t v2_24 = vorrq_s8(v2_23, v5_20)
v5_20.d = v1_23.d u>> 7
v5_20:4.d = v1_23:4.d u>> 7
v5_20:8.d = v1_23:8.d u>> 7
v5_20:0xc.d = v1_23:0xc.d u>> 7
v7_15.d = v1_23.d << 0x19
v7_15:4.d = v1_23:4.d << 0x19
v7_15:8.d = v1_23:8.d << 0x19
v7_15:0xc.d = v1_23:0xc.d << 0x19
v16_7.d = v6_17.d u>> 1
v16_7:4.d = v6_17:4.d u>> 1
v16_7:8.d = v6_17:8.d u>> 1
v16_7:0xc.d = v6_17:0xc.d u>> 1
uint128_t v2_25 = v1_23 ^ v2_24
uint128_t v1_24 = v6_17 ^ v3_24 ^ v1_23
uint128_t v3_25
v3_25.d = v6_17.d << 0x1f
v3_25:4.d = v6_17:4.d << 0x1f
v3_25:8.d = v6_17:8.d << 0x1f
v3_25:0xc.d = v6_17:0xc.d << 0x1f
uint128_t v3_26 = vorrq_s8(v3_25, v16_7)
v6_17.d <<= 7
v6_17:4.d <<= 7
v6_17:8.d <<= 7
v6_17:0xc.d <<= 7
uint128_t v5_21 = vorrq_s8(v7_15, v5_20)
uint128_t v6_18 = v2_25 ^ v6_17
uint128_t v3_27 = v1_24 ^ v3_26
v7_15.d = v1_24.d u>> 0xd
v7_15:4.d = v1_24:4.d u>> 0xd
v7_15:8.d = v1_24:8.d u>> 0xd
v7_15:0xc.d = v1_24:0xc.d u>> 0xd
v16_7.d = v1_24.d << 0x13
v16_7:4.d = v1_24:4.d << 0x13
v16_7:8.d = v1_24:8.d << 0x13
v16_7:0xc.d = v1_24:0xc.d << 0x13
v1_24.d <<= 3
v1_24:4.d <<= 3
v1_24:8.d <<= 3
v1_24:0xc.d <<= 3
uint128_t v5_22 = v6_18 ^ v5_21
v2_25.d <<= 0x1d
v2_25:4.d <<= 0x1d
v2_25:8.d <<= 0x1d
v2_25:0xc.d <<= 0x1d
uint128_t v3_28 = v3_27 ^ v6_18
v6_18.d u>>= 3
v6_18:4.d u>>= 3
v6_18:8.d u>>= 3
v6_18:0xc.d u>>= 3
uint128_t v7_16 = vorrq_s8(v16_7, v7_15)
uint128_t v1_25 = v5_22 ^ v1_24
uint128_t v2_26 = vorrq_s8(v2_25, v6_18)
int32_t temp0_23 = x8_1[0x68]
v4_21.d = temp0_23
v4_21:4.d = temp0_23
v4_21:8.d = temp0_23
v4_21:0xc.d = temp0_23
int32_t temp0_24 = x8_1[0x69]
uint128_t v0_22
v0_22.d = temp0_24
v0_22:4.d = temp0_24
v0_22:8.d = temp0_24
v0_22:0xc.d = temp0_24
uint128_t v2_27 = v1_25 ^ v2_26
uint128_t v1_26 = v1_25 ^ v7_16
int32_t temp0_25 = x8_1[0x6a]
v16_7.d = temp0_25
v16_7:4.d = temp0_25
v16_7:8.d = temp0_25
v16_7:0xc.d = temp0_25
int32_t temp0_26 = x8_1[0x6b]
v5_22.d = temp0_26
v5_22:4.d = temp0_26
v5_22:8.d = temp0_26
v5_22:0xc.d = temp0_26
uint128_t v6_20 = (v2_27 & v1_26) ^ v3_28
uint128_t v3_30 = vorrq_s8(v2_27, v3_28) ^ v1_26
uint128_t v2_28 = v6_20 ^ v2_27
uint128_t v6_21 = not.o(v6_20)
uint128_t v17_3 = (v1_26 & v7_16 & v6_20) ^ v2_28
uint128_t v4_22 = v3_30 ^ v4_21
uint128_t v6_22
v6_22.d = v4_22.d u>> 5
v6_22:4.d = v4_22:4.d u>> 5
v6_22:8.d = v4_22:8.d u>> 5
v6_22:0xc.d = v4_22:0xc.d u>> 5
v4_22.d <<= 0x1b
v4_22:4.d <<= 0x1b
v4_22:8.d <<= 0x1b
v4_22:0xc.d <<= 0x1b
uint128_t v0_23 = v17_3 ^ v0_22
uint128_t v2_30 = v16_7 ^ v6_21 ^ vorrq_s8(v2_28 & v3_30, v7_16)
uint128_t v5_23
v5_23.d = v0_23.d << 7
v5_23:4.d = v0_23:4.d << 7
v5_23:8.d = v0_23:8.d << 7
v5_23:0xc.d = v0_23:0xc.d << 7
uint128_t v4_24 = v0_23 ^ vorrq_s8(v4_22, v6_22)
v16_7.d = v0_23.d u>> 1
v16_7:4.d = v0_23:4.d u>> 1
v16_7:8.d = v0_23:8.d u>> 1
v16_7:0xc.d = v0_23:0xc.d u>> 1
v0_23.d <<= 0x1f
v0_23:4.d <<= 0x1f
v0_23:8.d <<= 0x1f
v0_23:0xc.d <<= 0x1f
uint128_t v3_33 = v5_22 ^ v6_21 ^ ((v7_16 ^ v6_21) & v3_30) ^ v17_3
uint128_t v0_24 = vorrq_s8(v0_23, v16_7)
v16_7.d = v2_30.d u>> 0x16
v16_7:4.d = v2_30:4.d u>> 0x16
v16_7:8.d = v2_30:8.d u>> 0x16
v16_7:0xc.d = v2_30:0xc.d u>> 0x16
v2_30.d <<= 0xa
v2_30:4.d <<= 0xa
v2_30:8.d <<= 0xa
v2_30:0xc.d <<= 0xa
uint128_t v2_31 = vorrq_s8(v2_30, v16_7)
uint128_t v5_24 = v3_33 ^ v5_23
uint128_t v4_25 = v4_24 ^ v3_33
v16_7.d = v3_33.d u>> 7
v16_7:4.d = v3_33:4.d u>> 7
v16_7:8.d = v3_33:8.d u>> 7
v16_7:0xc.d = v3_33:0xc.d u>> 7
v3_33.d <<= 0x19
v3_33:4.d <<= 0x19
v3_33:8.d <<= 0x19
v3_33:0xc.d <<= 0x19
uint128_t v2_32 = v5_24 ^ v2_31
v5_24.d = v4_25.d << 3
v5_24:4.d = v4_25:4.d << 3
v5_24:8.d = v4_25:8.d << 3
v5_24:0xc.d = v4_25:0xc.d << 3
uint128_t v0_25 = v4_25 ^ v0_24
uint128_t v3_35 = v2_32 ^ vorrq_s8(v3_33, v16_7)
v16_7.d = v4_25.d u>> 0xd
v16_7:4.d = v4_25:4.d u>> 0xd
v16_7:8.d = v4_25:8.d u>> 0xd
v16_7:0xc.d = v4_25:0xc.d u>> 0xd
v4_25.d <<= 0x13
v4_25:4.d <<= 0x13
v4_25:8.d <<= 0x13
v4_25:0xc.d <<= 0x13
uint128_t v0_26 = v0_25 ^ v2_32
uint128_t v3_36 = v3_35 ^ v5_24
uint128_t v4_26 = vorrq_s8(v4_25, v16_7)
v16_7.d = v2_32.d u>> 3
v16_7:4.d = v2_32:4.d u>> 3
v16_7:8.d = v2_32:8.d u>> 3
v16_7:0xc.d = v2_32:0xc.d u>> 3
v2_32.d <<= 0x1d
v2_32:4.d <<= 0x1d
v2_32:8.d <<= 0x1d
v2_32:0xc.d <<= 0x1d
uint128_t v5_25 = v3_36 ^ v0_26
int32_t temp0_27 = x8_1[0x64]
uint128_t v1_29
v1_29.d = temp0_27
v1_29:4.d = temp0_27
v1_29:8.d = temp0_27
v1_29:0xc.d = temp0_27
uint128_t v2_33 = vorrq_s8(v2_32, v16_7)
int32_t temp0_28 = x8_1[0x66]
uint128_t v7_17
v7_17.d = temp0_28
v7_17:4.d = temp0_28
v7_17:8.d = temp0_28
v7_17:0xc.d = temp0_28
uint128_t v0_27 = v0_26 ^ v2_33
uint128_t v16_8 = vorrq_s8(v5_25, v4_26)
uint128_t v3_38 = (v5_25 & v3_36) ^ v4_26
int32_t temp0_29 = x8_1[0x65]
v6_22.d = temp0_29
v6_22:4.d = temp0_29
v6_22:8.d = temp0_29
v6_22:0xc.d = temp0_29
uint128_t v2_34 = v3_38 ^ v2_33
uint128_t v4_28 = vorrq_s8(v2_34, v16_8 ^ v0_27) ^ v3_38 ^ v5_25
uint128_t v5_26 = vorrq_s8(v3_38, v5_25)
uint128_t v3_39 = v3_38 ^ v7_17
int32_t temp0_30 = x8_1[0x67]
v7_17.d = temp0_30
v7_17:4.d = temp0_30
v7_17:8.d = temp0_30
v7_17:0xc.d = temp0_30
uint128_t v6_23 = v4_28 ^ v6_22
uint128_t v0_29 = v4_28 ^ v0_27 ^ v5_26
int128_t v3_40 = v3_39 ^ ((v5_26 & not.o(v4_28)) | not.o(v0_29))
uint128_t v2_35 = v2_34 ^ v7_17
uint128_t v0_31 = v1_29 ^ not.o(v0_29)
int128_t v4_30
v4_30.d = v3_40.d u>> 0x16
v4_30:4.d = v3_40:4.d u>> 0x16
v4_30:8.d = v3_40:8.d u>> 0x16
v4_30:0xc.d = v3_40:0xc.d u>> 0x16
v3_40.d <<= 0xa
v3_40:4.d <<= 0xa
v3_40:8.d <<= 0xa
v3_40:0xc.d <<= 0xa
v7_17.d = v2_35.d u>> 7
v7_17:4.d = v2_35:4.d u>> 7
v7_17:8.d = v2_35:8.d u>> 7
v7_17:0xc.d = v2_35:0xc.d u>> 7
uint128_t v16_10
v16_10.d = v2_35.d << 0x19
v16_10:4.d = v2_35:4.d << 0x19
v16_10:8.d = v2_35:8.d << 0x19
v16_10:0xc.d = v2_35:0xc.d << 0x19
uint128_t v3_41 = vorrq_s8(v3_40, v4_30)
v4_30.d = v0_31.d u>> 5
v4_30:4.d = v0_31:4.d u>> 5
v4_30:8.d = v0_31:8.d u>> 5
v4_30:0xc.d = v0_31:0xc.d u>> 5
v0_31.d <<= 0x1b
v0_31:4.d <<= 0x1b
v0_31:8.d <<= 0x1b
v0_31:0xc.d <<= 0x1b
uint128_t v7_18 = vorrq_s8(v16_10, v7_17)
v5_26.d = v6_23.d << 7
v5_26:4.d = v6_23:4.d << 7
v5_26:8.d = v6_23:8.d << 7
v5_26:0xc.d = v6_23:0xc.d << 7
uint128_t v2_36 = v5_26 ^ v2_35
uint128_t v0_33 = v6_23 ^ v2_35 ^ vorrq_s8(v0_31, v4_30)
v5_26.d = v6_23.d u>> 1
v5_26:4.d = v6_23:4.d u>> 1
v5_26:8.d = v6_23:8.d u>> 1
v5_26:0xc.d = v6_23:0xc.d u>> 1
v6_23.d <<= 0x1f
v6_23:4.d <<= 0x1f
v6_23:8.d <<= 0x1f
v6_23:0xc.d <<= 0x1f
uint128_t v2_37 = v2_36 ^ v3_41
v3_41.d = v0_33.d << 3
v3_41:4.d = v0_33:4.d << 3
v3_41:8.d = v0_33:8.d << 3
v3_41:0xc.d = v0_33:0xc.d << 3
uint128_t v16_11
v16_11.d = v2_37.d u>> 3
v16_11:4.d = v2_37:4.d u>> 3
v16_11:8.d = v2_37:8.d u>> 3
v16_11:0xc.d = v2_37:0xc.d u>> 3
uint128_t v3_42 = v3_41 ^ v7_18
v7_18.d = v2_37.d << 0x1d
v7_18:4.d = v2_37:4.d << 0x1d
v7_18:8.d = v2_37:8.d << 0x1d
v7_18:0xc.d = v2_37:0xc.d << 0x1d
uint128_t v5_28 = v0_33 ^ vorrq_s8(v6_23, v5_26)
uint128_t v7_19 = vorrq_s8(v7_18, v16_11)
v16_11.d = v0_33.d u>> 0xd
v16_11:4.d = v0_33:4.d u>> 0xd
v16_11:8.d = v0_33:8.d u>> 0xd
v16_11:0xc.d = v0_33:0xc.d u>> 0xd
v0_33.d <<= 0x13
v0_33:4.d <<= 0x13
v0_33:8.d <<= 0x13
v0_33:0xc.d <<= 0x13
uint128_t v5_29 = v5_28 ^ v2_37
uint128_t v0_34 = vorrq_s8(v0_33, v16_11)
int32_t temp0_31 = x8_1[0x60]
v6_23.d = temp0_31
v6_23:4.d = temp0_31
v6_23:8.d = temp0_31
v6_23:0xc.d = temp0_31
uint128_t v2_38 = v3_42 ^ v2_37
uint128_t v16_12 = vorrq_s8(v5_29, v0_34)
uint128_t v5_30 = not.o(v5_29)
int32_t temp0_32 = x8_1[0x62]
v4_30.d = temp0_32
v4_30:4.d = temp0_32
v4_30:8.d = temp0_32
v4_30:0xc.d = temp0_32
uint128_t v0_35 = v0_34 ^ v5_30
int32_t temp0_33 = x8_1[0x61]
v1_29.d = temp0_33
v1_29:4.d = temp0_33
v1_29:8.d = temp0_33
v1_29:0xc.d = temp0_33
uint128_t v3_45 = v2_38 ^ not.o(v7_19) ^ v16_12
uint128_t v7_21 = (v5_30 | not.o(v7_19)) ^ v0_35
uint128_t v0_36 = v0_35 & v2_38
int32_t temp0_34 = x8_1[0x63]
v16_12.d = temp0_34
v16_12:4.d = temp0_34
v16_12:8.d = temp0_34
v16_12:0xc.d = temp0_34
uint128_t v5_31 = v0_36 ^ v5_30
uint128_t v17_4 = vorrq_s8(v3_45, v0_36) ^ v7_21
uint128_t v0_38 = v3_45 ^ v4_30
uint128_t v2_40 = v5_31 ^ v2_38 ^ v3_45
uint128_t v4_31 = v17_4 ^ v6_23
uint128_t v7_22 = v3_45 ^ v7_21
v3_45.d = v0_38.d u>> 0x16
v3_45:4.d = v0_38:4.d u>> 0x16
v3_45:8.d = v0_38:8.d u>> 0x16
v3_45:0xc.d = v0_38:0xc.d u>> 0x16
v0_38.d <<= 0xa
v0_38:4.d <<= 0xa
v0_38:8.d <<= 0xa
v0_38:0xc.d <<= 0xa
uint128_t v2_41 = v2_40 ^ v17_4
v6_23.d = v4_31.d u>> 5
v6_23:4.d = v4_31:4.d u>> 5
v6_23:8.d = v4_31:8.d u>> 5
v6_23:0xc.d = v4_31:0xc.d u>> 5
v4_31.d <<= 0x1b
v4_31:4.d <<= 0x1b
v4_31:8.d <<= 0x1b
v4_31:0xc.d <<= 0x1b
uint128_t v2_42 = v2_41 ^ v16_12
uint128_t v1_31 = v5_31 ^ v1_29 ^ (v2_41 & v7_22)
uint128_t v3_47 = v2_42 ^ vorrq_s8(v0_38, v3_45)
uint128_t v4_33 = v2_42 ^ vorrq_s8(v4_31, v6_23)
v6_23.d = v2_42.d u>> 7
v6_23:4.d = v2_42:4.d u>> 7
v6_23:8.d = v2_42:8.d u>> 7
v6_23:0xc.d = v2_42:0xc.d u>> 7
v2_42.d <<= 0x19
v2_42:4.d <<= 0x19
v2_42:8.d <<= 0x19
v2_42:0xc.d <<= 0x19
uint128_t v4_34 = v4_33 ^ v1_31
uint128_t v2_43 = vorrq_s8(v2_42, v6_23)
v6_23.d = v1_31.d u>> 1
v6_23:4.d = v1_31:4.d u>> 1
v6_23:8.d = v1_31:8.d u>> 1
v6_23:0xc.d = v1_31:0xc.d u>> 1
uint128_t v7_23
v7_23.d = v1_31.d << 0x1f
v7_23:4.d = v1_31:4.d << 0x1f
v7_23:8.d = v1_31:8.d << 0x1f
v7_23:0xc.d = v1_31:0xc.d << 0x1f
v1_31.d <<= 7
v1_31:4.d <<= 7
v1_31:8.d <<= 7
v1_31:0xc.d <<= 7
uint128_t v6_24 = vorrq_s8(v7_23, v6_23)
uint128_t v1_32 = v3_47 ^ v1_31
v7_23.d = v4_34.d u>> 0xd
v7_23:4.d = v4_34:4.d u>> 0xd
v7_23:8.d = v4_34:8.d u>> 0xd
v7_23:0xc.d = v4_34:0xc.d u>> 0xd
v16_12.d = v4_34.d << 0x13
v16_12:4.d = v4_34:4.d << 0x13
v16_12:8.d = v4_34:8.d << 0x13
v16_12:0xc.d = v4_34:0xc.d << 0x13
uint128_t v6_25 = v4_34 ^ v6_24
v4_34.d <<= 3
v4_34:4.d <<= 3
v4_34:8.d <<= 3
v4_34:0xc.d <<= 3
v3_47.d <<= 0x1d
v3_47:4.d <<= 0x1d
v3_47:8.d <<= 0x1d
v3_47:0xc.d <<= 0x1d
uint128_t v2_45 = v1_32 ^ v2_43 ^ v4_34
v4_34.d = v1_32.d u>> 3
v4_34:4.d = v1_32:4.d u>> 3
v4_34:8.d = v1_32:8.d u>> 3
v4_34:0xc.d = v1_32:0xc.d u>> 3
uint128_t v7_24 = vorrq_s8(v16_12, v7_23)
uint128_t v3_48 = vorrq_s8(v3_47, v4_34)
uint128_t v1_33 = v6_25 ^ v1_32
uint128_t v6_26 = vorrq_s8(v2_45, v3_48)
uint128_t v3_49 = v7_24 ^ v3_48
int32_t temp0_35 = x8_1[0x5c]
v5_31.d = temp0_35
v5_31:4.d = temp0_35
v5_31:8.d = temp0_35
v5_31:0xc.d = temp0_35
uint128_t v4_35 = v2_45 & v7_24
int32_t temp0_36 = x8_1[0x5e]
v16_12.d = temp0_36
v16_12:4.d = temp0_36
v16_12:8.d = temp0_36
v16_12:0xc.d = temp0_36
uint128_t v1_34 = vorrq_s8(v4_35, v1_33)
uint128_t v3_50 = v6_26 & not.o(v3_49)
uint128_t v4_36 = v4_35 ^ not.o(v3_49)
int32_t temp0_37 = x8_1[0x5d]
v0_38.d = temp0_37
v0_38:4.d = temp0_37
v0_38:8.d = temp0_37
v0_38:0xc.d = temp0_37
uint128_t v1_35 = v3_50 ^ v1_34
uint128_t v7_26 = v4_36 ^ v3_50
uint128_t v3_51 = vorrq_s8(v4_36, v3_50)
int32_t temp0_38 = x8_1[0x5f]
v4_36.d = temp0_38
v4_36:4.d = temp0_38
v4_36:8.d = temp0_38
v4_36:0xc.d = temp0_38
uint128_t v2_47 = (v1_33 ^ v2_45) & v6_26
uint128_t v6_27 = v1_35 ^ v6_26
uint128_t v1_36 = v1_35 ^ v16_12
uint128_t v2_48 = v3_51 ^ v2_47
uint128_t v5_34 = v2_47 ^ v5_31 ^ v7_26 ^ v6_27
uint128_t v6_28 = vorrq_s8(v2_48, v6_27)
uint128_t v2_49 = v2_48 ^ v0_38
v0_38.d = v1_36.d u>> 0x16
v0_38:4.d = v1_36:4.d u>> 0x16
v0_38:8.d = v1_36:8.d u>> 0x16
v0_38:0xc.d = v1_36:0xc.d u>> 0x16
v1_36.d <<= 0xa
v1_36:4.d <<= 0xa
v1_36:8.d <<= 0xa
v1_36:0xc.d <<= 0xa
uint128_t v0_39 = vorrq_s8(v1_36, v0_38)
uint128_t v1_37 = v7_26 ^ v4_36 ^ v6_28
uint128_t v4_37
v4_37.d = v5_34.d u>> 5
v4_37:4.d = v5_34:4.d u>> 5
v4_37:8.d = v5_34:8.d u>> 5
v4_37:0xc.d = v5_34:0xc.d u>> 5
v5_34.d <<= 0x1b
v5_34:4.d <<= 0x1b
v5_34:8.d <<= 0x1b
v5_34:0xc.d <<= 0x1b
v6_28.d = v2_49.d << 7
v6_28:4.d = v2_49:4.d << 7
v6_28:8.d = v2_49:8.d << 7
v6_28:0xc.d = v2_49:0xc.d << 7
uint128_t v4_38 = vorrq_s8(v5_34, v4_37)
v5_34.d = v2_49.d u>> 1
v5_34:4.d = v2_49:4.d u>> 1
v5_34:8.d = v2_49:8.d u>> 1
v5_34:0xc.d = v2_49:0xc.d u>> 1
uint128_t v6_29 = v6_28 ^ v0_39
v0_39.d = v2_49.d << 0x1f
v0_39:4.d = v2_49:4.d << 0x1f
v0_39:8.d = v2_49:8.d << 0x1f
v0_39:0xc.d = v2_49:0xc.d << 0x1f
uint128_t v2_51 = v1_37 ^ v2_49 ^ v4_38
v4_38.d = v1_37.d u>> 7
v4_38:4.d = v1_37:4.d u>> 7
v4_38:8.d = v1_37:8.d u>> 7
v4_38:0xc.d = v1_37:0xc.d u>> 7
uint128_t v6_30 = v6_29 ^ v1_37
v1_37.d <<= 0x19
v1_37:4.d <<= 0x19
v1_37:8.d <<= 0x19
v1_37:0xc.d <<= 0x19
uint128_t v5_35 = vorrq_s8(v0_39, v5_34)
uint128_t v1_38 = vorrq_s8(v1_37, v4_38)
v4_38.d = v2_51.d << 3
v4_38:4.d = v2_51:4.d << 3
v4_38:8.d = v2_51:8.d << 3
v4_38:0xc.d = v2_51:0xc.d << 3
int32_t temp0_39 = x8_1[0x59]
v16_12.d = temp0_39
v16_12:4.d = temp0_39
v16_12:8.d = temp0_39
v16_12:0xc.d = temp0_39
uint128_t v1_40 = v6_30 ^ v1_38 ^ v4_38
v4_38.d = v6_30.d u>> 3
v4_38:4.d = v6_30:4.d u>> 3
v4_38:8.d = v6_30:8.d u>> 3
v4_38:0xc.d = v6_30:0xc.d u>> 3
uint128_t v5_37 = v2_51 ^ v5_35 ^ v6_30
v6_30.d <<= 0x1d
v6_30:4.d <<= 0x1d
v6_30:8.d <<= 0x1d
v6_30:0xc.d <<= 0x1d
int32_t temp0_40 = x8_1[0x5a]
v3_51.d = temp0_40
v3_51:4.d = temp0_40
v3_51:8.d = temp0_40
v3_51:0xc.d = temp0_40
uint128_t v4_39 = vorrq_s8(v6_30, v4_38)
v6_30.d = v2_51.d u>> 0xd
v6_30:4.d = v2_51:4.d u>> 0xd
v6_30:8.d = v2_51:8.d u>> 0xd
v6_30:0xc.d = v2_51:0xc.d u>> 0xd
v2_51.d <<= 0x13
v2_51:4.d <<= 0x13
v2_51:8.d <<= 0x13
v2_51:0xc.d <<= 0x13
uint128_t v2_53 = v4_39 ^ vorrq_s8(v2_51, v6_30)
uint128_t v1_41 = v1_40 ^ v5_37
uint128_t v6_32 = vorrq_s8(v1_40 ^ v4_39, v2_53)
uint128_t v4_41 = (v2_53 & v4_39) ^ v1_41
int32_t temp0_41 = x8_1[0x5b]
v7_26.d = temp0_41
v7_26:4.d = temp0_41
v7_26:8.d = temp0_41
v7_26:0xc.d = temp0_41
uint128_t v1_42 = v6_32 ^ v1_41
uint128_t v6_33 = not.o(v4_41)
uint128_t v2_54 = v2_53 ^ v6_33
uint128_t v6_34 = v16_12 ^ v6_33
int32_t temp0_42 = x8_1[0x58]
v16_12.d = temp0_42
v16_12:4.d = temp0_42
v16_12:8.d = temp0_42
v16_12:0xc.d = temp0_42
uint128_t v5_39 = (v1_42 & v5_37) ^ v2_54
uint128_t v1_44 = (v1_42 ^ v2_54) | not.o(v4_41)
v4_41.d = v6_34.d u>> 1
v4_41:4.d = v6_34:4.d u>> 1
v4_41:8.d = v6_34:8.d u>> 1
v4_41:0xc.d = v6_34:0xc.d u>> 1
uint128_t v16_13 = v5_39 ^ v16_12
uint128_t v5_40 = v1_42 ^ v7_26 ^ v5_39
uint128_t v7_27
v7_27.d = v6_34.d << 0x1f
v7_27:4.d = v6_34:4.d << 0x1f
v7_27:8.d = v6_34:8.d << 0x1f
v7_27:0xc.d = v6_34:0xc.d << 0x1f
uint128_t v4_42 = vorrq_s8(v7_27, v4_41)
v7_27.d = v6_34.d << 7
v7_27:4.d = v6_34:4.d << 7
v7_27:8.d = v6_34:8.d << 7
v7_27:0xc.d = v6_34:0xc.d << 7
uint128_t v1_45 = v5_37 ^ v3_51 ^ v6_32 ^ v1_44
uint128_t v3_53
v3_53.d = v16_13.d u>> 5
v3_53:4.d = v16_13:4.d u>> 5
v3_53:8.d = v16_13:8.d u>> 5
v3_53:0xc.d = v16_13:0xc.d u>> 5
v16_13.d <<= 0x1b
v16_13:4.d <<= 0x1b
v16_13:8.d <<= 0x1b
v16_13:0xc.d <<= 0x1b
uint128_t v7_28 = v5_40 ^ v7_27
uint128_t v6_35 = v5_40 ^ v6_34
uint128_t v3_54 = vorrq_s8(v16_13, v3_53)
v16_13.d = v5_40.d u>> 7
v16_13:4.d = v5_40:4.d u>> 7
v16_13:8.d = v5_40:8.d u>> 7
v16_13:0xc.d = v5_40:0xc.d u>> 7
v5_40.d <<= 0x19
v5_40:4.d <<= 0x19
v5_40:8.d <<= 0x19
v5_40:0xc.d <<= 0x19
uint128_t v5_41 = vorrq_s8(v5_40, v16_13)
v16_13.d = v1_45.d u>> 0x16
v16_13:4.d = v1_45:4.d u>> 0x16
v16_13:8.d = v1_45:8.d u>> 0x16
v16_13:0xc.d = v1_45:0xc.d u>> 0x16
v1_45.d <<= 0xa
v1_45:4.d <<= 0xa
v1_45:8.d <<= 0xa
v1_45:0xc.d <<= 0xa
uint128_t v3_55 = v6_35 ^ v3_54
uint128_t v1_47 = v7_28 ^ vorrq_s8(v1_45, v16_13)
v6_35.d = v3_55.d << 3
v6_35:4.d = v3_55:4.d << 3
v6_35:8.d = v3_55:8.d << 3
v6_35:0xc.d = v3_55:0xc.d << 3
v7_28.d = v3_55.d u>> 0xd
v7_28:4.d = v3_55:4.d u>> 0xd
v7_28:8.d = v3_55:8.d u>> 0xd
v7_28:0xc.d = v3_55:0xc.d u>> 0xd
v16_13.d = v3_55.d << 0x13
v16_13:4.d = v3_55:4.d << 0x13
v16_13:8.d = v3_55:8.d << 0x13
v16_13:0xc.d = v3_55:0xc.d << 0x13
uint128_t v4_43 = v1_47 ^ v5_41
v5_41.d = v1_47.d u>> 3
v5_41:4.d = v1_47:4.d u>> 3
v5_41:8.d = v1_47:8.d u>> 3
v5_41:0xc.d = v1_47:0xc.d u>> 3
uint128_t v7_29 = vorrq_s8(v16_13, v7_28)
v16_13.d = v1_47.d << 0x1d
v16_13:4.d = v1_47:4.d << 0x1d
v16_13:8.d = v1_47:8.d << 0x1d
v16_13:0xc.d = v1_47:0xc.d << 0x1d
uint128_t v1_48 = v4_42 ^ v3_55 ^ v1_47
uint128_t v3_57 = v4_43 ^ v6_35
uint128_t v4_44 = vorrq_s8(v16_13, v5_41)
uint128_t v5_42 = not.o(v1_48)
uint128_t v16_14 = vorrq_s8(v3_57, v7_29)
int32_t temp0_43 = x8_1[0x56]
v2_54.d = temp0_43
v2_54:4.d = temp0_43
v2_54:8.d = temp0_43
v2_54:0xc.d = temp0_43
uint128_t v6_37 = v4_44 ^ v5_42 ^ v16_14
v0_39.d = x8_1[0x55]
uint128_t v3_58 = v6_37 ^ v3_57
int32_t temp0_44 = x8_1[0x54]
v16_14.d = temp0_44
v16_14:4.d = temp0_44
v16_14:8.d = temp0_44
v16_14:0xc.d = temp0_44
uint128_t v4_47 = v3_58 ^ ((v4_44 | not.o(v1_48)) & v7_29)
uint128_t v3_60 = vorrq_s8(v3_58, v7_29) ^ v5_42
uint128_t v2_55 = v7_29 ^ v2_54
int32_t temp0_45 = x8_1[0x57]
v7_29.d = temp0_45
v7_29:4.d = temp0_45
v7_29:8.d = temp0_45
v7_29:0xc.d = temp0_45
uint128_t v3_61 = v3_60 ^ v4_47
uint128_t v1_50 = (v4_47 & not.o(v1_48)) ^ v6_37
uint128_t v3_62 = v1_50 ^ v3_61
uint128_t v1_51 = v1_50 ^ v16_14
uint128_t v0_42 = v3_62 ^ vdupq_laneq_s32(not.o(v0_39), 0)
uint128_t v2_57 = v2_55 ^ (v3_61 & v6_37) ^ v3_62
v3_62.d = v1_51.d u>> 5
v3_62:4.d = v1_51:4.d u>> 5
v3_62:8.d = v1_51:8.d u>> 5
v3_62:0xc.d = v1_51:0xc.d u>> 5
v1_51.d <<= 0x1b
v1_51:4.d <<= 0x1b
v1_51:8.d <<= 0x1b
v1_51:0xc.d <<= 0x1b
uint128_t v7_30 = v4_47 ^ v7_29
uint128_t v1_52 = vorrq_s8(v1_51, v3_62)
v3_62.d = v0_42.d << 7
v3_62:4.d = v0_42:4.d << 7
v3_62:8.d = v0_42:8.d << 7
v3_62:0xc.d = v0_42:0xc.d << 7
v4_47.d = v7_30.d u>> 7
v4_47:4.d = v7_30:4.d u>> 7
v4_47:8.d = v7_30:8.d u>> 7
v4_47:0xc.d = v7_30:0xc.d u>> 7
v16_14.d = v7_30.d << 0x19
v16_14:4.d = v7_30:4.d << 0x19
v16_14:8.d = v7_30:8.d << 0x19
v16_14:0xc.d = v7_30:0xc.d << 0x19
uint128_t v1_53 = v0_42 ^ v7_30 ^ v1_52
uint128_t v7_31
v7_31.d = v0_42.d u>> 1
v7_31:4.d = v0_42:4.d u>> 1
v7_31:8.d = v0_42:8.d u>> 1
v7_31:0xc.d = v0_42:0xc.d u>> 1
v0_42.d <<= 0x1f
v0_42:4.d <<= 0x1f
v0_42:8.d <<= 0x1f
v0_42:0xc.d <<= 0x1f
uint128_t v0_43 = vorrq_s8(v0_42, v7_31)
v7_31.d = v2_57.d u>> 0x16
v7_31:4.d = v2_57:4.d u>> 0x16
v7_31:8.d = v2_57:8.d u>> 0x16
v7_31:0xc.d = v2_57:0xc.d u>> 0x16
v2_57.d <<= 0xa
v2_57:4.d <<= 0xa
v2_57:8.d <<= 0xa
v2_57:0xc.d <<= 0xa
uint128_t v4_48 = vorrq_s8(v16_14, v4_47)
uint128_t v2_58 = vorrq_s8(v2_57, v7_31)
v7_31.d = v1_53.d << 3
v7_31:4.d = v1_53:4.d << 3
v7_31:8.d = v1_53:8.d << 3
v7_31:0xc.d = v1_53:0xc.d << 3
uint128_t v0_44 = v1_53 ^ v0_43
uint128_t v2_59 = v3_62 ^ v7_30 ^ v2_58
uint128_t v3_63
v3_63.d = v1_53.d u>> 0xd
v3_63:4.d = v1_53:4.d u>> 0xd
v3_63:8.d = v1_53:8.d u>> 0xd
v3_63:0xc.d = v1_53:0xc.d u>> 0xd
v1_53.d <<= 0x13
v1_53:4.d <<= 0x13
v1_53:8.d <<= 0x13
v1_53:0xc.d <<= 0x13
uint128_t v1_54 = vorrq_s8(v1_53, v3_63)
uint128_t v3_64 = v7_31 ^ v4_48 ^ v2_59
uint128_t v0_45 = v0_44 ^ v2_59
uint128_t v4_49
v4_49.d = v2_59.d u>> 3
v4_49:4.d = v2_59:4.d u>> 3
v4_49:8.d = v2_59:8.d u>> 3
v4_49:0xc.d = v2_59:0xc.d u>> 3
v2_59.d <<= 0x1d
v2_59:4.d <<= 0x1d
v2_59:8.d <<= 0x1d
v2_59:0xc.d <<= 0x1d
uint128_t v2_60 = vorrq_s8(v2_59, v4_49)
int32_t temp0_46 = x8_1[0x50]
v5_42.d = temp0_46
v5_42:4.d = temp0_46
v5_42:8.d = temp0_46
v5_42:0xc.d = temp0_46
int32_t temp0_47 = x8_1[0x52]
v16_14.d = temp0_47
v16_14:4.d = temp0_47
v16_14:8.d = temp0_47
v16_14:0xc.d = temp0_47
uint128_t v4_51 = (v3_64 & v2_60) ^ v0_45
uint128_t v0_46 = vorrq_s8(v3_64, v0_45)
int32_t temp0_48 = x8_1[0x53]
uint128_t v6_38
v6_38.d = temp0_48
v6_38:4.d = temp0_48
v6_38:8.d = temp0_48
v6_38:0xc.d = temp0_48
uint128_t v0_49 = (v0_46 & v1_54) ^ v2_60 ^ v4_51
uint128_t v3_65 = v0_49 ^ v3_64
uint128_t v6_39 = v0_49 ^ v6_38
uint128_t v2_62 = v3_65 ^ (v4_51 & v1_54)
uint128_t v1_55 = v3_65 & not.o(v1_54)
v3_65.d = v6_39.d u>> 7
v3_65:4.d = v6_39:4.d u>> 7
v3_65:8.d = v6_39:8.d u>> 7
v3_65:0xc.d = v6_39:0xc.d u>> 7
uint128_t v7_33 = v2_62 ^ not.o(v1_54)
uint128_t v2_63 = v2_62 ^ v16_14
v16_14.d = v6_39.d << 0x19
v16_14:4.d = v6_39:4.d << 0x19
v16_14:8.d = v6_39:8.d << 0x19
v16_14:0xc.d = v6_39:0xc.d << 0x19
uint128_t v3_66 = vorrq_s8(v16_14, v3_65)
int32_t temp0_49 = x8_1[0x51]
v16_14.d = temp0_49
v16_14:4.d = temp0_49
v16_14:8.d = temp0_49
v16_14:0xc.d = temp0_49
uint128_t v1_56 = v1_55 ^ v4_51
uint128_t v5_43 = v7_33 ^ v5_42
uint128_t v1_57 = v1_56 ^ v16_14
uint128_t v4_52
v4_52.d = v5_43.d u>> 5
v4_52:4.d = v5_43:4.d u>> 5
v4_52:8.d = v5_43:8.d u>> 5
v4_52:0xc.d = v5_43:0xc.d u>> 5
v5_43.d <<= 0x1b
v5_43:4.d <<= 0x1b
v5_43:8.d <<= 0x1b
v5_43:0xc.d <<= 0x1b
uint128_t v7_34
v7_34.d = v1_57.d << 7
v7_34:4.d = v1_57:4.d << 7
v7_34:8.d = v1_57:8.d << 7
v7_34:0xc.d = v1_57:0xc.d << 7
uint128_t v5_44 = v1_57 ^ v6_39
uint128_t v0_52 = v2_63 ^ vorrq_s8((v7_33 & v4_51) ^ v0_49, v7_33 ^ v1_56)
uint128_t v6_40 = v7_34 ^ v6_39
v7_34.d = v1_57.d u>> 1
v7_34:4.d = v1_57:4.d u>> 1
v7_34:8.d = v1_57:8.d u>> 1
v7_34:0xc.d = v1_57:0xc.d u>> 1
v1_57.d <<= 0x1f
v1_57:4.d <<= 0x1f
v1_57:8.d <<= 0x1f
v1_57:0xc.d <<= 0x1f
uint128_t v4_54 = v5_44 ^ vorrq_s8(v5_43, v4_52)
v5_44.d = v0_52.d u>> 0x16
v5_44:4.d = v0_52:4.d u>> 0x16
v5_44:8.d = v0_52:8.d u>> 0x16
v5_44:0xc.d = v0_52:0xc.d u>> 0x16
v0_52.d <<= 0xa
v0_52:4.d <<= 0xa
v0_52:8.d <<= 0xa
v0_52:0xc.d <<= 0xa
uint128_t v1_58 = vorrq_s8(v1_57, v7_34)
uint128_t v0_53 = vorrq_s8(v0_52, v5_44)
v5_44.d = v4_54.d << 3
v5_44:4.d = v4_54:4.d << 3
v5_44:8.d = v4_54:8.d << 3
v5_44:0xc.d = v4_54:0xc.d << 3
uint128_t v1_59 = v4_54 ^ v1_58
uint128_t v3_67 = v5_44 ^ v3_66
v5_44.d = v4_54.d u>> 0xd
v5_44:4.d = v4_54:4.d u>> 0xd
v5_44:8.d = v4_54:8.d u>> 0xd
v5_44:0xc.d = v4_54:0xc.d u>> 0xd
v4_54.d <<= 0x13
v4_54:4.d <<= 0x13
v4_54:8.d <<= 0x13
v4_54:0xc.d <<= 0x13
uint128_t v0_54 = v6_40 ^ v0_53
uint128_t v4_55 = vorrq_s8(v4_54, v5_44)
uint128_t v3_68 = v3_67 ^ v0_54
uint128_t v1_60 = v1_59 ^ v0_54
v6_40.d = v0_54.d u>> 3
v6_40:4.d = v0_54:4.d u>> 3
v6_40:8.d = v0_54:8.d u>> 3
v6_40:0xc.d = v0_54:0xc.d u>> 3
v0_54.d <<= 0x1d
v0_54:4.d <<= 0x1d
v0_54:8.d <<= 0x1d
v0_54:0xc.d <<= 0x1d
uint128_t v0_55 = vorrq_s8(v0_54, v6_40)
uint128_t v6_41 = v1_60 ^ v0_55
int32_t temp0_50 = x8_1[0x4d]
v7_34.d = temp0_50
v7_34:4.d = temp0_50
v7_34:8.d = temp0_50
v7_34:0xc.d = temp0_50
uint128_t v4_56 = v6_41 ^ v4_55
uint128_t v17_5 = (v6_41 & v0_55) ^ v4_56
uint128_t v0_57 = v4_56 & v1_60
int32_t temp0_51 = x8_1[0x4f]
v1_60.d = temp0_51
v1_60:4.d = temp0_51
v1_60:8.d = temp0_51
v1_60:0xc.d = temp0_51
int32_t temp0_52 = x8_1[0x4c]
v16_14.d = temp0_52
v16_14:4.d = temp0_52
v16_14:8.d = temp0_52
v16_14:0xc.d = temp0_52
int32_t temp0_53 = x8_1[0x4e]
v2_63.d = temp0_53
v2_63:4.d = temp0_53
v2_63:8.d = temp0_53
v2_63:0xc.d = temp0_53
uint128_t v3_69 = vorrq_s8(v17_5, v3_68)
uint128_t v4_57 = v3_67 ^ v1_59 ^ v17_5
uint128_t v5_46 = v3_69 ^ v6_41
uint128_t v0_58 = v3_69 ^ v0_57
uint128_t v3_71 = (v5_46 & v3_69) ^ v4_57
uint128_t v6_42 = v5_46 ^ v16_14
uint128_t v5_48 = vorrq_s8(v0_58 ^ v4_57, v5_46) ^ v17_5
v16_14.d = v6_42.d u>> 5
v16_14:4.d = v6_42:4.d u>> 5
v16_14:8.d = v6_42:8.d u>> 5
v16_14:0xc.d = v6_42:0xc.d u>> 5
v6_42.d <<= 0x1b
v6_42:4.d <<= 0x1b
v6_42:8.d <<= 0x1b
v6_42:0xc.d <<= 0x1b
uint128_t v2_64 = v3_71 ^ v2_63
uint128_t v3_72 = vorrq_s8(v6_42, v16_14)
uint128_t v6_43 = v5_48 ^ v7_34
uint128_t v1_63 = v0_58 ^ v1_60 ^ v3_71 ^ v5_48
v5_48.d = v2_64.d u>> 0x16
v5_48:4.d = v2_64:4.d u>> 0x16
v5_48:8.d = v2_64:8.d u>> 0x16
v5_48:0xc.d = v2_64:0xc.d u>> 0x16
v2_64.d <<= 0xa
v2_64:4.d <<= 0xa
v2_64:8.d <<= 0xa
v2_64:0xc.d <<= 0xa
uint128_t v2_65 = vorrq_s8(v2_64, v5_48)
v5_48.d = v1_63.d u>> 7
v5_48:4.d = v1_63:4.d u>> 7
v5_48:8.d = v1_63:8.d u>> 7
v5_48:0xc.d = v1_63:0xc.d u>> 7
v7_34.d = v1_63.d << 0x19
v7_34:4.d = v1_63:4.d << 0x19
v7_34:8.d = v1_63:8.d << 0x19
v7_34:0xc.d = v1_63:0xc.d << 0x19
v16_14.d = v6_43.d u>> 1
v16_14:4.d = v6_43:4.d u>> 1
v16_14:8.d = v6_43:8.d u>> 1
v16_14:0xc.d = v6_43:0xc.d u>> 1
uint128_t v2_66 = v1_63 ^ v2_65
uint128_t v1_64 = v6_43 ^ v3_72 ^ v1_63
uint128_t v3_73
v3_73.d = v6_43.d << 0x1f
v3_73:4.d = v6_43:4.d << 0x1f
v3_73:8.d = v6_43:8.d << 0x1f
v3_73:0xc.d = v6_43:0xc.d << 0x1f
uint128_t v3_74 = vorrq_s8(v3_73, v16_14)
v6_43.d <<= 7
v6_43:4.d <<= 7
v6_43:8.d <<= 7
v6_43:0xc.d <<= 7
uint128_t v5_49 = vorrq_s8(v7_34, v5_48)
uint128_t v6_44 = v2_66 ^ v6_43
uint128_t v3_75 = v1_64 ^ v3_74
v7_34.d = v1_64.d u>> 0xd
v7_34:4.d = v1_64:4.d u>> 0xd
v7_34:8.d = v1_64:8.d u>> 0xd
v7_34:0xc.d = v1_64:0xc.d u>> 0xd
v16_14.d = v1_64.d << 0x13
v16_14:4.d = v1_64:4.d << 0x13
v16_14:8.d = v1_64:8.d << 0x13
v16_14:0xc.d = v1_64:0xc.d << 0x13
v1_64.d <<= 3
v1_64:4.d <<= 3
v1_64:8.d <<= 3
v1_64:0xc.d <<= 3
uint128_t v5_50 = v6_44 ^ v5_49
v2_66.d <<= 0x1d
v2_66:4.d <<= 0x1d
v2_66:8.d <<= 0x1d
v2_66:0xc.d <<= 0x1d
uint128_t v3_76 = v3_75 ^ v6_44
v6_44.d u>>= 3
v6_44:4.d u>>= 3
v6_44:8.d u>>= 3
v6_44:0xc.d u>>= 3
uint128_t v7_35 = vorrq_s8(v16_14, v7_34)
uint128_t v1_65 = v5_50 ^ v1_64
uint128_t v2_67 = vorrq_s8(v2_66, v6_44)
int32_t temp0_54 = x8_1[0x48]
v4_57.d = temp0_54
v4_57:4.d = temp0_54
v4_57:8.d = temp0_54
v4_57:0xc.d = temp0_54
int32_t temp0_55 = x8_1[0x49]
uint128_t v0_59
v0_59.d = temp0_55
v0_59:4.d = temp0_55
v0_59:8.d = temp0_55
v0_59:0xc.d = temp0_55
uint128_t v2_68 = v1_65 ^ v2_67
uint128_t v1_66 = v1_65 ^ v7_35
int32_t temp0_56 = x8_1[0x4a]
v16_14.d = temp0_56
v16_14:4.d = temp0_56
v16_14:8.d = temp0_56
v16_14:0xc.d = temp0_56
int32_t temp0_57 = x8_1[0x4b]
v5_50.d = temp0_57
v5_50:4.d = temp0_57
v5_50:8.d = temp0_57
v5_50:0xc.d = temp0_57
uint128_t v6_46 = (v2_68 & v1_66) ^ v3_76
uint128_t v3_78 = vorrq_s8(v2_68, v3_76) ^ v1_66
uint128_t v2_69 = v6_46 ^ v2_68
uint128_t v6_47 = not.o(v6_46)
uint128_t v17_6 = (v1_66 & v7_35 & v6_46) ^ v2_69
uint128_t v4_58 = v3_78 ^ v4_57
uint128_t v6_48
v6_48.d = v4_58.d u>> 5
v6_48:4.d = v4_58:4.d u>> 5
v6_48:8.d = v4_58:8.d u>> 5
v6_48:0xc.d = v4_58:0xc.d u>> 5
v4_58.d <<= 0x1b
v4_58:4.d <<= 0x1b
v4_58:8.d <<= 0x1b
v4_58:0xc.d <<= 0x1b
uint128_t v0_60 = v17_6 ^ v0_59
uint128_t v2_71 = v16_14 ^ v6_47 ^ vorrq_s8(v2_69 & v3_78, v7_35)
uint128_t v5_51
v5_51.d = v0_60.d << 7
v5_51:4.d = v0_60:4.d << 7
v5_51:8.d = v0_60:8.d << 7
v5_51:0xc.d = v0_60:0xc.d << 7
uint128_t v4_60 = v0_60 ^ vorrq_s8(v4_58, v6_48)
v16_14.d = v0_60.d u>> 1
v16_14:4.d = v0_60:4.d u>> 1
v16_14:8.d = v0_60:8.d u>> 1
v16_14:0xc.d = v0_60:0xc.d u>> 1
v0_60.d <<= 0x1f
v0_60:4.d <<= 0x1f
v0_60:8.d <<= 0x1f
v0_60:0xc.d <<= 0x1f
uint128_t v3_81 = v5_50 ^ v6_47 ^ ((v7_35 ^ v6_47) & v3_78) ^ v17_6
uint128_t v0_61 = vorrq_s8(v0_60, v16_14)
v16_14.d = v2_71.d u>> 0x16
v16_14:4.d = v2_71:4.d u>> 0x16
v16_14:8.d = v2_71:8.d u>> 0x16
v16_14:0xc.d = v2_71:0xc.d u>> 0x16
v2_71.d <<= 0xa
v2_71:4.d <<= 0xa
v2_71:8.d <<= 0xa
v2_71:0xc.d <<= 0xa
uint128_t v2_72 = vorrq_s8(v2_71, v16_14)
uint128_t v5_52 = v3_81 ^ v5_51
uint128_t v4_61 = v4_60 ^ v3_81
v16_14.d = v3_81.d u>> 7
v16_14:4.d = v3_81:4.d u>> 7
v16_14:8.d = v3_81:8.d u>> 7
v16_14:0xc.d = v3_81:0xc.d u>> 7
v3_81.d <<= 0x19
v3_81:4.d <<= 0x19
v3_81:8.d <<= 0x19
v3_81:0xc.d <<= 0x19
uint128_t v2_73 = v5_52 ^ v2_72
v5_52.d = v4_61.d << 3
v5_52:4.d = v4_61:4.d << 3
v5_52:8.d = v4_61:8.d << 3
v5_52:0xc.d = v4_61:0xc.d << 3
uint128_t v0_62 = v4_61 ^ v0_61
uint128_t v3_83 = v2_73 ^ vorrq_s8(v3_81, v16_14)
v16_14.d = v4_61.d u>> 0xd
v16_14:4.d = v4_61:4.d u>> 0xd
v16_14:8.d = v4_61:8.d u>> 0xd
v16_14:0xc.d = v4_61:0xc.d u>> 0xd
v4_61.d <<= 0x13
v4_61:4.d <<= 0x13
v4_61:8.d <<= 0x13
v4_61:0xc.d <<= 0x13
uint128_t v0_63 = v0_62 ^ v2_73
uint128_t v3_84 = v3_83 ^ v5_52
uint128_t v4_62 = vorrq_s8(v4_61, v16_14)
v16_14.d = v2_73.d u>> 3
v16_14:4.d = v2_73:4.d u>> 3
v16_14:8.d = v2_73:8.d u>> 3
v16_14:0xc.d = v2_73:0xc.d u>> 3
v2_73.d <<= 0x1d
v2_73:4.d <<= 0x1d
v2_73:8.d <<= 0x1d
v2_73:0xc.d <<= 0x1d
uint128_t v5_53 = v3_84 ^ v0_63
int32_t temp0_58 = x8_1[0x44]
uint128_t v1_69
v1_69.d = temp0_58
v1_69:4.d = temp0_58
v1_69:8.d = temp0_58
v1_69:0xc.d = temp0_58
uint128_t v2_74 = vorrq_s8(v2_73, v16_14)
int32_t temp0_59 = x8_1[0x46]
uint128_t v7_36
v7_36.d = temp0_59
v7_36:4.d = temp0_59
v7_36:8.d = temp0_59
v7_36:0xc.d = temp0_59
uint128_t v0_64 = v0_63 ^ v2_74
uint128_t v16_15 = vorrq_s8(v5_53, v4_62)
uint128_t v3_86 = (v5_53 & v3_84) ^ v4_62
int32_t temp0_60 = x8_1[0x45]
v6_48.d = temp0_60
v6_48:4.d = temp0_60
v6_48:8.d = temp0_60
v6_48:0xc.d = temp0_60
uint128_t v2_75 = v3_86 ^ v2_74
uint128_t v4_64 = vorrq_s8(v2_75, v16_15 ^ v0_64) ^ v3_86 ^ v5_53
uint128_t v5_54 = vorrq_s8(v3_86, v5_53)
uint128_t v3_87 = v3_86 ^ v7_36
int32_t temp0_61 = x8_1[0x47]
v7_36.d = temp0_61
v7_36:4.d = temp0_61
v7_36:8.d = temp0_61
v7_36:0xc.d = temp0_61
uint128_t v6_49 = v4_64 ^ v6_48
uint128_t v0_66 = v4_64 ^ v0_64 ^ v5_54
uint128_t v3_88 = v3_87 ^ ((v5_54 & not.o(v4_64)) | not.o(v0_66))
uint128_t v2_76 = v2_75 ^ v7_36
uint128_t v0_68 = v1_69 ^ not.o(v0_66)
int128_t v4_66
v4_66.d = v3_88.d u>> 0x16
v4_66:4.d = v3_88:4.d u>> 0x16
v4_66:8.d = v3_88:8.d u>> 0x16
v4_66:0xc.d = v3_88:0xc.d u>> 0x16
v3_88.d <<= 0xa
v3_88:4.d <<= 0xa
v3_88:8.d <<= 0xa
v3_88:0xc.d <<= 0xa
v7_36.d = v2_76.d u>> 7
v7_36:4.d = v2_76:4.d u>> 7
v7_36:8.d = v2_76:8.d u>> 7
v7_36:0xc.d = v2_76:0xc.d u>> 7
uint128_t v16_17
v16_17.d = v2_76.d << 0x19
v16_17:4.d = v2_76:4.d << 0x19
v16_17:8.d = v2_76:8.d << 0x19
v16_17:0xc.d = v2_76:0xc.d << 0x19
uint128_t v3_89 = vorrq_s8(v3_88, v4_66)
v4_66.d = v0_68.d u>> 5
v4_66:4.d = v0_68:4.d u>> 5
v4_66:8.d = v0_68:8.d u>> 5
v4_66:0xc.d = v0_68:0xc.d u>> 5
v0_68.d <<= 0x1b
v0_68:4.d <<= 0x1b
v0_68:8.d <<= 0x1b
v0_68:0xc.d <<= 0x1b
uint128_t v7_37 = vorrq_s8(v16_17, v7_36)
v5_54.d = v6_49.d << 7
v5_54:4.d = v6_49:4.d << 7
v5_54:8.d = v6_49:8.d << 7
v5_54:0xc.d = v6_49:0xc.d << 7
uint128_t v2_77 = v5_54 ^ v2_76
uint128_t v0_70 = v6_49 ^ v2_76 ^ vorrq_s8(v0_68, v4_66)
v5_54.d = v6_49.d u>> 1
v5_54:4.d = v6_49:4.d u>> 1
v5_54:8.d = v6_49:8.d u>> 1
v5_54:0xc.d = v6_49:0xc.d u>> 1
v6_49.d <<= 0x1f
v6_49:4.d <<= 0x1f
v6_49:8.d <<= 0x1f
v6_49:0xc.d <<= 0x1f
uint128_t v2_78 = v2_77 ^ v3_89
v3_89.d = v0_70.d << 3
v3_89:4.d = v0_70:4.d << 3
v3_89:8.d = v0_70:8.d << 3
v3_89:0xc.d = v0_70:0xc.d << 3
uint128_t v16_18
v16_18.d = v2_78.d u>> 3
v16_18:4.d = v2_78:4.d u>> 3
v16_18:8.d = v2_78:8.d u>> 3
v16_18:0xc.d = v2_78:0xc.d u>> 3
uint128_t v3_90 = v3_89 ^ v7_37
v7_37.d = v2_78.d << 0x1d
v7_37:4.d = v2_78:4.d << 0x1d
v7_37:8.d = v2_78:8.d << 0x1d
v7_37:0xc.d = v2_78:0xc.d << 0x1d
uint128_t v5_56 = v0_70 ^ vorrq_s8(v6_49, v5_54)
uint128_t v7_38 = vorrq_s8(v7_37, v16_18)
v16_18.d = v0_70.d u>> 0xd
v16_18:4.d = v0_70:4.d u>> 0xd
v16_18:8.d = v0_70:8.d u>> 0xd
v16_18:0xc.d = v0_70:0xc.d u>> 0xd
v0_70.d <<= 0x13
v0_70:4.d <<= 0x13
v0_70:8.d <<= 0x13
v0_70:0xc.d <<= 0x13
uint128_t v5_57 = v5_56 ^ v2_78
uint128_t v0_71 = vorrq_s8(v0_70, v16_18)
int32_t temp0_62 = x8_1[0x40]
v6_49.d = temp0_62
v6_49:4.d = temp0_62
v6_49:8.d = temp0_62
v6_49:0xc.d = temp0_62
uint128_t v2_79 = v3_90 ^ v2_78
uint128_t v16_19 = vorrq_s8(v5_57, v0_71)
uint128_t v5_58 = not.o(v5_57)
int32_t temp0_63 = x8_1[0x42]
v4_66.d = temp0_63
v4_66:4.d = temp0_63
v4_66:8.d = temp0_63
v4_66:0xc.d = temp0_63
uint128_t v0_72 = v0_71 ^ v5_58
int32_t temp0_64 = x8_1[0x41]
v1_69.d = temp0_64
v1_69:4.d = temp0_64
v1_69:8.d = temp0_64
v1_69:0xc.d = temp0_64
uint128_t v3_93 = v2_79 ^ not.o(v7_38) ^ v16_19
uint128_t v7_40 = (v5_58 | not.o(v7_38)) ^ v0_72
uint128_t v0_73 = v0_72 & v2_79
int32_t temp0_65 = x8_1[0x43]
v16_19.d = temp0_65
v16_19:4.d = temp0_65
v16_19:8.d = temp0_65
v16_19:0xc.d = temp0_65
uint128_t v5_59 = v0_73 ^ v5_58
uint128_t v17_7 = vorrq_s8(v3_93, v0_73) ^ v7_40
uint128_t v0_75 = v3_93 ^ v4_66
uint128_t v2_81 = v5_59 ^ v2_79 ^ v3_93
uint128_t v4_67 = v17_7 ^ v6_49
uint128_t v7_41 = v3_93 ^ v7_40
v3_93.d = v0_75.d u>> 0x16
v3_93:4.d = v0_75:4.d u>> 0x16
v3_93:8.d = v0_75:8.d u>> 0x16
v3_93:0xc.d = v0_75:0xc.d u>> 0x16
v0_75.d <<= 0xa
v0_75:4.d <<= 0xa
v0_75:8.d <<= 0xa
v0_75:0xc.d <<= 0xa
uint128_t v2_82 = v2_81 ^ v17_7
v6_49.d = v4_67.d u>> 5
v6_49:4.d = v4_67:4.d u>> 5
v6_49:8.d = v4_67:8.d u>> 5
v6_49:0xc.d = v4_67:0xc.d u>> 5
v4_67.d <<= 0x1b
v4_67:4.d <<= 0x1b
v4_67:8.d <<= 0x1b
v4_67:0xc.d <<= 0x1b
uint128_t v2_83 = v2_82 ^ v16_19
uint128_t v1_71 = v5_59 ^ v1_69 ^ (v2_82 & v7_41)
uint128_t v3_95 = v2_83 ^ vorrq_s8(v0_75, v3_93)
uint128_t v4_69 = v2_83 ^ vorrq_s8(v4_67, v6_49)
v6_49.d = v2_83.d u>> 7
v6_49:4.d = v2_83:4.d u>> 7
v6_49:8.d = v2_83:8.d u>> 7
v6_49:0xc.d = v2_83:0xc.d u>> 7
v2_83.d <<= 0x19
v2_83:4.d <<= 0x19
v2_83:8.d <<= 0x19
v2_83:0xc.d <<= 0x19
uint128_t v4_70 = v4_69 ^ v1_71
uint128_t v2_84 = vorrq_s8(v2_83, v6_49)
v6_49.d = v1_71.d u>> 1
v6_49:4.d = v1_71:4.d u>> 1
v6_49:8.d = v1_71:8.d u>> 1
v6_49:0xc.d = v1_71:0xc.d u>> 1
uint128_t v7_42
v7_42.d = v1_71.d << 0x1f
v7_42:4.d = v1_71:4.d << 0x1f
v7_42:8.d = v1_71:8.d << 0x1f
v7_42:0xc.d = v1_71:0xc.d << 0x1f
v1_71.d <<= 7
v1_71:4.d <<= 7
v1_71:8.d <<= 7
v1_71:0xc.d <<= 7
uint128_t v6_50 = vorrq_s8(v7_42, v6_49)
uint128_t v1_72 = v3_95 ^ v1_71
v7_42.d = v4_70.d u>> 0xd
v7_42:4.d = v4_70:4.d u>> 0xd
v7_42:8.d = v4_70:8.d u>> 0xd
v7_42:0xc.d = v4_70:0xc.d u>> 0xd
v16_19.d = v4_70.d << 0x13
v16_19:4.d = v4_70:4.d << 0x13
v16_19:8.d = v4_70:8.d << 0x13
v16_19:0xc.d = v4_70:0xc.d << 0x13
uint128_t v6_51 = v4_70 ^ v6_50
v4_70.d <<= 3
v4_70:4.d <<= 3
v4_70:8.d <<= 3
v4_70:0xc.d <<= 3
v3_95.d <<= 0x1d
v3_95:4.d <<= 0x1d
v3_95:8.d <<= 0x1d
v3_95:0xc.d <<= 0x1d
uint128_t v2_86 = v1_72 ^ v2_84 ^ v4_70
v4_70.d = v1_72.d u>> 3
v4_70:4.d = v1_72:4.d u>> 3
v4_70:8.d = v1_72:8.d u>> 3
v4_70:0xc.d = v1_72:0xc.d u>> 3
uint128_t v7_43 = vorrq_s8(v16_19, v7_42)
uint128_t v3_96 = vorrq_s8(v3_95, v4_70)
uint128_t v1_73 = v6_51 ^ v1_72
uint128_t v6_52 = vorrq_s8(v2_86, v3_96)
uint128_t v3_97 = v7_43 ^ v3_96
int32_t temp0_66 = x8_1[0x3c]
v5_59.d = temp0_66
v5_59:4.d = temp0_66
v5_59:8.d = temp0_66
v5_59:0xc.d = temp0_66
uint128_t v4_71 = v2_86 & v7_43
int32_t temp0_67 = x8_1[0x3e]
v16_19.d = temp0_67
v16_19:4.d = temp0_67
v16_19:8.d = temp0_67
v16_19:0xc.d = temp0_67
uint128_t v1_74 = vorrq_s8(v4_71, v1_73)
int128_t v3_98 = v6_52 & not.o(v3_97)
uint128_t v4_72 = v4_71 ^ not.o(v3_97)
int32_t temp0_68 = x8_1[0x3d]
v0_75.d = temp0_68
v0_75:4.d = temp0_68
v0_75:8.d = temp0_68
v0_75:0xc.d = temp0_68
int128_t v1_75 = v3_98 ^ v1_74
int128_t v7_45 = v4_72 ^ v3_98
uint128_t v3_99 = vorrq_s8(v4_72, v3_98)
int32_t temp0_69 = x8_1[0x3f]
v4_72.d = temp0_69
v4_72:4.d = temp0_69
v4_72:8.d = temp0_69
v4_72:0xc.d = temp0_69
uint128_t v2_88 = (v1_73 ^ v2_86) & v6_52
int128_t v6_53 = v1_75 ^ v6_52
uint128_t v1_76 = v1_75 ^ v16_19
uint128_t v2_89 = v3_99 ^ v2_88
uint128_t v5_62 = v2_88 ^ v5_59 ^ v7_45 ^ v6_53
uint128_t v6_54 = vorrq_s8(v2_89, v6_53)
uint128_t v2_90 = v2_89 ^ v0_75
v0_75.d = v1_76.d u>> 0x16
v0_75:4.d = v1_76:4.d u>> 0x16
v0_75:8.d = v1_76:8.d u>> 0x16
v0_75:0xc.d = v1_76:0xc.d u>> 0x16
v1_76.d <<= 0xa
v1_76:4.d <<= 0xa
v1_76:8.d <<= 0xa
v1_76:0xc.d <<= 0xa
uint128_t v0_76 = vorrq_s8(v1_76, v0_75)
int128_t v1_77 = v7_45 ^ v4_72 ^ v6_54
int128_t v4_73
v4_73.d = v5_62.d u>> 5
v4_73:4.d = v5_62:4.d u>> 5
v4_73:8.d = v5_62:8.d u>> 5
v4_73:0xc.d = v5_62:0xc.d u>> 5
v5_62.d <<= 0x1b
v5_62:4.d <<= 0x1b
v5_62:8.d <<= 0x1b
v5_62:0xc.d <<= 0x1b
v6_54.d = v2_90.d << 7
v6_54:4.d = v2_90:4.d << 7
v6_54:8.d = v2_90:8.d << 7
v6_54:0xc.d = v2_90:0xc.d << 7
uint128_t v4_74 = vorrq_s8(v5_62, v4_73)
v5_62.d = v2_90.d u>> 1
v5_62:4.d = v2_90:4.d u>> 1
v5_62:8.d = v2_90:8.d u>> 1
v5_62:0xc.d = v2_90:0xc.d u>> 1
uint128_t v6_55 = v6_54 ^ v0_76
v0_76.d = v2_90.d << 0x1f
v0_76:4.d = v2_90:4.d << 0x1f
v0_76:8.d = v2_90:8.d << 0x1f
v0_76:0xc.d = v2_90:0xc.d << 0x1f
uint128_t v2_92 = v1_77 ^ v2_90 ^ v4_74
v4_74.d = v1_77.d u>> 7
v4_74:4.d = v1_77:4.d u>> 7
v4_74:8.d = v1_77:8.d u>> 7
v4_74:0xc.d = v1_77:0xc.d u>> 7
int128_t v6_56 = v6_55 ^ v1_77
v1_77.d <<= 0x19
v1_77:4.d <<= 0x19
v1_77:8.d <<= 0x19
v1_77:0xc.d <<= 0x19
uint128_t v5_63 = vorrq_s8(v0_76, v5_62)
uint128_t v1_78 = vorrq_s8(v1_77, v4_74)
v4_74.d = v2_92.d << 3
v4_74:4.d = v2_92:4.d << 3
v4_74:8.d = v2_92:8.d << 3
v4_74:0xc.d = v2_92:0xc.d << 3
int32_t temp0_70 = x8_1[0x39]
v16_19.d = temp0_70
v16_19:4.d = temp0_70
v16_19:8.d = temp0_70
v16_19:0xc.d = temp0_70
int128_t v1_80 = v6_56 ^ v1_78 ^ v4_74
v4_74.d = v6_56.d u>> 3
v4_74:4.d = v6_56:4.d u>> 3
v4_74:8.d = v6_56:8.d u>> 3
v4_74:0xc.d = v6_56:0xc.d u>> 3
int128_t v5_65 = v2_92 ^ v5_63 ^ v6_56
v6_56.d <<= 0x1d
v6_56:4.d <<= 0x1d
v6_56:8.d <<= 0x1d
v6_56:0xc.d <<= 0x1d
int32_t temp0_71 = x8_1[0x3a]
v3_99.d = temp0_71
v3_99:4.d = temp0_71
v3_99:8.d = temp0_71
v3_99:0xc.d = temp0_71
uint128_t v4_75 = vorrq_s8(v6_56, v4_74)
v6_56.d = v2_92.d u>> 0xd
v6_56:4.d = v2_92:4.d u>> 0xd
v6_56:8.d = v2_92:8.d u>> 0xd
v6_56:0xc.d = v2_92:0xc.d u>> 0xd
v2_92.d <<= 0x13
v2_92:4.d <<= 0x13
v2_92:8.d <<= 0x13
v2_92:0xc.d <<= 0x13
uint128_t v2_94 = v4_75 ^ vorrq_s8(v2_92, v6_56)
int128_t v1_81 = v1_80 ^ v5_65
uint128_t v6_58 = vorrq_s8(v1_80 ^ v4_75, v2_94)
int128_t v4_77 = (v2_94 & v4_75) ^ v1_81
int32_t temp0_72 = x8_1[0x3b]
v7_45.d = temp0_72
v7_45:4.d = temp0_72
v7_45:8.d = temp0_72
v7_45:0xc.d = temp0_72
int128_t v1_82 = v6_58 ^ v1_81
int128_t v6_59 = not.o(v4_77)
int128_t v2_95 = v2_94 ^ v6_59
int128_t v6_60 = v16_19 ^ v6_59
int32_t temp0_73 = x8_1[0x38]
v16_19.d = temp0_73
v16_19:4.d = temp0_73
v16_19:8.d = temp0_73
v16_19:0xc.d = temp0_73
int128_t v5_67 = (v1_82 & v5_65) ^ v2_95
int128_t v1_84 = (v1_82 ^ v2_95) | not.o(v4_77)
v4_77.d = v6_60.d u>> 1
v4_77:4.d = v6_60:4.d u>> 1
v4_77:8.d = v6_60:8.d u>> 1
v4_77:0xc.d = v6_60:0xc.d u>> 1
uint128_t v16_20 = v5_67 ^ v16_19
int128_t v5_68 = v1_82 ^ v7_45 ^ v5_67
uint128_t v7_46
v7_46.d = v6_60.d << 0x1f
v7_46:4.d = v6_60:4.d << 0x1f
v7_46:8.d = v6_60:8.d << 0x1f
v7_46:0xc.d = v6_60:0xc.d << 0x1f
uint128_t v4_78 = vorrq_s8(v7_46, v4_77)
v7_46.d = v6_60.d << 7
v7_46:4.d = v6_60:4.d << 7
v7_46:8.d = v6_60:8.d << 7
v7_46:0xc.d = v6_60:0xc.d << 7
int128_t v1_85 = v5_65 ^ v3_99 ^ v6_58 ^ v1_84
int128_t v3_101
v3_101.d = v16_20.d u>> 5
v3_101:4.d = v16_20:4.d u>> 5
v3_101:8.d = v16_20:8.d u>> 5
v3_101:0xc.d = v16_20:0xc.d u>> 5
v16_20.d <<= 0x1b
v16_20:4.d <<= 0x1b
v16_20:8.d <<= 0x1b
v16_20:0xc.d <<= 0x1b
int128_t v7_47 = v5_68 ^ v7_46
int128_t v6_61 = v5_68 ^ v6_60
uint128_t v3_102 = vorrq_s8(v16_20, v3_101)
v16_20.d = v5_68.d u>> 7
v16_20:4.d = v5_68:4.d u>> 7
v16_20:8.d = v5_68:8.d u>> 7
v16_20:0xc.d = v5_68:0xc.d u>> 7
v5_68.d <<= 0x19
v5_68:4.d <<= 0x19
v5_68:8.d <<= 0x19
v5_68:0xc.d <<= 0x19
uint128_t v5_69 = vorrq_s8(v5_68, v16_20)
v16_20.d = v1_85.d u>> 0x16
v16_20:4.d = v1_85:4.d u>> 0x16
v16_20:8.d = v1_85:8.d u>> 0x16
v16_20:0xc.d = v1_85:0xc.d u>> 0x16
v1_85.d <<= 0xa
v1_85:4.d <<= 0xa
v1_85:8.d <<= 0xa
v1_85:0xc.d <<= 0xa
int128_t v3_103 = v6_61 ^ v3_102
int128_t v1_87 = v7_47 ^ vorrq_s8(v1_85, v16_20)
v6_61.d = v3_103.d << 3
v6_61:4.d = v3_103:4.d << 3
v6_61:8.d = v3_103:8.d << 3
v6_61:0xc.d = v3_103:0xc.d << 3
v7_47.d = v3_103.d u>> 0xd
v7_47:4.d = v3_103:4.d u>> 0xd
v7_47:8.d = v3_103:8.d u>> 0xd
v7_47:0xc.d = v3_103:0xc.d u>> 0xd
v16_20.d = v3_103.d << 0x13
v16_20:4.d = v3_103:4.d << 0x13
v16_20:8.d = v3_103:8.d << 0x13
v16_20:0xc.d = v3_103:0xc.d << 0x13
int128_t v4_79 = v1_87 ^ v5_69
v5_69.d = v1_87.d u>> 3
v5_69:4.d = v1_87:4.d u>> 3
v5_69:8.d = v1_87:8.d u>> 3
v5_69:0xc.d = v1_87:0xc.d u>> 3
uint128_t v7_48 = vorrq_s8(v16_20, v7_47)
v16_20.d = v1_87.d << 0x1d
v16_20:4.d = v1_87:4.d << 0x1d
v16_20:8.d = v1_87:8.d << 0x1d
v16_20:0xc.d = v1_87:0xc.d << 0x1d
int128_t v1_88 = v4_78 ^ v3_103 ^ v1_87
int128_t v3_105 = v4_79 ^ v6_61
uint128_t v4_80 = vorrq_s8(v16_20, v5_69)
int128_t v5_70 = not.o(v1_88)
uint128_t v16_21 = vorrq_s8(v3_105, v7_48)
int32_t temp0_74 = x8_1[0x36]
v2_95.d = temp0_74
v2_95:4.d = temp0_74
v2_95:8.d = temp0_74
v2_95:0xc.d = temp0_74
int128_t v6_63 = v4_80 ^ v5_70 ^ v16_21
v0_76.d = x8_1[0x35]
int128_t v3_106 = v6_63 ^ v3_105
int32_t temp0_75 = x8_1[0x34]
v16_21.d = temp0_75
v16_21:4.d = temp0_75
v16_21:8.d = temp0_75
v16_21:0xc.d = temp0_75
int128_t v4_83 = v3_106 ^ ((v4_80 | not.o(v1_88)) & v7_48)
int128_t v3_108 = vorrq_s8(v3_106, v7_48) ^ v5_70
int128_t v2_96 = v7_48 ^ v2_95
int32_t temp0_76 = x8_1[0x37]
v7_48.d = temp0_76
v7_48:4.d = temp0_76
v7_48:8.d = temp0_76
v7_48:0xc.d = temp0_76
int128_t v3_109 = v3_108 ^ v4_83
int128_t v1_90 = (v4_83 & not.o(v1_88)) ^ v6_63
int128_t v3_110 = v1_90 ^ v3_109
int128_t v1_91 = v1_90 ^ v16_21
int128_t v0_79 = v3_110 ^ vdupq_laneq_s32(not.o(v0_76), 0)
int128_t v2_98 = v2_96 ^ (v3_109 & v6_63) ^ v3_110
v3_110.d = v1_91.d u>> 5
v3_110:4.d = v1_91:4.d u>> 5
v3_110:8.d = v1_91:8.d u>> 5
v3_110:0xc.d = v1_91:0xc.d u>> 5
v1_91.d <<= 0x1b
v1_91:4.d <<= 0x1b
v1_91:8.d <<= 0x1b
v1_91:0xc.d <<= 0x1b
int128_t v7_49 = v4_83 ^ v7_48
uint128_t v1_92 = vorrq_s8(v1_91, v3_110)
v3_110.d = v0_79.d << 7
v3_110:4.d = v0_79:4.d << 7
v3_110:8.d = v0_79:8.d << 7
v3_110:0xc.d = v0_79:0xc.d << 7
v4_83.d = v7_49.d u>> 7
v4_83:4.d = v7_49:4.d u>> 7
v4_83:8.d = v7_49:8.d u>> 7
v4_83:0xc.d = v7_49:0xc.d u>> 7
v16_21.d = v7_49.d << 0x19
v16_21:4.d = v7_49:4.d << 0x19
v16_21:8.d = v7_49:8.d << 0x19
v16_21:0xc.d = v7_49:0xc.d << 0x19
uint128_t v1_93 = v0_79 ^ v7_49 ^ v1_92
uint128_t v7_50
v7_50.d = v0_79.d u>> 1
v7_50:4.d = v0_79:4.d u>> 1
v7_50:8.d = v0_79:8.d u>> 1
v7_50:0xc.d = v0_79:0xc.d u>> 1
v0_79.d <<= 0x1f
v0_79:4.d <<= 0x1f
v0_79:8.d <<= 0x1f
v0_79:0xc.d <<= 0x1f
uint128_t v0_80 = vorrq_s8(v0_79, v7_50)
v7_50.d = v2_98.d u>> 0x16
v7_50:4.d = v2_98:4.d u>> 0x16
v7_50:8.d = v2_98:8.d u>> 0x16
v7_50:0xc.d = v2_98:0xc.d u>> 0x16
v2_98.d <<= 0xa
v2_98:4.d <<= 0xa
v2_98:8.d <<= 0xa
v2_98:0xc.d <<= 0xa
uint128_t v4_84 = vorrq_s8(v16_21, v4_83)
uint128_t v2_99 = vorrq_s8(v2_98, v7_50)
v7_50.d = v1_93.d << 3
v7_50:4.d = v1_93:4.d << 3
v7_50:8.d = v1_93:8.d << 3
v7_50:0xc.d = v1_93:0xc.d << 3
uint128_t v0_81 = v1_93 ^ v0_80
uint128_t v2_100 = v3_110 ^ v7_49 ^ v2_99
int128_t v3_111
v3_111.d = v1_93.d u>> 0xd
v3_111:4.d = v1_93:4.d u>> 0xd
v3_111:8.d = v1_93:8.d u>> 0xd
v3_111:0xc.d = v1_93:0xc.d u>> 0xd
v1_93.d <<= 0x13
v1_93:4.d <<= 0x13
v1_93:8.d <<= 0x13
v1_93:0xc.d <<= 0x13
uint128_t v1_94 = vorrq_s8(v1_93, v3_111)
uint128_t v3_112 = v7_50 ^ v4_84 ^ v2_100
uint128_t v0_82 = v0_81 ^ v2_100
uint128_t v4_85
v4_85.d = v2_100.d u>> 3
v4_85:4.d = v2_100:4.d u>> 3
v4_85:8.d = v2_100:8.d u>> 3
v4_85:0xc.d = v2_100:0xc.d u>> 3
v2_100.d <<= 0x1d
v2_100:4.d <<= 0x1d
v2_100:8.d <<= 0x1d
v2_100:0xc.d <<= 0x1d
uint128_t v2_101 = vorrq_s8(v2_100, v4_85)
int32_t temp0_77 = x8_1[0x30]
v5_70.d = temp0_77
v5_70:4.d = temp0_77
v5_70:8.d = temp0_77
v5_70:0xc.d = temp0_77
int32_t temp0_78 = x8_1[0x32]
v16_21.d = temp0_78
v16_21:4.d = temp0_78
v16_21:8.d = temp0_78
v16_21:0xc.d = temp0_78
uint128_t v4_87 = (v3_112 & v2_101) ^ v0_82
uint128_t v0_83 = vorrq_s8(v3_112, v0_82)
int32_t temp0_79 = x8_1[0x33]
int128_t v6_64
v6_64.d = temp0_79
v6_64:4.d = temp0_79
v6_64:8.d = temp0_79
v6_64:0xc.d = temp0_79
uint128_t v0_86 = (v0_83 & v1_94) ^ v2_101 ^ v4_87
uint128_t v3_113 = v0_86 ^ v3_112
int128_t v6_65 = v0_86 ^ v6_64
uint128_t v2_103 = v3_113 ^ (v4_87 & v1_94)
uint128_t v1_95 = v3_113 & not.o(v1_94)
v3_113.d = v6_65.d u>> 7
v3_113:4.d = v6_65:4.d u>> 7
v3_113:8.d = v6_65:8.d u>> 7
v3_113:0xc.d = v6_65:0xc.d u>> 7
uint128_t v7_52 = v2_103 ^ not.o(v1_94)
uint128_t v2_104 = v2_103 ^ v16_21
v16_21.d = v6_65.d << 0x19
v16_21:4.d = v6_65:4.d << 0x19
v16_21:8.d = v6_65:8.d << 0x19
v16_21:0xc.d = v6_65:0xc.d << 0x19
uint128_t v3_114 = vorrq_s8(v16_21, v3_113)
int32_t temp0_80 = x8_1[0x31]
v16_21.d = temp0_80
v16_21:4.d = temp0_80
v16_21:8.d = temp0_80
v16_21:0xc.d = temp0_80
uint128_t v1_96 = v1_95 ^ v4_87
int128_t v5_71 = v7_52 ^ v5_70
uint128_t v1_97 = v1_96 ^ v16_21
uint128_t v4_88
v4_88.d = v5_71.d u>> 5
v4_88:4.d = v5_71:4.d u>> 5
v4_88:8.d = v5_71:8.d u>> 5
v4_88:0xc.d = v5_71:0xc.d u>> 5
v5_71.d <<= 0x1b
v5_71:4.d <<= 0x1b
v5_71:8.d <<= 0x1b
v5_71:0xc.d <<= 0x1b
uint128_t v7_53
v7_53.d = v1_97.d << 7
v7_53:4.d = v1_97:4.d << 7
v7_53:8.d = v1_97:8.d << 7
v7_53:0xc.d = v1_97:0xc.d << 7
uint128_t v5_72 = v1_97 ^ v6_65
uint128_t v0_89 = v2_104 ^ vorrq_s8((v7_52 & v4_87) ^ v0_86, v7_52 ^ v1_96)
int128_t v6_66 = v7_53 ^ v6_65
v7_53.d = v1_97.d u>> 1
v7_53:4.d = v1_97:4.d u>> 1
v7_53:8.d = v1_97:8.d u>> 1
v7_53:0xc.d = v1_97:0xc.d u>> 1
v1_97.d <<= 0x1f
v1_97:4.d <<= 0x1f
v1_97:8.d <<= 0x1f
v1_97:0xc.d <<= 0x1f
uint128_t v4_90 = v5_72 ^ vorrq_s8(v5_71, v4_88)
v5_72.d = v0_89.d u>> 0x16
v5_72:4.d = v0_89:4.d u>> 0x16
v5_72:8.d = v0_89:8.d u>> 0x16
v5_72:0xc.d = v0_89:0xc.d u>> 0x16
v0_89.d <<= 0xa
v0_89:4.d <<= 0xa
v0_89:8.d <<= 0xa
v0_89:0xc.d <<= 0xa
uint128_t v1_98 = vorrq_s8(v1_97, v7_53)
uint128_t v0_90 = vorrq_s8(v0_89, v5_72)
v5_72.d = v4_90.d << 3
v5_72:4.d = v4_90:4.d << 3
v5_72:8.d = v4_90:8.d << 3
v5_72:0xc.d = v4_90:0xc.d << 3
uint128_t v1_99 = v4_90 ^ v1_98
uint128_t v3_115 = v5_72 ^ v3_114
v5_72.d = v4_90.d u>> 0xd
v5_72:4.d = v4_90:4.d u>> 0xd
v5_72:8.d = v4_90:8.d u>> 0xd
v5_72:0xc.d = v4_90:0xc.d u>> 0xd
v4_90.d <<= 0x13
v4_90:4.d <<= 0x13
v4_90:8.d <<= 0x13
v4_90:0xc.d <<= 0x13
uint128_t v0_91 = v6_66 ^ v0_90
uint128_t v4_91 = vorrq_s8(v4_90, v5_72)
uint128_t v3_116 = v3_115 ^ v0_91
uint128_t v1_100 = v1_99 ^ v0_91
v6_66.d = v0_91.d u>> 3
v6_66:4.d = v0_91:4.d u>> 3
v6_66:8.d = v0_91:8.d u>> 3
v6_66:0xc.d = v0_91:0xc.d u>> 3
v0_91.d <<= 0x1d
v0_91:4.d <<= 0x1d
v0_91:8.d <<= 0x1d
v0_91:0xc.d <<= 0x1d
uint128_t v0_92 = vorrq_s8(v0_91, v6_66)
uint128_t v6_67 = v1_100 ^ v0_92
int32_t temp0_81 = x8_1[0x2d]
v7_53.d = temp0_81
v7_53:4.d = temp0_81
v7_53:8.d = temp0_81
v7_53:0xc.d = temp0_81
uint128_t v4_92 = v6_67 ^ v4_91
uint128_t v17_8 = (v6_67 & v0_92) ^ v4_92
uint128_t v0_94 = v4_92 & v1_100
int32_t temp0_82 = x8_1[0x2f]
v1_100.d = temp0_82
v1_100:4.d = temp0_82
v1_100:8.d = temp0_82
v1_100:0xc.d = temp0_82
int32_t temp0_83 = x8_1[0x2c]
v16_21.d = temp0_83
v16_21:4.d = temp0_83
v16_21:8.d = temp0_83
v16_21:0xc.d = temp0_83
int32_t temp0_84 = x8_1[0x2e]
v2_104.d = temp0_84
v2_104:4.d = temp0_84
v2_104:8.d = temp0_84
v2_104:0xc.d = temp0_84
uint128_t v3_117 = vorrq_s8(v17_8, v3_116)
uint128_t v4_93 = v3_115 ^ v1_99 ^ v17_8
uint128_t v5_74 = v3_117 ^ v6_67
uint128_t v0_95 = v3_117 ^ v0_94
uint128_t v3_119 = (v5_74 & v3_117) ^ v4_93
uint128_t v6_68 = v5_74 ^ v16_21
uint128_t v5_76 = vorrq_s8(v0_95 ^ v4_93, v5_74) ^ v17_8
v16_21.d = v6_68.d u>> 5
v16_21:4.d = v6_68:4.d u>> 5
v16_21:8.d = v6_68:8.d u>> 5
v16_21:0xc.d = v6_68:0xc.d u>> 5
v6_68.d <<= 0x1b
v6_68:4.d <<= 0x1b
v6_68:8.d <<= 0x1b
v6_68:0xc.d <<= 0x1b
uint128_t v2_105 = v3_119 ^ v2_104
uint128_t v3_120 = vorrq_s8(v6_68, v16_21)
uint128_t v6_69 = v5_76 ^ v7_53
uint128_t v1_103 = v0_95 ^ v1_100 ^ v3_119 ^ v5_76
v5_76.d = v2_105.d u>> 0x16
v5_76:4.d = v2_105:4.d u>> 0x16
v5_76:8.d = v2_105:8.d u>> 0x16
v5_76:0xc.d = v2_105:0xc.d u>> 0x16
v2_105.d <<= 0xa
v2_105:4.d <<= 0xa
v2_105:8.d <<= 0xa
v2_105:0xc.d <<= 0xa
uint128_t v2_106 = vorrq_s8(v2_105, v5_76)
v5_76.d = v1_103.d u>> 7
v5_76:4.d = v1_103:4.d u>> 7
v5_76:8.d = v1_103:8.d u>> 7
v5_76:0xc.d = v1_103:0xc.d u>> 7
v7_53.d = v1_103.d << 0x19
v7_53:4.d = v1_103:4.d << 0x19
v7_53:8.d = v1_103:8.d << 0x19
v7_53:0xc.d = v1_103:0xc.d << 0x19
v16_21.d = v6_69.d u>> 1
v16_21:4.d = v6_69:4.d u>> 1
v16_21:8.d = v6_69:8.d u>> 1
v16_21:0xc.d = v6_69:0xc.d u>> 1
uint128_t v2_107 = v1_103 ^ v2_106
uint128_t v1_104 = v6_69 ^ v3_120 ^ v1_103
uint128_t v3_121
v3_121.d = v6_69.d << 0x1f
v3_121:4.d = v6_69:4.d << 0x1f
v3_121:8.d = v6_69:8.d << 0x1f
v3_121:0xc.d = v6_69:0xc.d << 0x1f
uint128_t v3_122 = vorrq_s8(v3_121, v16_21)
v6_69.d <<= 7
v6_69:4.d <<= 7
v6_69:8.d <<= 7
v6_69:0xc.d <<= 7
uint128_t v5_77 = vorrq_s8(v7_53, v5_76)
uint128_t v6_70 = v2_107 ^ v6_69
uint128_t v3_123 = v1_104 ^ v3_122
v7_53.d = v1_104.d u>> 0xd
v7_53:4.d = v1_104:4.d u>> 0xd
v7_53:8.d = v1_104:8.d u>> 0xd
v7_53:0xc.d = v1_104:0xc.d u>> 0xd
v16_21.d = v1_104.d << 0x13
v16_21:4.d = v1_104:4.d << 0x13
v16_21:8.d = v1_104:8.d << 0x13
v16_21:0xc.d = v1_104:0xc.d << 0x13
v1_104.d <<= 3
v1_104:4.d <<= 3
v1_104:8.d <<= 3
v1_104:0xc.d <<= 3
uint128_t v5_78 = v6_70 ^ v5_77
v2_107.d <<= 0x1d
v2_107:4.d <<= 0x1d
v2_107:8.d <<= 0x1d
v2_107:0xc.d <<= 0x1d
uint128_t v3_124 = v3_123 ^ v6_70
v6_70.d u>>= 3
v6_70:4.d u>>= 3
v6_70:8.d u>>= 3
v6_70:0xc.d u>>= 3
uint128_t v7_54 = vorrq_s8(v16_21, v7_53)
uint128_t v1_105 = v5_78 ^ v1_104
uint128_t v2_108 = vorrq_s8(v2_107, v6_70)
int32_t temp0_85 = x8_1[0x28]
v4_93.d = temp0_85
v4_93:4.d = temp0_85
v4_93:8.d = temp0_85
v4_93:0xc.d = temp0_85
int32_t temp0_86 = x8_1[0x29]
uint128_t v0_96
v0_96.d = temp0_86
v0_96:4.d = temp0_86
v0_96:8.d = temp0_86
v0_96:0xc.d = temp0_86
uint128_t v2_109 = v1_105 ^ v2_108
uint128_t v1_106 = v1_105 ^ v7_54
int32_t temp0_87 = x8_1[0x2a]
v16_21.d = temp0_87
v16_21:4.d = temp0_87
v16_21:8.d = temp0_87
v16_21:0xc.d = temp0_87
int32_t temp0_88 = x8_1[0x2b]
v5_78.d = temp0_88
v5_78:4.d = temp0_88
v5_78:8.d = temp0_88
v5_78:0xc.d = temp0_88
uint128_t v6_72 = (v2_109 & v1_106) ^ v3_124
uint128_t v3_126 = vorrq_s8(v2_109, v3_124) ^ v1_106
uint128_t v2_110 = v6_72 ^ v2_109
uint128_t v6_73 = not.o(v6_72)
uint128_t v17_9 = (v1_106 & v7_54 & v6_72) ^ v2_110
uint128_t v4_94 = v3_126 ^ v4_93
uint128_t v6_74
v6_74.d = v4_94.d u>> 5
v6_74:4.d = v4_94:4.d u>> 5
v6_74:8.d = v4_94:8.d u>> 5
v6_74:0xc.d = v4_94:0xc.d u>> 5
v4_94.d <<= 0x1b
v4_94:4.d <<= 0x1b
v4_94:8.d <<= 0x1b
v4_94:0xc.d <<= 0x1b
uint128_t v0_97 = v17_9 ^ v0_96
uint128_t v2_112 = v16_21 ^ v6_73 ^ vorrq_s8(v2_110 & v3_126, v7_54)
uint128_t v5_79
v5_79.d = v0_97.d << 7
v5_79:4.d = v0_97:4.d << 7
v5_79:8.d = v0_97:8.d << 7
v5_79:0xc.d = v0_97:0xc.d << 7
uint128_t v4_96 = v0_97 ^ vorrq_s8(v4_94, v6_74)
v16_21.d = v0_97.d u>> 1
v16_21:4.d = v0_97:4.d u>> 1
v16_21:8.d = v0_97:8.d u>> 1
v16_21:0xc.d = v0_97:0xc.d u>> 1
v0_97.d <<= 0x1f
v0_97:4.d <<= 0x1f
v0_97:8.d <<= 0x1f
v0_97:0xc.d <<= 0x1f
uint128_t v3_129 = v5_78 ^ v6_73 ^ ((v7_54 ^ v6_73) & v3_126) ^ v17_9
uint128_t v0_98 = vorrq_s8(v0_97, v16_21)
v16_21.d = v2_112.d u>> 0x16
v16_21:4.d = v2_112:4.d u>> 0x16
v16_21:8.d = v2_112:8.d u>> 0x16
v16_21:0xc.d = v2_112:0xc.d u>> 0x16
v2_112.d <<= 0xa
v2_112:4.d <<= 0xa
v2_112:8.d <<= 0xa
v2_112:0xc.d <<= 0xa
uint128_t v2_113 = vorrq_s8(v2_112, v16_21)
uint128_t v5_80 = v3_129 ^ v5_79
uint128_t v4_97 = v4_96 ^ v3_129
v16_21.d = v3_129.d u>> 7
v16_21:4.d = v3_129:4.d u>> 7
v16_21:8.d = v3_129:8.d u>> 7
v16_21:0xc.d = v3_129:0xc.d u>> 7
v3_129.d <<= 0x19
v3_129:4.d <<= 0x19
v3_129:8.d <<= 0x19
v3_129:0xc.d <<= 0x19
uint128_t v2_114 = v5_80 ^ v2_113
v5_80.d = v4_97.d << 3
v5_80:4.d = v4_97:4.d << 3
v5_80:8.d = v4_97:8.d << 3
v5_80:0xc.d = v4_97:0xc.d << 3
uint128_t v0_99 = v4_97 ^ v0_98
uint128_t v3_131 = v2_114 ^ vorrq_s8(v3_129, v16_21)
v16_21.d = v4_97.d u>> 0xd
v16_21:4.d = v4_97:4.d u>> 0xd
v16_21:8.d = v4_97:8.d u>> 0xd
v16_21:0xc.d = v4_97:0xc.d u>> 0xd
v4_97.d <<= 0x13
v4_97:4.d <<= 0x13
v4_97:8.d <<= 0x13
v4_97:0xc.d <<= 0x13
uint128_t v0_100 = v0_99 ^ v2_114
uint128_t v3_132 = v3_131 ^ v5_80
uint128_t v4_98 = vorrq_s8(v4_97, v16_21)
v16_21.d = v2_114.d u>> 3
v16_21:4.d = v2_114:4.d u>> 3
v16_21:8.d = v2_114:8.d u>> 3
v16_21:0xc.d = v2_114:0xc.d u>> 3
v2_114.d <<= 0x1d
v2_114:4.d <<= 0x1d
v2_114:8.d <<= 0x1d
v2_114:0xc.d <<= 0x1d
uint128_t v5_81 = v3_132 ^ v0_100
int32_t temp0_89 = x8_1[0x24]
uint128_t v1_109
v1_109.d = temp0_89
v1_109:4.d = temp0_89
v1_109:8.d = temp0_89
v1_109:0xc.d = temp0_89
uint128_t v2_115 = vorrq_s8(v2_114, v16_21)
int32_t temp0_90 = x8_1[0x26]
uint128_t v7_55
v7_55.d = temp0_90
v7_55:4.d = temp0_90
v7_55:8.d = temp0_90
v7_55:0xc.d = temp0_90
uint128_t v0_101 = v0_100 ^ v2_115
uint128_t v16_22 = vorrq_s8(v5_81, v4_98)
uint128_t v3_134 = (v5_81 & v3_132) ^ v4_98
int32_t temp0_91 = x8_1[0x25]
v6_74.d = temp0_91
v6_74:4.d = temp0_91
v6_74:8.d = temp0_91
v6_74:0xc.d = temp0_91
uint128_t v2_116 = v3_134 ^ v2_115
uint128_t v4_100 = vorrq_s8(v2_116, v16_22 ^ v0_101) ^ v3_134 ^ v5_81
uint128_t v5_82 = vorrq_s8(v3_134, v5_81)
uint128_t v3_135 = v3_134 ^ v7_55
int32_t temp0_92 = x8_1[0x27]
v7_55.d = temp0_92
v7_55:4.d = temp0_92
v7_55:8.d = temp0_92
v7_55:0xc.d = temp0_92
uint128_t v6_75 = v4_100 ^ v6_74
uint128_t v0_103 = v4_100 ^ v0_101 ^ v5_82
int128_t v3_136 = v3_135 ^ ((v5_82 & not.o(v4_100)) | not.o(v0_103))
uint128_t v2_117 = v2_116 ^ v7_55
uint128_t v0_105 = v1_109 ^ not.o(v0_103)
int128_t v4_102
v4_102.d = v3_136.d u>> 0x16
v4_102:4.d = v3_136:4.d u>> 0x16
v4_102:8.d = v3_136:8.d u>> 0x16
v4_102:0xc.d = v3_136:0xc.d u>> 0x16
v3_136.d <<= 0xa
v3_136:4.d <<= 0xa
v3_136:8.d <<= 0xa
v3_136:0xc.d <<= 0xa
v7_55.d = v2_117.d u>> 7
v7_55:4.d = v2_117:4.d u>> 7
v7_55:8.d = v2_117:8.d u>> 7
v7_55:0xc.d = v2_117:0xc.d u>> 7
uint128_t v16_24
v16_24.d = v2_117.d << 0x19
v16_24:4.d = v2_117:4.d << 0x19
v16_24:8.d = v2_117:8.d << 0x19
v16_24:0xc.d = v2_117:0xc.d << 0x19
uint128_t v3_137 = vorrq_s8(v3_136, v4_102)
v4_102.d = v0_105.d u>> 5
v4_102:4.d = v0_105:4.d u>> 5
v4_102:8.d = v0_105:8.d u>> 5
v4_102:0xc.d = v0_105:0xc.d u>> 5
v0_105.d <<= 0x1b
v0_105:4.d <<= 0x1b
v0_105:8.d <<= 0x1b
v0_105:0xc.d <<= 0x1b
uint128_t v7_56 = vorrq_s8(v16_24, v7_55)
v5_82.d = v6_75.d << 7
v5_82:4.d = v6_75:4.d << 7
v5_82:8.d = v6_75:8.d << 7
v5_82:0xc.d = v6_75:0xc.d << 7
uint128_t v2_118 = v5_82 ^ v2_117
uint128_t v0_107 = v6_75 ^ v2_117 ^ vorrq_s8(v0_105, v4_102)
v5_82.d = v6_75.d u>> 1
v5_82:4.d = v6_75:4.d u>> 1
v5_82:8.d = v6_75:8.d u>> 1
v5_82:0xc.d = v6_75:0xc.d u>> 1
v6_75.d <<= 0x1f
v6_75:4.d <<= 0x1f
v6_75:8.d <<= 0x1f
v6_75:0xc.d <<= 0x1f
uint128_t v2_119 = v2_118 ^ v3_137
v3_137.d = v0_107.d << 3
v3_137:4.d = v0_107:4.d << 3
v3_137:8.d = v0_107:8.d << 3
v3_137:0xc.d = v0_107:0xc.d << 3
uint128_t v16_25
v16_25.d = v2_119.d u>> 3
v16_25:4.d = v2_119:4.d u>> 3
v16_25:8.d = v2_119:8.d u>> 3
v16_25:0xc.d = v2_119:0xc.d u>> 3
uint128_t v3_138 = v3_137 ^ v7_56
v7_56.d = v2_119.d << 0x1d
v7_56:4.d = v2_119:4.d << 0x1d
v7_56:8.d = v2_119:8.d << 0x1d
v7_56:0xc.d = v2_119:0xc.d << 0x1d
uint128_t v5_84 = v0_107 ^ vorrq_s8(v6_75, v5_82)
uint128_t v7_57 = vorrq_s8(v7_56, v16_25)
v16_25.d = v0_107.d u>> 0xd
v16_25:4.d = v0_107:4.d u>> 0xd
v16_25:8.d = v0_107:8.d u>> 0xd
v16_25:0xc.d = v0_107:0xc.d u>> 0xd
v0_107.d <<= 0x13
v0_107:4.d <<= 0x13
v0_107:8.d <<= 0x13
v0_107:0xc.d <<= 0x13
uint128_t v5_85 = v5_84 ^ v2_119
uint128_t v0_108 = vorrq_s8(v0_107, v16_25)
int32_t temp0_93 = x8_1[0x20]
v6_75.d = temp0_93
v6_75:4.d = temp0_93
v6_75:8.d = temp0_93
v6_75:0xc.d = temp0_93
uint128_t v2_120 = v3_138 ^ v2_119
uint128_t v16_26 = vorrq_s8(v5_85, v0_108)
uint128_t v5_86 = not.o(v5_85)
int32_t temp0_94 = x8_1[0x22]
v4_102.d = temp0_94
v4_102:4.d = temp0_94
v4_102:8.d = temp0_94
v4_102:0xc.d = temp0_94
uint128_t v0_109 = v0_108 ^ v5_86
int32_t temp0_95 = x8_1[0x21]
v1_109.d = temp0_95
v1_109:4.d = temp0_95
v1_109:8.d = temp0_95
v1_109:0xc.d = temp0_95
uint128_t v3_141 = v2_120 ^ not.o(v7_57) ^ v16_26
uint128_t v7_59 = (v5_86 | not.o(v7_57)) ^ v0_109
uint128_t v0_110 = v0_109 & v2_120
int32_t temp0_96 = x8_1[0x23]
v16_26.d = temp0_96
v16_26:4.d = temp0_96
v16_26:8.d = temp0_96
v16_26:0xc.d = temp0_96
uint128_t v5_87 = v0_110 ^ v5_86
uint128_t v17_10 = vorrq_s8(v3_141, v0_110) ^ v7_59
uint128_t v0_112 = v3_141 ^ v4_102
uint128_t v2_122 = v5_87 ^ v2_120 ^ v3_141
uint128_t v4_103 = v17_10 ^ v6_75
uint128_t v7_60 = v3_141 ^ v7_59
v3_141.d = v0_112.d u>> 0x16
v3_141:4.d = v0_112:4.d u>> 0x16
v3_141:8.d = v0_112:8.d u>> 0x16
v3_141:0xc.d = v0_112:0xc.d u>> 0x16
v0_112.d <<= 0xa
v0_112:4.d <<= 0xa
v0_112:8.d <<= 0xa
v0_112:0xc.d <<= 0xa
uint128_t v2_123 = v2_122 ^ v17_10
v6_75.d = v4_103.d u>> 5
v6_75:4.d = v4_103:4.d u>> 5
v6_75:8.d = v4_103:8.d u>> 5
v6_75:0xc.d = v4_103:0xc.d u>> 5
v4_103.d <<= 0x1b
v4_103:4.d <<= 0x1b
v4_103:8.d <<= 0x1b
v4_103:0xc.d <<= 0x1b
uint128_t v2_124 = v2_123 ^ v16_26
uint128_t v1_111 = v5_87 ^ v1_109 ^ (v2_123 & v7_60)
uint128_t v3_143 = v2_124 ^ vorrq_s8(v0_112, v3_141)
uint128_t v4_105 = v2_124 ^ vorrq_s8(v4_103, v6_75)
v6_75.d = v2_124.d u>> 7
v6_75:4.d = v2_124:4.d u>> 7
v6_75:8.d = v2_124:8.d u>> 7
v6_75:0xc.d = v2_124:0xc.d u>> 7
v2_124.d <<= 0x19
v2_124:4.d <<= 0x19
v2_124:8.d <<= 0x19
v2_124:0xc.d <<= 0x19
uint128_t v4_106 = v4_105 ^ v1_111
uint128_t v2_125 = vorrq_s8(v2_124, v6_75)
v6_75.d = v1_111.d u>> 1
v6_75:4.d = v1_111:4.d u>> 1
v6_75:8.d = v1_111:8.d u>> 1
v6_75:0xc.d = v1_111:0xc.d u>> 1
uint128_t v7_61
v7_61.d = v1_111.d << 0x1f
v7_61:4.d = v1_111:4.d << 0x1f
v7_61:8.d = v1_111:8.d << 0x1f
v7_61:0xc.d = v1_111:0xc.d << 0x1f
v1_111.d <<= 7
v1_111:4.d <<= 7
v1_111:8.d <<= 7
v1_111:0xc.d <<= 7
uint128_t v6_76 = vorrq_s8(v7_61, v6_75)
uint128_t v1_112 = v3_143 ^ v1_111
v7_61.d = v4_106.d u>> 0xd
v7_61:4.d = v4_106:4.d u>> 0xd
v7_61:8.d = v4_106:8.d u>> 0xd
v7_61:0xc.d = v4_106:0xc.d u>> 0xd
v16_26.d = v4_106.d << 0x13
v16_26:4.d = v4_106:4.d << 0x13
v16_26:8.d = v4_106:8.d << 0x13
v16_26:0xc.d = v4_106:0xc.d << 0x13
uint128_t v6_77 = v4_106 ^ v6_76
v4_106.d <<= 3
v4_106:4.d <<= 3
v4_106:8.d <<= 3
v4_106:0xc.d <<= 3
v3_143.d <<= 0x1d
v3_143:4.d <<= 0x1d
v3_143:8.d <<= 0x1d
v3_143:0xc.d <<= 0x1d
uint128_t v2_127 = v1_112 ^ v2_125 ^ v4_106
v4_106.d = v1_112.d u>> 3
v4_106:4.d = v1_112:4.d u>> 3
v4_106:8.d = v1_112:8.d u>> 3
v4_106:0xc.d = v1_112:0xc.d u>> 3
uint128_t v7_62 = vorrq_s8(v16_26, v7_61)
uint128_t v3_144 = vorrq_s8(v3_143, v4_106)
uint128_t v1_113 = v6_77 ^ v1_112
uint128_t v6_78 = vorrq_s8(v2_127, v3_144)
uint128_t v3_145 = v7_62 ^ v3_144
int32_t temp0_97 = x8_1[0x1c]
v5_87.d = temp0_97
v5_87:4.d = temp0_97
v5_87:8.d = temp0_97
v5_87:0xc.d = temp0_97
uint128_t v4_107 = v2_127 & v7_62
int32_t temp0_98 = x8_1[0x1e]
v16_26.d = temp0_98
v16_26:4.d = temp0_98
v16_26:8.d = temp0_98
v16_26:0xc.d = temp0_98
uint128_t v1_114 = vorrq_s8(v4_107, v1_113)
uint128_t v3_146 = v6_78 & not.o(v3_145)
uint128_t v4_108 = v4_107 ^ not.o(v3_145)
int32_t temp0_99 = x8_1[0x1d]
v0_112.d = temp0_99
v0_112:4.d = temp0_99
v0_112:8.d = temp0_99
v0_112:0xc.d = temp0_99
uint128_t v1_115 = v3_146 ^ v1_114
uint128_t v7_64 = v4_108 ^ v3_146
uint128_t v3_147 = vorrq_s8(v4_108, v3_146)
int32_t temp0_100 = x8_1[0x1f]
v4_108.d = temp0_100
v4_108:4.d = temp0_100
v4_108:8.d = temp0_100
v4_108:0xc.d = temp0_100
uint128_t v2_129 = (v1_113 ^ v2_127) & v6_78
uint128_t v6_79 = v1_115 ^ v6_78
uint128_t v1_116 = v1_115 ^ v16_26
uint128_t v2_130 = v3_147 ^ v2_129
uint128_t v5_90 = v2_129 ^ v5_87 ^ v7_64 ^ v6_79
uint128_t v6_80 = vorrq_s8(v2_130, v6_79)
uint128_t v2_131 = v2_130 ^ v0_112
v0_112.d = v1_116.d u>> 0x16
v0_112:4.d = v1_116:4.d u>> 0x16
v0_112:8.d = v1_116:8.d u>> 0x16
v0_112:0xc.d = v1_116:0xc.d u>> 0x16
v1_116.d <<= 0xa
v1_116:4.d <<= 0xa
v1_116:8.d <<= 0xa
v1_116:0xc.d <<= 0xa
uint128_t v0_113 = vorrq_s8(v1_116, v0_112)
uint128_t v1_117 = v7_64 ^ v4_108 ^ v6_80
uint128_t v4_109
v4_109.d = v5_90.d u>> 5
v4_109:4.d = v5_90:4.d u>> 5
v4_109:8.d = v5_90:8.d u>> 5
v4_109:0xc.d = v5_90:0xc.d u>> 5
v5_90.d <<= 0x1b
v5_90:4.d <<= 0x1b
v5_90:8.d <<= 0x1b
v5_90:0xc.d <<= 0x1b
v6_80.d = v2_131.d << 7
v6_80:4.d = v2_131:4.d << 7
v6_80:8.d = v2_131:8.d << 7
v6_80:0xc.d = v2_131:0xc.d << 7
uint128_t v4_110 = vorrq_s8(v5_90, v4_109)
v5_90.d = v2_131.d u>> 1
v5_90:4.d = v2_131:4.d u>> 1
v5_90:8.d = v2_131:8.d u>> 1
v5_90:0xc.d = v2_131:0xc.d u>> 1
uint128_t v6_81 = v6_80 ^ v0_113
v0_113.d = v2_131.d << 0x1f
v0_113:4.d = v2_131:4.d << 0x1f
v0_113:8.d = v2_131:8.d << 0x1f
v0_113:0xc.d = v2_131:0xc.d << 0x1f
uint128_t v2_133 = v1_117 ^ v2_131 ^ v4_110
v4_110.d = v1_117.d u>> 7
v4_110:4.d = v1_117:4.d u>> 7
v4_110:8.d = v1_117:8.d u>> 7
v4_110:0xc.d = v1_117:0xc.d u>> 7
uint128_t v6_82 = v6_81 ^ v1_117
v1_117.d <<= 0x19
v1_117:4.d <<= 0x19
v1_117:8.d <<= 0x19
v1_117:0xc.d <<= 0x19
uint128_t v5_91 = vorrq_s8(v0_113, v5_90)
uint128_t v1_118 = vorrq_s8(v1_117, v4_110)
v4_110.d = v2_133.d << 3
v4_110:4.d = v2_133:4.d << 3
v4_110:8.d = v2_133:8.d << 3
v4_110:0xc.d = v2_133:0xc.d << 3
int32_t temp0_101 = x8_1[0x19]
v16_26.d = temp0_101
v16_26:4.d = temp0_101
v16_26:8.d = temp0_101
v16_26:0xc.d = temp0_101
uint128_t v1_120 = v6_82 ^ v1_118 ^ v4_110
v4_110.d = v6_82.d u>> 3
v4_110:4.d = v6_82:4.d u>> 3
v4_110:8.d = v6_82:8.d u>> 3
v4_110:0xc.d = v6_82:0xc.d u>> 3
uint128_t v5_93 = v2_133 ^ v5_91 ^ v6_82
v6_82.d <<= 0x1d
v6_82:4.d <<= 0x1d
v6_82:8.d <<= 0x1d
v6_82:0xc.d <<= 0x1d
int32_t temp0_102 = x8_1[0x1a]
v3_147.d = temp0_102
v3_147:4.d = temp0_102
v3_147:8.d = temp0_102
v3_147:0xc.d = temp0_102
uint128_t v4_111 = vorrq_s8(v6_82, v4_110)
v6_82.d = v2_133.d u>> 0xd
v6_82:4.d = v2_133:4.d u>> 0xd
v6_82:8.d = v2_133:8.d u>> 0xd
v6_82:0xc.d = v2_133:0xc.d u>> 0xd
v2_133.d <<= 0x13
v2_133:4.d <<= 0x13
v2_133:8.d <<= 0x13
v2_133:0xc.d <<= 0x13
uint128_t v2_135 = v4_111 ^ vorrq_s8(v2_133, v6_82)
uint128_t v1_121 = v1_120 ^ v5_93
uint128_t v6_84 = vorrq_s8(v1_120 ^ v4_111, v2_135)
uint128_t v4_113 = (v2_135 & v4_111) ^ v1_121
int32_t temp0_103 = x8_1[0x1b]
v7_64.d = temp0_103
v7_64:4.d = temp0_103
v7_64:8.d = temp0_103
v7_64:0xc.d = temp0_103
uint128_t v1_122 = v6_84 ^ v1_121
uint128_t v6_85 = not.o(v4_113)
uint128_t v2_136 = v2_135 ^ v6_85
uint128_t v6_86 = v16_26 ^ v6_85
int32_t temp0_104 = x8_1[0x18]
v16_26.d = temp0_104
v16_26:4.d = temp0_104
v16_26:8.d = temp0_104
v16_26:0xc.d = temp0_104
uint128_t v5_95 = (v1_122 & v5_93) ^ v2_136
uint128_t v1_124 = (v1_122 ^ v2_136) | not.o(v4_113)
v4_113.d = v6_86.d u>> 1
v4_113:4.d = v6_86:4.d u>> 1
v4_113:8.d = v6_86:8.d u>> 1
v4_113:0xc.d = v6_86:0xc.d u>> 1
uint128_t v16_27 = v5_95 ^ v16_26
uint128_t v5_96 = v1_122 ^ v7_64 ^ v5_95
uint128_t v7_65
v7_65.d = v6_86.d << 0x1f
v7_65:4.d = v6_86:4.d << 0x1f
v7_65:8.d = v6_86:8.d << 0x1f
v7_65:0xc.d = v6_86:0xc.d << 0x1f
uint128_t v4_114 = vorrq_s8(v7_65, v4_113)
v7_65.d = v6_86.d << 7
v7_65:4.d = v6_86:4.d << 7
v7_65:8.d = v6_86:8.d << 7
v7_65:0xc.d = v6_86:0xc.d << 7
uint128_t v1_125 = v5_93 ^ v3_147 ^ v6_84 ^ v1_124
uint128_t v3_149
v3_149.d = v16_27.d u>> 5
v3_149:4.d = v16_27:4.d u>> 5
v3_149:8.d = v16_27:8.d u>> 5
v3_149:0xc.d = v16_27:0xc.d u>> 5
v16_27.d <<= 0x1b
v16_27:4.d <<= 0x1b
v16_27:8.d <<= 0x1b
v16_27:0xc.d <<= 0x1b
uint128_t v7_66 = v5_96 ^ v7_65
uint128_t v6_87 = v5_96 ^ v6_86
uint128_t v3_150 = vorrq_s8(v16_27, v3_149)
v16_27.d = v5_96.d u>> 7
v16_27:4.d = v5_96:4.d u>> 7
v16_27:8.d = v5_96:8.d u>> 7
v16_27:0xc.d = v5_96:0xc.d u>> 7
v5_96.d <<= 0x19
v5_96:4.d <<= 0x19
v5_96:8.d <<= 0x19
v5_96:0xc.d <<= 0x19
uint128_t v5_97 = vorrq_s8(v5_96, v16_27)
v16_27.d = v1_125.d u>> 0x16
v16_27:4.d = v1_125:4.d u>> 0x16
v16_27:8.d = v1_125:8.d u>> 0x16
v16_27:0xc.d = v1_125:0xc.d u>> 0x16
v1_125.d <<= 0xa
v1_125:4.d <<= 0xa
v1_125:8.d <<= 0xa
v1_125:0xc.d <<= 0xa
uint128_t v3_151 = v6_87 ^ v3_150
uint128_t v1_127 = v7_66 ^ vorrq_s8(v1_125, v16_27)
v6_87.d = v3_151.d << 3
v6_87:4.d = v3_151:4.d << 3
v6_87:8.d = v3_151:8.d << 3
v6_87:0xc.d = v3_151:0xc.d << 3
v7_66.d = v3_151.d u>> 0xd
v7_66:4.d = v3_151:4.d u>> 0xd
v7_66:8.d = v3_151:8.d u>> 0xd
v7_66:0xc.d = v3_151:0xc.d u>> 0xd
v16_27.d = v3_151.d << 0x13
v16_27:4.d = v3_151:4.d << 0x13
v16_27:8.d = v3_151:8.d << 0x13
v16_27:0xc.d = v3_151:0xc.d << 0x13
uint128_t v4_115 = v1_127 ^ v5_97
v5_97.d = v1_127.d u>> 3
v5_97:4.d = v1_127:4.d u>> 3
v5_97:8.d = v1_127:8.d u>> 3
v5_97:0xc.d = v1_127:0xc.d u>> 3
uint128_t v7_67 = vorrq_s8(v16_27, v7_66)
v16_27.d = v1_127.d << 0x1d
v16_27:4.d = v1_127:4.d << 0x1d
v16_27:8.d = v1_127:8.d << 0x1d
v16_27:0xc.d = v1_127:0xc.d << 0x1d
uint128_t v1_128 = v4_114 ^ v3_151 ^ v1_127
uint128_t v3_153 = v4_115 ^ v6_87
uint128_t v4_116 = vorrq_s8(v16_27, v5_97)
uint128_t v5_98 = not.o(v1_128)
uint128_t v16_28 = vorrq_s8(v3_153, v7_67)
int32_t temp0_105 = x8_1[0x16]
v2_136.d = temp0_105
v2_136:4.d = temp0_105
v2_136:8.d = temp0_105
v2_136:0xc.d = temp0_105
uint128_t v6_89 = v4_116 ^ v5_98 ^ v16_28
v0_113.d = x8_1[0x15]
uint128_t v3_154 = v6_89 ^ v3_153
int32_t temp0_106 = x8_1[0x14]
v16_28.d = temp0_106
v16_28:4.d = temp0_106
v16_28:8.d = temp0_106
v16_28:0xc.d = temp0_106
uint128_t v4_119 = v3_154 ^ ((v4_116 | not.o(v1_128)) & v7_67)
uint128_t v3_156 = vorrq_s8(v3_154, v7_67) ^ v5_98
uint128_t v2_137 = v7_67 ^ v2_136
int32_t temp0_107 = x8_1[0x17]
v7_67.d = temp0_107
v7_67:4.d = temp0_107
v7_67:8.d = temp0_107
v7_67:0xc.d = temp0_107
uint128_t v3_157 = v3_156 ^ v4_119
uint128_t v1_130 = (v4_119 & not.o(v1_128)) ^ v6_89
uint128_t v3_158 = v1_130 ^ v3_157
uint128_t v1_131 = v1_130 ^ v16_28
uint128_t v0_116 = v3_158 ^ vdupq_laneq_s32(not.o(v0_113), 0)
uint128_t v2_139 = v2_137 ^ (v3_157 & v6_89) ^ v3_158
v3_158.d = v1_131.d u>> 5
v3_158:4.d = v1_131:4.d u>> 5
v3_158:8.d = v1_131:8.d u>> 5
v3_158:0xc.d = v1_131:0xc.d u>> 5
v1_131.d <<= 0x1b
v1_131:4.d <<= 0x1b
v1_131:8.d <<= 0x1b
v1_131:0xc.d <<= 0x1b
uint128_t v7_68 = v4_119 ^ v7_67
uint128_t v1_132 = vorrq_s8(v1_131, v3_158)
v3_158.d = v0_116.d << 7
v3_158:4.d = v0_116:4.d << 7
v3_158:8.d = v0_116:8.d << 7
v3_158:0xc.d = v0_116:0xc.d << 7
v4_119.d = v7_68.d u>> 7
v4_119:4.d = v7_68:4.d u>> 7
v4_119:8.d = v7_68:8.d u>> 7
v4_119:0xc.d = v7_68:0xc.d u>> 7
v16_28.d = v7_68.d << 0x19
v16_28:4.d = v7_68:4.d << 0x19
v16_28:8.d = v7_68:8.d << 0x19
v16_28:0xc.d = v7_68:0xc.d << 0x19
uint128_t v1_133 = v0_116 ^ v7_68 ^ v1_132
uint128_t v7_69
v7_69.d = v0_116.d u>> 1
v7_69:4.d = v0_116:4.d u>> 1
v7_69:8.d = v0_116:8.d u>> 1
v7_69:0xc.d = v0_116:0xc.d u>> 1
v0_116.d <<= 0x1f
v0_116:4.d <<= 0x1f
v0_116:8.d <<= 0x1f
v0_116:0xc.d <<= 0x1f
uint128_t v0_117 = vorrq_s8(v0_116, v7_69)
v7_69.d = v2_139.d u>> 0x16
v7_69:4.d = v2_139:4.d u>> 0x16
v7_69:8.d = v2_139:8.d u>> 0x16
v7_69:0xc.d = v2_139:0xc.d u>> 0x16
v2_139.d <<= 0xa
v2_139:4.d <<= 0xa
v2_139:8.d <<= 0xa
v2_139:0xc.d <<= 0xa
uint128_t v4_120 = vorrq_s8(v16_28, v4_119)
uint128_t v2_140 = vorrq_s8(v2_139, v7_69)
v7_69.d = v1_133.d << 3
v7_69:4.d = v1_133:4.d << 3
v7_69:8.d = v1_133:8.d << 3
v7_69:0xc.d = v1_133:0xc.d << 3
uint128_t v0_118 = v1_133 ^ v0_117
uint128_t v2_141 = v3_158 ^ v7_68 ^ v2_140
uint128_t v3_159
v3_159.d = v1_133.d u>> 0xd
v3_159:4.d = v1_133:4.d u>> 0xd
v3_159:8.d = v1_133:8.d u>> 0xd
v3_159:0xc.d = v1_133:0xc.d u>> 0xd
v1_133.d <<= 0x13
v1_133:4.d <<= 0x13
v1_133:8.d <<= 0x13
v1_133:0xc.d <<= 0x13
uint128_t v1_134 = vorrq_s8(v1_133, v3_159)
uint128_t v3_160 = v7_69 ^ v4_120 ^ v2_141
uint128_t v0_119 = v0_118 ^ v2_141
uint128_t v4_121
v4_121.d = v2_141.d u>> 3
v4_121:4.d = v2_141:4.d u>> 3
v4_121:8.d = v2_141:8.d u>> 3
v4_121:0xc.d = v2_141:0xc.d u>> 3
v2_141.d <<= 0x1d
v2_141:4.d <<= 0x1d
v2_141:8.d <<= 0x1d
v2_141:0xc.d <<= 0x1d
uint128_t v2_142 = vorrq_s8(v2_141, v4_121)
int32_t temp0_108 = x8_1[0x10]
v5_98.d = temp0_108
v5_98:4.d = temp0_108
v5_98:8.d = temp0_108
v5_98:0xc.d = temp0_108
int32_t temp0_109 = x8_1[0x12]
v16_28.d = temp0_109
v16_28:4.d = temp0_109
v16_28:8.d = temp0_109
v16_28:0xc.d = temp0_109
uint128_t v4_123 = (v3_160 & v2_142) ^ v0_119
uint128_t v0_120 = vorrq_s8(v3_160, v0_119)
int32_t temp0_110 = x8_1[0x13]
uint128_t v6_90
v6_90.d = temp0_110
v6_90:4.d = temp0_110
v6_90:8.d = temp0_110
v6_90:0xc.d = temp0_110
uint128_t v0_123 = (v0_120 & v1_134) ^ v2_142 ^ v4_123
uint128_t v3_161 = v0_123 ^ v3_160
uint128_t v6_91 = v0_123 ^ v6_90
uint128_t v2_144 = v3_161 ^ (v4_123 & v1_134)
uint128_t v1_135 = v3_161 & not.o(v1_134)
v3_161.d = v6_91.d u>> 7
v3_161:4.d = v6_91:4.d u>> 7
v3_161:8.d = v6_91:8.d u>> 7
v3_161:0xc.d = v6_91:0xc.d u>> 7
uint128_t v7_71 = v2_144 ^ not.o(v1_134)
uint128_t v2_145 = v2_144 ^ v16_28
v16_28.d = v6_91.d << 0x19
v16_28:4.d = v6_91:4.d << 0x19
v16_28:8.d = v6_91:8.d << 0x19
v16_28:0xc.d = v6_91:0xc.d << 0x19
uint128_t v3_162 = vorrq_s8(v16_28, v3_161)
int32_t temp0_111 = x8_1[0x11]
v16_28.d = temp0_111
v16_28:4.d = temp0_111
v16_28:8.d = temp0_111
v16_28:0xc.d = temp0_111
uint128_t v1_136 = v1_135 ^ v4_123
uint128_t v5_99 = v7_71 ^ v5_98
uint128_t v1_137 = v1_136 ^ v16_28
uint128_t v4_124
v4_124.d = v5_99.d u>> 5
v4_124:4.d = v5_99:4.d u>> 5
v4_124:8.d = v5_99:8.d u>> 5
v4_124:0xc.d = v5_99:0xc.d u>> 5
v5_99.d <<= 0x1b
v5_99:4.d <<= 0x1b
v5_99:8.d <<= 0x1b
v5_99:0xc.d <<= 0x1b
uint128_t v7_72
v7_72.d = v1_137.d << 7
v7_72:4.d = v1_137:4.d << 7
v7_72:8.d = v1_137:8.d << 7
v7_72:0xc.d = v1_137:0xc.d << 7
uint128_t v5_100 = v1_137 ^ v6_91
uint128_t v0_126 = v2_145 ^ vorrq_s8((v7_71 & v4_123) ^ v0_123, v7_71 ^ v1_136)
uint128_t v6_92 = v7_72 ^ v6_91
v7_72.d = v1_137.d u>> 1
v7_72:4.d = v1_137:4.d u>> 1
v7_72:8.d = v1_137:8.d u>> 1
v7_72:0xc.d = v1_137:0xc.d u>> 1
v1_137.d <<= 0x1f
v1_137:4.d <<= 0x1f
v1_137:8.d <<= 0x1f
v1_137:0xc.d <<= 0x1f
uint128_t v4_126 = v5_100 ^ vorrq_s8(v5_99, v4_124)
v5_100.d = v0_126.d u>> 0x16
v5_100:4.d = v0_126:4.d u>> 0x16
v5_100:8.d = v0_126:8.d u>> 0x16
v5_100:0xc.d = v0_126:0xc.d u>> 0x16
v0_126.d <<= 0xa
v0_126:4.d <<= 0xa
v0_126:8.d <<= 0xa
v0_126:0xc.d <<= 0xa
uint128_t v1_138 = vorrq_s8(v1_137, v7_72)
uint128_t v0_127 = vorrq_s8(v0_126, v5_100)
v5_100.d = v4_126.d << 3
v5_100:4.d = v4_126:4.d << 3
v5_100:8.d = v4_126:8.d << 3
v5_100:0xc.d = v4_126:0xc.d << 3
uint128_t v1_139 = v4_126 ^ v1_138
uint128_t v3_163 = v5_100 ^ v3_162
v5_100.d = v4_126.d u>> 0xd
v5_100:4.d = v4_126:4.d u>> 0xd
v5_100:8.d = v4_126:8.d u>> 0xd
v5_100:0xc.d = v4_126:0xc.d u>> 0xd
v4_126.d <<= 0x13
v4_126:4.d <<= 0x13
v4_126:8.d <<= 0x13
v4_126:0xc.d <<= 0x13
uint128_t v0_128 = v6_92 ^ v0_127
uint128_t v4_127 = vorrq_s8(v4_126, v5_100)
uint128_t v3_164 = v3_163 ^ v0_128
uint128_t v1_140 = v1_139 ^ v0_128
v6_92.d = v0_128.d u>> 3
v6_92:4.d = v0_128:4.d u>> 3
v6_92:8.d = v0_128:8.d u>> 3
v6_92:0xc.d = v0_128:0xc.d u>> 3
v0_128.d <<= 0x1d
v0_128:4.d <<= 0x1d
v0_128:8.d <<= 0x1d
v0_128:0xc.d <<= 0x1d
uint128_t v0_129 = vorrq_s8(v0_128, v6_92)
uint128_t v6_93 = v1_140 ^ v0_129
int32_t temp0_112 = x8_1[0xd]
v2_145.d = temp0_112
v2_145:4.d = temp0_112
v2_145:8.d = temp0_112
v2_145:0xc.d = temp0_112
uint128_t v4_128 = v6_93 ^ v4_127
uint128_t v0_131 = (v6_93 & v0_129) ^ v4_128
uint128_t v1_141 = v4_128 & v1_140
int32_t temp0_113 = x8_1[0xf]
v4_128.d = temp0_113
v4_128:4.d = temp0_113
v4_128:8.d = temp0_113
v4_128:0xc.d = temp0_113
uint128_t v3_165 = vorrq_s8(v0_131, v3_164)
uint128_t v6_94 = v3_165 ^ v6_93
int32_t temp0_114 = x8_1[0xc]
v7_72.d = temp0_114
v7_72:4.d = temp0_114
v7_72:8.d = temp0_114
v7_72:0xc.d = temp0_114
uint128_t v5_102 = v3_163 ^ v1_139 ^ v0_131
uint128_t v1_142 = v3_165 ^ v1_141
uint128_t v1_143 = v1_142 ^ v5_102
uint128_t v3_167 = (v6_94 & v3_165) ^ v5_102
int32_t temp0_115 = x8_1[0xe]
v5_102.d = temp0_115
v5_102:4.d = temp0_115
v5_102:8.d = temp0_115
v5_102:0xc.d = temp0_115
uint128_t v7_73 = v6_94 ^ v7_72
uint128_t v0_132 = vorrq_s8(v1_143, v6_94) ^ v0_131
uint128_t v1_144
v1_144.d = v7_73.d u>> 5
v1_144:4.d = v7_73:4.d u>> 5
v1_144:8.d = v7_73:8.d u>> 5
v1_144:0xc.d = v7_73:0xc.d u>> 5
v7_73.d <<= 0x1b
v7_73:4.d <<= 0x1b
v7_73:8.d <<= 0x1b
v7_73:0xc.d <<= 0x1b
uint128_t v5_103 = v3_167 ^ v5_102
uint128_t v1_145 = vorrq_s8(v7_73, v1_144)
uint128_t v2_146 = v0_132 ^ v2_145
uint128_t v0_133 = v1_142 ^ v4_128 ^ v3_167 ^ v0_132
uint128_t v3_168
v3_168.d = v5_103.d u>> 0x16
v3_168:4.d = v5_103:4.d u>> 0x16
v3_168:8.d = v5_103:8.d u>> 0x16
v3_168:0xc.d = v5_103:0xc.d u>> 0x16
uint128_t v4_129
v4_129.d = v5_103.d << 0xa
v4_129:4.d = v5_103:4.d << 0xa
v4_129:8.d = v5_103:8.d << 0xa
v4_129:0xc.d = v5_103:0xc.d << 0xa
uint128_t v3_169 = vorrq_s8(v4_129, v3_168)
v4_129.d = v0_133.d u>> 7
v4_129:4.d = v0_133:4.d u>> 7
v4_129:8.d = v0_133:8.d u>> 7
v4_129:0xc.d = v0_133:0xc.d u>> 7
v5_103.d = v0_133.d << 0x19
v5_103:4.d = v0_133:4.d << 0x19
v5_103:8.d = v0_133:8.d << 0x19
v5_103:0xc.d = v0_133:0xc.d << 0x19
v7_73.d = v2_146.d u>> 1
v7_73:4.d = v2_146:4.d u>> 1
v7_73:8.d = v2_146:8.d u>> 1
v7_73:0xc.d = v2_146:0xc.d u>> 1
uint128_t v3_170 = v0_133 ^ v3_169
uint128_t v0_134 = v2_146 ^ v1_145 ^ v0_133
uint128_t v1_146
v1_146.d = v2_146.d << 0x1f
v1_146:4.d = v2_146:4.d << 0x1f
v1_146:8.d = v2_146:8.d << 0x1f
v1_146:0xc.d = v2_146:0xc.d << 0x1f
uint128_t v1_147 = vorrq_s8(v1_146, v7_73)
v2_146.d <<= 7
v2_146:4.d <<= 7
v2_146:8.d <<= 7
v2_146:0xc.d <<= 7
uint128_t v4_130 = vorrq_s8(v5_103, v4_129)
uint128_t v2_147 = v3_170 ^ v2_146
uint128_t v1_148 = v0_134 ^ v1_147
v5_103.d = v0_134.d u>> 0xd
v5_103:4.d = v0_134:4.d u>> 0xd
v5_103:8.d = v0_134:8.d u>> 0xd
v5_103:0xc.d = v0_134:0xc.d u>> 0xd
v7_73.d = v0_134.d << 0x13
v7_73:4.d = v0_134:4.d << 0x13
v7_73:8.d = v0_134:8.d << 0x13
v7_73:0xc.d = v0_134:0xc.d << 0x13
v0_134.d <<= 3
v0_134:4.d <<= 3
v0_134:8.d <<= 3
v0_134:0xc.d <<= 3
uint128_t v4_131 = v2_147 ^ v4_130
v3_170.d <<= 0x1d
v3_170:4.d <<= 0x1d
v3_170:8.d <<= 0x1d
v3_170:0xc.d <<= 0x1d
uint128_t v1_149 = v1_148 ^ v2_147
v2_147.d u>>= 3
v2_147:4.d u>>= 3
v2_147:8.d u>>= 3
v2_147:0xc.d u>>= 3
uint128_t v5_104 = vorrq_s8(v7_73, v5_103)
uint128_t v0_135 = v4_131 ^ v0_134
uint128_t v2_149 = v0_135 ^ vorrq_s8(v3_170, v2_147)
uint128_t v0_136 = v0_135 ^ v5_104
int32_t temp0_116 = x8_1[0xa]
v7_73.d = temp0_116
v7_73:4.d = temp0_116
v7_73:8.d = temp0_116
v7_73:0xc.d = temp0_116
int32_t temp0_117 = x8_1[0xb]
v4_131.d = temp0_117
v4_131:4.d = temp0_117
v4_131:8.d = temp0_117
v4_131:0xc.d = temp0_117
uint128_t v3_172 = (v2_149 & v0_136) ^ v1_149
uint128_t v1_151 = vorrq_s8(v2_149, v1_149) ^ v0_136
int32_t temp0_118 = x8_1[8]
v6_94.d = temp0_118
v6_94:4.d = temp0_118
v6_94:8.d = temp0_118
v6_94:0xc.d = temp0_118
uint128_t v2_150 = v3_172 ^ v2_149
uint128_t v3_173 = not.o(v3_172)
uint128_t v0_139 = (v0_136 & v5_104 & v3_172) ^ v2_150
uint128_t v3_174 = v5_104 ^ v3_173
uint128_t v2_152 = vorrq_s8(v2_150 & v1_151, v5_104)
int32_t temp0_119 = x8_1[9]
v5_104.d = temp0_119
v5_104:4.d = temp0_119
v5_104:8.d = temp0_119
v5_104:0xc.d = temp0_119
uint128_t v6_95 = v1_151 ^ v6_94
uint128_t v1_152 = v3_174 & v1_151
v3_174.d = v6_95.d u>> 5
v3_174:4.d = v6_95:4.d u>> 5
v3_174:8.d = v6_95:8.d u>> 5
v3_174:0xc.d = v6_95:0xc.d u>> 5
v6_95.d <<= 0x1b
v6_95:4.d <<= 0x1b
v6_95:8.d <<= 0x1b
v6_95:0xc.d <<= 0x1b
uint128_t v5_105 = v0_139 ^ v5_104
uint128_t v2_153 = v7_73 ^ v3_173 ^ v2_152
uint128_t v4_132
v4_132.d = v5_105.d << 7
v4_132:4.d = v5_105:4.d << 7
v4_132:8.d = v5_105:8.d << 7
v4_132:0xc.d = v5_105:0xc.d << 7
uint128_t v3_176 = v5_105 ^ vorrq_s8(v6_95, v3_174)
uint128_t v0_140 = v4_131 ^ v3_173 ^ v1_152 ^ v0_139
uint128_t v1_153
v1_153.d = v5_105.d u>> 1
v1_153:4.d = v5_105:4.d u>> 1
v1_153:8.d = v5_105:8.d u>> 1
v1_153:0xc.d = v5_105:0xc.d u>> 1
v5_105.d <<= 0x1f
v5_105:4.d <<= 0x1f
v5_105:8.d <<= 0x1f
v5_105:0xc.d <<= 0x1f
uint128_t v1_154 = vorrq_s8(v5_105, v1_153)
v5_105.d = v2_153.d u>> 0x16
v5_105:4.d = v2_153:4.d u>> 0x16
v5_105:8.d = v2_153:8.d u>> 0x16
v5_105:0xc.d = v2_153:0xc.d u>> 0x16
v2_153.d <<= 0xa
v2_153:4.d <<= 0xa
v2_153:8.d <<= 0xa
v2_153:0xc.d <<= 0xa
uint128_t v2_154 = vorrq_s8(v2_153, v5_105)
uint128_t v4_133 = v0_140 ^ v4_132
uint128_t v3_177 = v3_176 ^ v0_140
v5_105.d = v0_140.d u>> 7
v5_105:4.d = v0_140:4.d u>> 7
v5_105:8.d = v0_140:8.d u>> 7
v5_105:0xc.d = v0_140:0xc.d u>> 7
v0_140.d <<= 0x19
v0_140:4.d <<= 0x19
v0_140:8.d <<= 0x19
v0_140:0xc.d <<= 0x19
uint128_t v2_155 = v4_133 ^ v2_154
uint128_t v0_141 = vorrq_s8(v0_140, v5_105)
int32_t temp0_120 = x8_1[5]
v6_95.d = temp0_120
v6_95:4.d = temp0_120
v6_95:8.d = temp0_120
v6_95:0xc.d = temp0_120
v4_133.d = v3_177.d << 3
v4_133:4.d = v3_177:4.d << 3
v4_133:8.d = v3_177:8.d << 3
v4_133:0xc.d = v3_177:0xc.d << 3
int32_t temp0_121 = x8_1[6]
uint128_t v7_74
v7_74.d = temp0_121
v7_74:4.d = temp0_121
v7_74:8.d = temp0_121
v7_74:0xc.d = temp0_121
uint128_t v1_156 = v3_177 ^ v1_154 ^ v2_155
uint128_t v0_143 = v2_155 ^ v0_141 ^ v4_133
v5_105.d = v3_177.d u>> 0xd
v5_105:4.d = v3_177:4.d u>> 0xd
v5_105:8.d = v3_177:8.d u>> 0xd
v5_105:0xc.d = v3_177:0xc.d u>> 0xd
v3_177.d <<= 0x13
v3_177:4.d <<= 0x13
v3_177:8.d <<= 0x13
v3_177:0xc.d <<= 0x13
uint128_t v4_134 = v0_143 ^ v1_156
uint128_t v3_178 = vorrq_s8(v3_177, v5_105)
v5_105.d = v2_155.d u>> 3
v5_105:4.d = v2_155:4.d u>> 3
v5_105:8.d = v2_155:8.d u>> 3
v5_105:0xc.d = v2_155:0xc.d u>> 3
v2_155.d <<= 0x1d
v2_155:4.d <<= 0x1d
v2_155:8.d <<= 0x1d
v2_155:0xc.d <<= 0x1d
uint128_t v2_156 = vorrq_s8(v2_155, v5_105)
uint128_t v0_145 = (v4_134 & v0_143) ^ v3_178
uint128_t v1_157 = v1_156 ^ v2_156
uint128_t v5_106 = vorrq_s8(v4_134, v3_178)
uint128_t v2_157 = v0_145 ^ v2_156
uint128_t v4_135 = vorrq_s8(v0_145, v4_134)
uint128_t v0_146 = v0_145 ^ v7_74
int32_t temp0_122 = x8_1[7]
v7_74.d = temp0_122
v7_74:4.d = temp0_122
v7_74:8.d = temp0_122
v7_74:0xc.d = temp0_122
uint128_t v3_180 = vorrq_s8(v2_157, v5_106 ^ v1_157) ^ v0_145 ^ v4_134
uint128_t v2_158 = v2_157 ^ v7_74
uint128_t v6_96 = v3_180 ^ v6_95
uint128_t v5_108
v5_108.d = v2_158.d u>> 7
v5_108:4.d = v2_158:4.d u>> 7
v5_108:8.d = v2_158:8.d u>> 7
v5_108:0xc.d = v2_158:0xc.d u>> 7
v7_74.d = v2_158.d << 0x19
v7_74:4.d = v2_158:4.d << 0x19
v7_74:8.d = v2_158:8.d << 0x19
v7_74:0xc.d = v2_158:0xc.d << 0x19
uint128_t v3_181 = v4_135 & not.o(v3_180)
uint128_t v1_159 = v3_180 ^ v1_157 ^ v4_135
v4_135.d = v6_96.d << 7
v4_135:4.d = v6_96:4.d << 7
v4_135:8.d = v6_96:8.d << 7
v4_135:0xc.d = v6_96:0xc.d << 7
uint128_t v5_109 = vorrq_s8(v7_74, v5_108)
uint128_t v7_75 = v6_96 ^ v2_158
uint128_t v2_159 = v4_135 ^ v2_158
v4_135.d = v6_96.d u>> 1
v4_135:4.d = v6_96:4.d u>> 1
v4_135:8.d = v6_96:8.d u>> 1
v4_135:0xc.d = v6_96:0xc.d u>> 1
v6_96.d <<= 0x1f
v6_96:4.d <<= 0x1f
v6_96:8.d <<= 0x1f
v6_96:0xc.d <<= 0x1f
uint128_t v4_136 = vorrq_s8(v6_96, v4_135)
int32_t temp0_123 = x8_1[4]
v6_96.d = temp0_123
v6_96:4.d = temp0_123
v6_96:8.d = temp0_123
v6_96:0xc.d = temp0_123
int128_t v0_147 = v0_146 ^ (v3_181 | not.o(v1_159))
uint128_t v1_161 = v6_96 ^ not.o(v1_159)
int128_t v3_182
v3_182.d = v0_147.d u>> 0x16
v3_182:4.d = v0_147:4.d u>> 0x16
v3_182:8.d = v0_147:8.d u>> 0x16
v3_182:0xc.d = v0_147:0xc.d u>> 0x16
v0_147.d <<= 0xa
v0_147:4.d <<= 0xa
v0_147:8.d <<= 0xa
v0_147:0xc.d <<= 0xa
uint128_t v0_148 = vorrq_s8(v0_147, v3_182)
v3_182.d = v1_161.d u>> 5
v3_182:4.d = v1_161:4.d u>> 5
v3_182:8.d = v1_161:8.d u>> 5
v3_182:0xc.d = v1_161:0xc.d u>> 5
v1_161.d <<= 0x1b
v1_161:4.d <<= 0x1b
v1_161:8.d <<= 0x1b
v1_161:0xc.d <<= 0x1b
uint128_t v1_163 = v7_75 ^ vorrq_s8(v1_161, v3_182)
uint128_t v0_149 = v2_159 ^ v0_148
v2_159.d = v1_163.d << 3
v2_159:4.d = v1_163:4.d << 3
v2_159:8.d = v1_163:8.d << 3
v2_159:0xc.d = v1_163:0xc.d << 3
int32_t temp0_124 = *x8_1
v6_96.d = temp0_124
v6_96:4.d = temp0_124
v6_96:8.d = temp0_124
v6_96:0xc.d = temp0_124
v7_75.d = v0_149.d u>> 3
v7_75:4.d = v0_149:4.d u>> 3
v7_75:8.d = v0_149:8.d u>> 3
v7_75:0xc.d = v0_149:0xc.d u>> 3
uint128_t v2_160 = v2_159 ^ v5_109
v5_109.d = v0_149.d << 0x1d
v5_109:4.d = v0_149:4.d << 0x1d
v5_109:8.d = v0_149:8.d << 0x1d
v5_109:0xc.d = v0_149:0xc.d << 0x1d
uint128_t v4_137 = v1_163 ^ v4_136
uint128_t v5_110 = vorrq_s8(v5_109, v7_75)
v7_75.d = v1_163.d u>> 0xd
v7_75:4.d = v1_163:4.d u>> 0xd
v7_75:8.d = v1_163:8.d u>> 0xd
v7_75:0xc.d = v1_163:0xc.d u>> 0xd
v1_163.d <<= 0x13
v1_163:4.d <<= 0x13
v1_163:8.d <<= 0x13
v1_163:0xc.d <<= 0x13
uint128_t v4_138 = v4_137 ^ v0_149
uint128_t v1_164 = vorrq_s8(v1_163, v7_75)
uint128_t v0_150 = v2_160 ^ v0_149
uint128_t v2_163 = v0_150 ^ not.o(v5_110) ^ vorrq_s8(v4_138, v1_164)
int32_t temp0_125 = *x8_1
uint128_t v7_76
v7_76.d = temp0_125
v7_76:4.d = temp0_125
v7_76:8.d = temp0_125
v7_76:0xc.d = temp0_125
uint128_t v4_139 = not.o(v4_138)
uint128_t v1_165 = v1_164 ^ v4_139
int32_t temp0_126 = x8_1[2]
v3_182.d = temp0_126
v3_182:4.d = temp0_126
v3_182:8.d = temp0_126
v3_182:0xc.d = temp0_126
uint128_t v5_112 = (v4_139 | not.o(v5_110)) ^ v1_165
uint128_t v1_166 = v1_165 & v0_150
uint128_t v4_140 = v1_166 ^ v4_139
uint128_t v4_141 = v4_140 ^ v7_76
int32_t temp0_127 = x8_1[3]
v7_76.d = temp0_127
v7_76:4.d = temp0_127
v7_76:8.d = temp0_127
v7_76:0xc.d = temp0_127
uint128_t v1_168 = vorrq_s8(v2_163, v1_166) ^ v5_112
int128_t v3_183 = v2_163 ^ v3_182
uint128_t v2_164 = v1_168 ^ v6_96
uint128_t v0_153 = v4_140 ^ v0_150 ^ v2_163 ^ v1_168
uint128_t v16_29 = vzip1q_s32(v2_164, v3_183)
uint128_t v1_169 = vzip2q_s32(v2_164, v3_183)
uint128_t v0_154 = v0_153 ^ v7_76
uint128_t v3_185 = v4_141 ^ (v0_153 & (v2_163 ^ v5_112))
uint128_t v17_11 = vzip1q_s32(v3_185, v0_154)
uint128_t v2_165 = vzip2q_s32(v3_185, v0_154)
int64_t v3_186 = vextq_s8(v16_29, v16_29, 8)
int64_t v5_114 = vextq_s8(v1_169, v1_169, 8)
int32_t* entry_x2
*entry_x2 = v16_29.d
entry_x2[1] = v17_11.d
entry_x2[2] = v16_29:4.d
entry_x2[3] = v17_11:4.d
int64_t v4_142 = vextq_s8(v17_11, v17_11, 8)
int64_t v6_97 = vextq_s8(v2_165, v2_165, 8)
entry_x2[4] = v3_186.d
entry_x2[5] = v4_142.d
entry_x2[6] = v3_186:4.d
entry_x2[7] = v4_142:4.d
entry_x2[8] = v1_169.d
entry_x2[9] = v2_165.d
entry_x2[0xa] = v1_169:4.d
entry_x2[0xb] = v2_165:4.d
entry_x2[0xc] = v5_114.d
entry_x2[0xd] = v6_97.d
entry_x2[0xe] = v5_114:4.d
entry_x2[0xf] = v6_97:4.d
