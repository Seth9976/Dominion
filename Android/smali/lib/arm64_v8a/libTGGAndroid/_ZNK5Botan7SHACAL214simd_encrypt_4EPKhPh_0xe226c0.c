// 函数: _ZNK5Botan7SHACAL214simd_encrypt_4EPKhPh
// 地址: 0xe226c0
// 来自: E:\torrent\Cursor\Dominion_1.0.3315\split_config.arm64_v8a\lib\arm64-v8a\libTGGAndroid.so

uint128_t v1 = *(arg2 + 0x10)
uint128_t v2 = *(arg2 + 0x20)
uint128_t v3 = *(arg2 + 0x30)
uint128_t v4 = *(arg2 + 0x40)
uint128_t v5 = *(arg2 + 0x50)
uint128_t v6 = *(arg2 + 0x60)
uint128_t v7 = *(arg2 + 0x70)
void* x9 = *(arg1 + 8)
uint128_t v0_1 = vrev32q_s8(*arg2)
uint128_t v1_1 = vrev32q_s8(v1)
uint128_t v2_1 = vrev32q_s8(v2)
uint128_t v3_1 = vrev32q_s8(v3)
uint128_t v4_1 = vrev32q_s8(v4)
uint128_t v5_1 = vrev32q_s8(v5)
uint128_t v6_1 = vrev32q_s8(v6)
uint128_t v7_1 = vrev32q_s8(v7)
uint128_t v16 = vzip1q_s32(v0_1, v4_1)
uint128_t v4_2 = vzip2q_s32(v0_1, v4_1)
uint128_t v0_2 = vzip1q_s32(v2_1, v6_1)
uint128_t v6_2 = vzip2q_s32(v2_1, v6_1)
uint128_t v17 = vzip1q_s32(v1_1, v5_1)
uint128_t v18 = vzip2q_s32(v1_1, v5_1)
uint128_t v5_2 = vzip1q_s32(v3_1, v7_1)
uint128_t v3_2 = vzip2q_s32(v3_1, v7_1)
int64_t i = 0
uint128_t v2_2 = vzip1q_s32(v16, v0_2)
uint128_t v0_3 = vzip2q_s32(v16, v0_2)
uint128_t v1_2 = vzip1q_s32(v4_2, v6_2)
uint128_t v4_3 = vzip2q_s32(v4_2, v6_2)
uint128_t v6_3 = vzip1q_s32(v17, v5_2)
uint128_t v5_3 = vzip2q_s32(v17, v5_2)
uint128_t v7_2 = vzip1q_s32(v18, v3_2)
uint128_t v3_3 = vzip2q_s32(v18, v3_2)

do
    v17.d = v6_3.d u>> 6
    v17:4.d = v6_3:4.d u>> 6
    v17:8.d = v6_3:8.d u>> 6
    v17:0xc.d = v6_3:0xc.d u>> 6
    v18.d = v6_3.d << 0x1a
    v18:4.d = v6_3:4.d << 0x1a
    v18:8.d = v6_3:8.d << 0x1a
    v18:0xc.d = v6_3:0xc.d << 0x1a
    uint128_t v19
    v19.d = v6_3.d u>> 0xb
    v19:4.d = v6_3:4.d u>> 0xb
    v19:8.d = v6_3:8.d u>> 0xb
    v19:0xc.d = v6_3:0xc.d u>> 0xb
    uint128_t v20
    v20.d = v6_3.d << 0x15
    v20:4.d = v6_3:4.d << 0x15
    v20:8.d = v6_3:8.d << 0x15
    v20:0xc.d = v6_3:0xc.d << 0x15
    uint128_t v18_1 = vorrq_s8(v18, v17)
    int32_t* x10_1 = x9 + 0x10 + i
    uint128_t v21
    v21.d = v6_3.d u>> 0x19
    v21:4.d = v6_3:4.d u>> 0x19
    v21:8.d = v6_3:8.d u>> 0x19
    v21:0xc.d = v6_3:0xc.d u>> 0x19
    uint128_t v22
    v22.d = v6_3.d << 7
    v22:4.d = v6_3:4.d << 7
    v22:8.d = v6_3:8.d << 7
    v22:0xc.d = v6_3:0xc.d << 7
    uint128_t v24_1 = vorrq_s8(v20, v19) ^ v18_1
    uint128_t v19_2 = vorrq_s8(v0_3, v2_2)
    uint128_t v23
    v23.d = v2_2.d u>> 2
    v23:4.d = v2_2:4.d u>> 2
    v23:8.d = v2_2:8.d u>> 2
    v23:0xc.d = v2_2:0xc.d u>> 2
    v17.d = v2_2.d << 0x1e
    v17:4.d = v2_2:4.d << 0x1e
    v17:8.d = v2_2:8.d << 0x1e
    v17:0xc.d = v2_2:0xc.d << 0x1e
    v20.d = v2_2.d u>> 0xd
    v20:4.d = v2_2:4.d u>> 0xd
    v20:8.d = v2_2:8.d u>> 0xd
    v20:0xc.d = v2_2:0xc.d u>> 0xd
    uint128_t v21_1 = vorrq_s8(v22, v21)
    v22.d = v2_2.d << 0x13
    v22:4.d = v2_2:4.d << 0x13
    v22:8.d = v2_2:8.d << 0x13
    v22:0xc.d = v2_2:0xc.d << 0x13
    uint128_t v23_1 = vorrq_s8(v17, v23)
    v17.d = v2_2.d u>> 0x16
    v17:4.d = v2_2:4.d u>> 0x16
    v17:8.d = v2_2:8.d u>> 0x16
    v17:0xc.d = v2_2:0xc.d u>> 0x16
    uint128_t v20_1 = vorrq_s8(v22, v20)
    v22.d = v2_2.d << 0xa
    v22:4.d = v2_2:4.d << 0xa
    v22:8.d = v2_2:8.d << 0xa
    v22:0xc.d = v2_2:0xc.d << 0xa
    uint128_t v19_4 = vorrq_s8(v1_2 & v19_2, v0_3 & v2_2)
    int32_t temp0_1 = *(x10_1 - 0x10)
    uint128_t v18_2
    v18_2.d = temp0_1
    v18_2:4.d = temp0_1
    v18_2:8.d = temp0_1
    v18_2:0xc.d = temp0_1
    uint128_t v22_1 = vorrq_s8(v22, v17)
    uint128_t v23_2 = v20_1 ^ v23_1
    int32_t temp0_2 = *(x10_1 - 0xc)
    v20_1.d = temp0_2
    v20_1:4.d = temp0_2
    v20_1:8.d = temp0_2
    v20_1:0xc.d = temp0_2
    uint128_t v23_3 = v23_2 ^ v22_1
    int32_t temp0_3 = *(x10_1 - 8)
    v22_1.d = temp0_3
    v22_1:4.d = temp0_3
    v22_1:8.d = temp0_3
    v22_1:0xc.d = temp0_3
    uint128_t v3_5 = (v24_1 ^ v21_1) + (v5_3 & v6_3) + (v7_2 & not.o(v6_3)) + v3_3 + v18_2
    uint128_t v4_4 = v3_5 + v4_3
    uint128_t v7_3 = v20_1 + v7_2
    uint128_t v3_6 = v19_4 + v23_3 + v3_5
    uint128_t v16_3
    v16_3.d = v4_4.d u>> 6
    v16_3:4.d = v4_4:4.d u>> 6
    v16_3:8.d = v4_4:8.d u>> 6
    v16_3:0xc.d = v4_4:0xc.d u>> 6
    uint128_t v17_1
    v17_1.d = v4_4.d << 0x1a
    v17_1:4.d = v4_4:4.d << 0x1a
    v17_1:8.d = v4_4:8.d << 0x1a
    v17_1:0xc.d = v4_4:0xc.d << 0x1a
    v18_2.d = v4_4.d u>> 0xb
    v18_2:4.d = v4_4:4.d u>> 0xb
    v18_2:8.d = v4_4:8.d u>> 0xb
    v18_2:0xc.d = v4_4:0xc.d u>> 0xb
    uint128_t v19_5
    v19_5.d = v4_4.d << 0x15
    v19_5:4.d = v4_4:4.d << 0x15
    v19_5:8.d = v4_4:8.d << 0x15
    v19_5:0xc.d = v4_4:0xc.d << 0x15
    v20_1.d = v4_4.d u>> 0x19
    v20_1:4.d = v4_4:4.d u>> 0x19
    v20_1:8.d = v4_4:8.d u>> 0x19
    v20_1:0xc.d = v4_4:0xc.d u>> 0x19
    uint128_t v21_2
    v21_2.d = v4_4.d << 7
    v21_2:4.d = v4_4:4.d << 7
    v21_2:8.d = v4_4:8.d << 7
    v21_2:0xc.d = v4_4:0xc.d << 7
    uint128_t v16_4 = vorrq_s8(v17_1, v16_3)
    v17_1.d = v3_6.d u>> 2
    v17_1:4.d = v3_6:4.d u>> 2
    v17_1:8.d = v3_6:8.d u>> 2
    v17_1:0xc.d = v3_6:0xc.d u>> 2
    uint128_t v18_3 = vorrq_s8(v19_5, v18_2)
    v19_5.d = v3_6.d << 0x1e
    v19_5:4.d = v3_6:4.d << 0x1e
    v19_5:8.d = v3_6:8.d << 0x1e
    v19_5:0xc.d = v3_6:0xc.d << 0x1e
    uint128_t v20_2 = vorrq_s8(v21_2, v20_1)
    v21_2.d = v3_6.d u>> 0xd
    v21_2:4.d = v3_6:4.d u>> 0xd
    v21_2:8.d = v3_6:8.d u>> 0xd
    v21_2:0xc.d = v3_6:0xc.d u>> 0xd
    uint128_t v23_4
    v23_4.d = v3_6.d << 0x13
    v23_4:4.d = v3_6:4.d << 0x13
    v23_4:8.d = v3_6:8.d << 0x13
    v23_4:0xc.d = v3_6:0xc.d << 0x13
    uint128_t v17_2 = vorrq_s8(v19_5, v17_1)
    v19_5.d = v3_6.d u>> 0x16
    v19_5:4.d = v3_6:4.d u>> 0x16
    v19_5:8.d = v3_6:8.d u>> 0x16
    v19_5:0xc.d = v3_6:0xc.d u>> 0x16
    uint128_t v21_3 = vorrq_s8(v23_4, v21_2)
    v23_4.d = v3_6.d << 0xa
    v23_4:4.d = v3_6:4.d << 0xa
    v23_4:8.d = v3_6:8.d << 0xa
    v23_4:0xc.d = v3_6:0xc.d << 0xa
    uint128_t v7_5 = vorrq_s8(v3_6, v2_2)
    uint128_t v19_6 = vorrq_s8(v23_4, v19_5)
    int32_t temp0_4 = *(x10_1 - 4)
    v23_4.d = temp0_4
    v23_4:4.d = temp0_4
    v23_4:8.d = temp0_4
    v23_4:0xc.d = temp0_4
    uint128_t v5_6 = v7_3 + (v4_4 & v6_3) + (v5_3 & not.o(v4_4)) + (v18_3 ^ v16_4 ^ v20_2)
    uint128_t v16_7 = v5_6 + v1_2
    uint128_t v1_3 = vorrq_s8(v7_5 & v0_3, v3_6 & v2_2) + (v21_3 ^ v17_2 ^ v19_6) + v5_6
    v5_6.d = v16_7.d u>> 6
    v5_6:4.d = v16_7:4.d u>> 6
    v5_6:8.d = v16_7:8.d u>> 6
    v5_6:0xc.d = v16_7:0xc.d u>> 6
    uint128_t v7_8
    v7_8.d = v16_7.d << 0x1a
    v7_8:4.d = v16_7:4.d << 0x1a
    v7_8:8.d = v16_7:8.d << 0x1a
    v7_8:0xc.d = v16_7:0xc.d << 0x1a
    uint128_t v17_4
    v17_4.d = v16_7.d u>> 0xb
    v17_4:4.d = v16_7:4.d u>> 0xb
    v17_4:8.d = v16_7:8.d u>> 0xb
    v17_4:0xc.d = v16_7:0xc.d u>> 0xb
    uint128_t v18_4
    v18_4.d = v16_7.d << 0x15
    v18_4:4.d = v16_7:4.d << 0x15
    v18_4:8.d = v16_7:8.d << 0x15
    v18_4:0xc.d = v16_7:0xc.d << 0x15
    v19_6.d = v16_7.d u>> 0x19
    v19_6:4.d = v16_7:4.d u>> 0x19
    v19_6:8.d = v16_7:8.d u>> 0x19
    v19_6:0xc.d = v16_7:0xc.d u>> 0x19
    v20_2.d = v16_7.d << 7
    v20_2:4.d = v16_7:4.d << 7
    v20_2:8.d = v16_7:8.d << 7
    v20_2:0xc.d = v16_7:0xc.d << 7
    uint128_t v5_7 = vorrq_s8(v7_8, v5_6)
    v7_8.d = v1_3.d u>> 2
    v7_8:4.d = v1_3:4.d u>> 2
    v7_8:8.d = v1_3:8.d u>> 2
    v7_8:0xc.d = v1_3:0xc.d u>> 2
    uint128_t v17_5 = vorrq_s8(v18_4, v17_4)
    v18_4.d = v1_3.d << 0x1e
    v18_4:4.d = v1_3:4.d << 0x1e
    v18_4:8.d = v1_3:8.d << 0x1e
    v18_4:0xc.d = v1_3:0xc.d << 0x1e
    uint128_t v19_7 = vorrq_s8(v20_2, v19_6)
    v20_2.d = v1_3.d u>> 0xd
    v20_2:4.d = v1_3:4.d u>> 0xd
    v20_2:8.d = v1_3:8.d u>> 0xd
    v20_2:0xc.d = v1_3:0xc.d u>> 0xd
    uint128_t v22_2
    v22_2.d = v1_3.d << 0x13
    v22_2:4.d = v1_3:4.d << 0x13
    v22_2:8.d = v1_3:8.d << 0x13
    v22_2:0xc.d = v1_3:0xc.d << 0x13
    uint128_t v7_9 = vorrq_s8(v18_4, v7_8)
    v18_4.d = v1_3.d u>> 0x16
    v18_4:4.d = v1_3:4.d u>> 0x16
    v18_4:8.d = v1_3:8.d u>> 0x16
    v18_4:0xc.d = v1_3:0xc.d u>> 0x16
    uint128_t v20_3 = vorrq_s8(v22_2, v20_2)
    v22_2.d = v1_3.d << 0xa
    v22_2:4.d = v1_3:4.d << 0xa
    v22_2:8.d = v1_3:8.d << 0xa
    v22_2:0xc.d = v1_3:0xc.d << 0xa
    uint128_t v21_6 = vorrq_s8(v1_3, v3_6)
    uint128_t v18_5 = vorrq_s8(v22_2, v18_4)
    int32_t temp0_5 = *x10_1
    v22_2.d = temp0_5
    v22_2:4.d = temp0_5
    v22_2:8.d = temp0_5
    v22_2:0xc.d = temp0_5
    uint128_t v5_10 = v22_1 + v5_3 + (v16_7 & v4_4) + (v6_3 & not.o(v16_7)) + (v17_5 ^ v5_7 ^ v19_7)
    uint128_t v7_12 = v5_10 + v0_3
    uint128_t v0_4 = vorrq_s8(v21_6 & v2_2, v1_3 & v3_6) + (v20_3 ^ v7_9 ^ v18_5) + v5_10
    v5_10.d = v7_12.d u>> 6
    v5_10:4.d = v7_12:4.d u>> 6
    v5_10:8.d = v7_12:8.d u>> 6
    v5_10:0xc.d = v7_12:0xc.d u>> 6
    uint128_t v6_6
    v6_6.d = v7_12.d << 0x1a
    v6_6:4.d = v7_12:4.d << 0x1a
    v6_6:8.d = v7_12:8.d << 0x1a
    v6_6:0xc.d = v7_12:0xc.d << 0x1a
    uint128_t v17_7
    v17_7.d = v7_12.d u>> 0xb
    v17_7:4.d = v7_12:4.d u>> 0xb
    v17_7:8.d = v7_12:8.d u>> 0xb
    v17_7:0xc.d = v7_12:0xc.d u>> 0xb
    v18_5.d = v7_12.d << 0x15
    v18_5:4.d = v7_12:4.d << 0x15
    v18_5:8.d = v7_12:8.d << 0x15
    v18_5:0xc.d = v7_12:0xc.d << 0x15
    v19_7.d = v7_12.d u>> 0x19
    v19_7:4.d = v7_12:4.d u>> 0x19
    v19_7:8.d = v7_12:8.d u>> 0x19
    v19_7:0xc.d = v7_12:0xc.d u>> 0x19
    v20_3.d = v7_12.d << 7
    v20_3:4.d = v7_12:4.d << 7
    v20_3:8.d = v7_12:8.d << 7
    v20_3:0xc.d = v7_12:0xc.d << 7
    uint128_t v5_11 = vorrq_s8(v6_6, v5_10)
    v6_6.d = v0_4.d u>> 2
    v6_6:4.d = v0_4:4.d u>> 2
    v6_6:8.d = v0_4:8.d u>> 2
    v6_6:0xc.d = v0_4:0xc.d u>> 2
    uint128_t v17_8 = vorrq_s8(v18_5, v17_7)
    v18_5.d = v0_4.d << 0x1e
    v18_5:4.d = v0_4:4.d << 0x1e
    v18_5:8.d = v0_4:8.d << 0x1e
    v18_5:0xc.d = v0_4:0xc.d << 0x1e
    uint128_t v19_8 = vorrq_s8(v20_3, v19_7)
    v20_3.d = v0_4.d u>> 0xd
    v20_3:4.d = v0_4:4.d u>> 0xd
    v20_3:8.d = v0_4:8.d u>> 0xd
    v20_3:0xc.d = v0_4:0xc.d u>> 0xd
    uint128_t v23_5
    v23_5.d = v0_4.d << 0x13
    v23_5:4.d = v0_4:4.d << 0x13
    v23_5:8.d = v0_4:8.d << 0x13
    v23_5:0xc.d = v0_4:0xc.d << 0x13
    uint128_t v6_7 = vorrq_s8(v18_5, v6_6)
    v18_5.d = v0_4.d u>> 0x16
    v18_5:4.d = v0_4:4.d u>> 0x16
    v18_5:8.d = v0_4:8.d u>> 0x16
    v18_5:0xc.d = v0_4:0xc.d u>> 0x16
    uint128_t v20_4 = vorrq_s8(v23_5, v20_3)
    v23_5.d = v0_4.d << 0xa
    v23_5:4.d = v0_4:4.d << 0xa
    v23_5:8.d = v0_4:8.d << 0xa
    v23_5:0xc.d = v0_4:0xc.d << 0xa
    uint128_t v21_10 = vorrq_s8(v0_4, v1_3)
    uint128_t v18_6 = vorrq_s8(v23_5, v18_5)
    int32_t temp0_6 = *x10_1
    v23_5.d = temp0_6
    v23_5:4.d = temp0_6
    v23_5:8.d = temp0_6
    v23_5:0xc.d = temp0_6
    int128_t v4_7 = v23_4 + v6_3 + (v7_12 & v16_7) + (v4_4 & not.o(v7_12)) + (v17_8 ^ v5_11 ^ v19_8)
    int128_t v6_10 = v4_7 + v2_2
    uint128_t v2_3 = vorrq_s8(v21_10 & v3_6, v0_4 & v1_3) + (v20_4 ^ v6_7 ^ v18_6) + v4_7
    v4_7.d = v6_10.d u>> 6
    v4_7:4.d = v6_10:4.d u>> 6
    v4_7:8.d = v6_10:8.d u>> 6
    v4_7:0xc.d = v6_10:0xc.d u>> 6
    uint128_t v5_14
    v5_14.d = v6_10.d << 0x1a
    v5_14:4.d = v6_10:4.d << 0x1a
    v5_14:8.d = v6_10:8.d << 0x1a
    v5_14:0xc.d = v6_10:0xc.d << 0x1a
    uint128_t v17_10
    v17_10.d = v6_10.d u>> 0xb
    v17_10:4.d = v6_10:4.d u>> 0xb
    v17_10:8.d = v6_10:8.d u>> 0xb
    v17_10:0xc.d = v6_10:0xc.d u>> 0xb
    v18_6.d = v6_10.d << 0x15
    v18_6:4.d = v6_10:4.d << 0x15
    v18_6:8.d = v6_10:8.d << 0x15
    v18_6:0xc.d = v6_10:0xc.d << 0x15
    v19_8.d = v6_10.d u>> 0x19
    v19_8:4.d = v6_10:4.d u>> 0x19
    v19_8:8.d = v6_10:8.d u>> 0x19
    v19_8:0xc.d = v6_10:0xc.d u>> 0x19
    v20_4.d = v6_10.d << 7
    v20_4:4.d = v6_10:4.d << 7
    v20_4:8.d = v6_10:8.d << 7
    v20_4:0xc.d = v6_10:0xc.d << 7
    uint128_t v4_8 = vorrq_s8(v5_14, v4_7)
    v5_14.d = v2_3.d u>> 2
    v5_14:4.d = v2_3:4.d u>> 2
    v5_14:8.d = v2_3:8.d u>> 2
    v5_14:0xc.d = v2_3:0xc.d u>> 2
    uint128_t v17_11 = vorrq_s8(v18_6, v17_10)
    v18_6.d = v2_3.d << 0x1e
    v18_6:4.d = v2_3:4.d << 0x1e
    v18_6:8.d = v2_3:8.d << 0x1e
    v18_6:0xc.d = v2_3:0xc.d << 0x1e
    uint128_t v19_9 = vorrq_s8(v20_4, v19_8)
    v20_4.d = v2_3.d u>> 0xd
    v20_4:4.d = v2_3:4.d u>> 0xd
    v20_4:8.d = v2_3:8.d u>> 0xd
    v20_4:0xc.d = v2_3:0xc.d u>> 0xd
    uint128_t v22_3
    v22_3.d = v2_3.d << 0x13
    v22_3:4.d = v2_3:4.d << 0x13
    v22_3:8.d = v2_3:8.d << 0x13
    v22_3:0xc.d = v2_3:0xc.d << 0x13
    uint128_t v5_15 = vorrq_s8(v18_6, v5_14)
    v18_6.d = v2_3.d u>> 0x16
    v18_6:4.d = v2_3:4.d u>> 0x16
    v18_6:8.d = v2_3:8.d u>> 0x16
    v18_6:0xc.d = v2_3:0xc.d u>> 0x16
    uint128_t v20_5 = vorrq_s8(v22_3, v20_4)
    v22_3.d = v2_3.d << 0xa
    v22_3:4.d = v2_3:4.d << 0xa
    v22_3:8.d = v2_3:8.d << 0xa
    v22_3:0xc.d = v2_3:0xc.d << 0xa
    uint128_t v21_14 = vorrq_s8(v2_3, v0_4)
    uint128_t v18_7 = vorrq_s8(v22_3, v18_6)
    int32_t temp0_7 = x10_1[2]
    v22_3.d = temp0_7
    v22_3:4.d = temp0_7
    v22_3:8.d = temp0_7
    v22_3:0xc.d = temp0_7
    int128_t v4_11 =
        v22_2 + v4_4 + (v6_10 & v7_12) + (v16_7 & not.o(v6_10)) + (v17_11 ^ v4_8 ^ v19_9)
    v3_3 = v4_11 + v3_6
    v4_3 = vorrq_s8(v21_14 & v1_3, v2_3 & v0_4) + (v20_5 ^ v5_15 ^ v18_7) + v4_11
    uint128_t v5_18
    v5_18.d = v3_3.d u>> 6
    v5_18:4.d = v3_3:4.d u>> 6
    v5_18:8.d = v3_3:8.d u>> 6
    v5_18:0xc.d = v3_3:0xc.d u>> 6
    int128_t v16_9
    v16_9.d = v3_3.d << 0x1a
    v16_9:4.d = v3_3:4.d << 0x1a
    v16_9:8.d = v3_3:8.d << 0x1a
    v16_9:0xc.d = v3_3:0xc.d << 0x1a
    uint128_t v17_13
    v17_13.d = v3_3.d u>> 0xb
    v17_13:4.d = v3_3:4.d u>> 0xb
    v17_13:8.d = v3_3:8.d u>> 0xb
    v17_13:0xc.d = v3_3:0xc.d u>> 0xb
    v18_7.d = v3_3.d << 0x15
    v18_7:4.d = v3_3:4.d << 0x15
    v18_7:8.d = v3_3:8.d << 0x15
    v18_7:0xc.d = v3_3:0xc.d << 0x15
    v19_9.d = v3_3.d u>> 0x19
    v19_9:4.d = v3_3:4.d u>> 0x19
    v19_9:8.d = v3_3:8.d u>> 0x19
    v19_9:0xc.d = v3_3:0xc.d u>> 0x19
    v20_5.d = v3_3.d << 7
    v20_5:4.d = v3_3:4.d << 7
    v20_5:8.d = v3_3:8.d << 7
    v20_5:0xc.d = v3_3:0xc.d << 7
    uint128_t v5_19 = vorrq_s8(v16_9, v5_18)
    v16_9.d = v4_3.d u>> 2
    v16_9:4.d = v4_3:4.d u>> 2
    v16_9:8.d = v4_3:8.d u>> 2
    v16_9:0xc.d = v4_3:0xc.d u>> 2
    uint128_t v17_14 = vorrq_s8(v18_7, v17_13)
    v18_7.d = v4_3.d << 0x1e
    v18_7:4.d = v4_3:4.d << 0x1e
    v18_7:8.d = v4_3:8.d << 0x1e
    v18_7:0xc.d = v4_3:0xc.d << 0x1e
    uint128_t v19_10 = vorrq_s8(v20_5, v19_9)
    v20_5.d = v4_3.d u>> 0xd
    v20_5:4.d = v4_3:4.d u>> 0xd
    v20_5:8.d = v4_3:8.d u>> 0xd
    v20_5:0xc.d = v4_3:0xc.d u>> 0xd
    v23.d = v4_3.d << 0x13
    v23:4.d = v4_3:4.d << 0x13
    v23:8.d = v4_3:8.d << 0x13
    v23:0xc.d = v4_3:0xc.d << 0x13
    uint128_t v16_10 = vorrq_s8(v18_7, v16_9)
    v18_7.d = v4_3.d u>> 0x16
    v18_7:4.d = v4_3:4.d u>> 0x16
    v18_7:8.d = v4_3:8.d u>> 0x16
    v18_7:0xc.d = v4_3:0xc.d u>> 0x16
    uint128_t v20_6 = vorrq_s8(v23, v20_5)
    v23.d = v4_3.d << 0xa
    v23:4.d = v4_3:4.d << 0xa
    v23:8.d = v4_3:8.d << 0xa
    v23:0xc.d = v4_3:0xc.d << 0xa
    uint128_t v21_18 = vorrq_s8(v4_3, v2_3)
    uint128_t v18_8 = vorrq_s8(v23, v18_7)
    uint128_t v17_16 = vorrq_s8(v21_18 & v0_4, v4_3 & v2_3)
    uint128_t v5_22 =
        v16_7 + v23_5 + (v3_3 & v6_10) + (v7_12 & not.o(v3_3)) + (v17_14 ^ v5_19 ^ v19_10)
    int32_t temp0_8 = x10_1[3]
    v23.d = temp0_8
    v23:4.d = temp0_8
    v23:8.d = temp0_8
    v23:0xc.d = temp0_8
    v7_2 = v5_22 + v1_3
    v1_2 = v17_16 + (v20_6 ^ v16_10 ^ v18_8) + v5_22
    v5_22.d = v7_2.d u>> 6
    v5_22:4.d = v7_2:4.d u>> 6
    v5_22:8.d = v7_2:8.d u>> 6
    v5_22:0xc.d = v7_2:0xc.d u>> 6
    uint128_t v16_13
    v16_13.d = v7_2.d << 0x1a
    v16_13:4.d = v7_2:4.d << 0x1a
    v16_13:8.d = v7_2:8.d << 0x1a
    v16_13:0xc.d = v7_2:0xc.d << 0x1a
    v17_16.d = v7_2.d u>> 0xb
    v17_16:4.d = v7_2:4.d u>> 0xb
    v17_16:8.d = v7_2:8.d u>> 0xb
    v17_16:0xc.d = v7_2:0xc.d u>> 0xb
    v18_8.d = v7_2.d << 0x15
    v18_8:4.d = v7_2:4.d << 0x15
    v18_8:8.d = v7_2:8.d << 0x15
    v18_8:0xc.d = v7_2:0xc.d << 0x15
    v19_10.d = v7_2.d u>> 0x19
    v19_10:4.d = v7_2:4.d u>> 0x19
    v19_10:8.d = v7_2:8.d u>> 0x19
    v19_10:0xc.d = v7_2:0xc.d u>> 0x19
    v20_6.d = v7_2.d << 7
    v20_6:4.d = v7_2:4.d << 7
    v20_6:8.d = v7_2:8.d << 7
    v20_6:0xc.d = v7_2:0xc.d << 7
    uint128_t v5_23 = vorrq_s8(v16_13, v5_22)
    v16_13.d = v1_2.d u>> 2
    v16_13:4.d = v1_2:4.d u>> 2
    v16_13:8.d = v1_2:8.d u>> 2
    v16_13:0xc.d = v1_2:0xc.d u>> 2
    uint128_t v17_17 = vorrq_s8(v18_8, v17_16)
    v18_8.d = v1_2.d << 0x1e
    v18_8:4.d = v1_2:4.d << 0x1e
    v18_8:8.d = v1_2:8.d << 0x1e
    v18_8:0xc.d = v1_2:0xc.d << 0x1e
    uint128_t v19_11 = vorrq_s8(v20_6, v19_10)
    v20_6.d = v1_2.d u>> 0xd
    v20_6:4.d = v1_2:4.d u>> 0xd
    v20_6:8.d = v1_2:8.d u>> 0xd
    v20_6:0xc.d = v1_2:0xc.d u>> 0xd
    uint128_t v22_4
    v22_4.d = v1_2.d << 0x13
    v22_4:4.d = v1_2:4.d << 0x13
    v22_4:8.d = v1_2:8.d << 0x13
    v22_4:0xc.d = v1_2:0xc.d << 0x13
    uint128_t v16_14 = vorrq_s8(v18_8, v16_13)
    v18_8.d = v1_2.d u>> 0x16
    v18_8:4.d = v1_2:4.d u>> 0x16
    v18_8:8.d = v1_2:8.d u>> 0x16
    v18_8:0xc.d = v1_2:0xc.d u>> 0x16
    uint128_t v20_7 = vorrq_s8(v22_4, v20_6)
    v22_4.d = v1_2.d << 0xa
    v22_4:4.d = v1_2:4.d << 0xa
    v22_4:8.d = v1_2:8.d << 0xa
    v22_4:0xc.d = v1_2:0xc.d << 0xa
    uint128_t v18_9 = vorrq_s8(v22_4, v18_8)
    int128_t v6_13 =
        v7_12 + v22_3 + (v7_2 & v3_3) + (v6_10 & not.o(v7_2)) + (v17_17 ^ v5_23 ^ v19_11)
    v5_3 = v6_13 + v0_4
    v0_3 = vorrq_s8(vorrq_s8(v1_2, v4_3) & v2_3, v1_2 & v4_3) + (v20_7 ^ v16_14 ^ v18_9) + v6_13
    v6_13.d = v5_3.d u>> 6
    v6_13:4.d = v5_3:4.d u>> 6
    v6_13:8.d = v5_3:8.d u>> 6
    v6_13:0xc.d = v5_3:0xc.d u>> 6
    uint128_t v16_17
    v16_17.d = v5_3.d << 0x1a
    v16_17:4.d = v5_3:4.d << 0x1a
    v16_17:8.d = v5_3:8.d << 0x1a
    v16_17:0xc.d = v5_3:0xc.d << 0x1a
    uint128_t v17_19
    v17_19.d = v5_3.d u>> 0xb
    v17_19:4.d = v5_3:4.d u>> 0xb
    v17_19:8.d = v5_3:8.d u>> 0xb
    v17_19:0xc.d = v5_3:0xc.d u>> 0xb
    v18_9.d = v5_3.d << 0x15
    v18_9:4.d = v5_3:4.d << 0x15
    v18_9:8.d = v5_3:8.d << 0x15
    v18_9:0xc.d = v5_3:0xc.d << 0x15
    v19_11.d = v5_3.d u>> 0x19
    v19_11:4.d = v5_3:4.d u>> 0x19
    v19_11:8.d = v5_3:8.d u>> 0x19
    v19_11:0xc.d = v5_3:0xc.d u>> 0x19
    v20_7.d = v5_3.d << 7
    v20_7:4.d = v5_3:4.d << 7
    v20_7:8.d = v5_3:8.d << 7
    v20_7:0xc.d = v5_3:0xc.d << 7
    uint128_t v6_14 = vorrq_s8(v16_17, v6_13)
    v16_17.d = v0_3.d u>> 2
    v16_17:4.d = v0_3:4.d u>> 2
    v16_17:8.d = v0_3:8.d u>> 2
    v16_17:0xc.d = v0_3:0xc.d u>> 2
    uint128_t v17_20 = vorrq_s8(v18_9, v17_19)
    v18_9.d = v0_3.d << 0x1e
    v18_9:4.d = v0_3:4.d << 0x1e
    v18_9:8.d = v0_3:8.d << 0x1e
    v18_9:0xc.d = v0_3:0xc.d << 0x1e
    v19 = vorrq_s8(v20_7, v19_11)
    v20_7.d = v0_3.d u>> 0xd
    v20_7:4.d = v0_3:4.d u>> 0xd
    v20_7:8.d = v0_3:8.d u>> 0xd
    v20_7:0xc.d = v0_3:0xc.d u>> 0xd
    v22.d = v0_3.d << 0x13
    v22:4.d = v0_3:4.d << 0x13
    v22:8.d = v0_3:8.d << 0x13
    v22:0xc.d = v0_3:0xc.d << 0x13
    uint128_t v16_18 = vorrq_s8(v18_9, v16_17)
    uint128_t v20_8 = vorrq_s8(v22, v20_7)
    v18_9.d = v0_3.d u>> 0x16
    v18_9:4.d = v0_3:4.d u>> 0x16
    v18_9:8.d = v0_3:8.d u>> 0x16
    v18_9:0xc.d = v0_3:0xc.d u>> 0x16
    v22.d = v0_3.d << 0xa
    v22:4.d = v0_3:4.d << 0xa
    v22:8.d = v0_3:8.d << 0xa
    v22:0xc.d = v0_3:0xc.d << 0xa
    uint128_t v20_9 = vorrq_s8(v0_3, v1_2)
    v18 = vorrq_s8(v22, v18_9)
    i += 0x20
    v17 = v6_10 + v23 + (v5_3 & v7_2) + (v3_3 & not.o(v5_3)) + (v17_20 ^ v6_14 ^ v19)
    v6_3 = v17 + v2_3
    v2_2 = vorrq_s8(v20_9 & v4_3, v0_3 & v1_2) + (v20_8 ^ v16_18 ^ v18) + v17
while (i != 0x100)

uint128_t v16_22 = vzip1q_s32(v2_2, v1_2)
uint128_t v1_4 = vzip2q_s32(v2_2, v1_2)
uint128_t v2_4 = vzip1q_s32(v0_3, v4_3)
uint128_t v0_5 = vzip2q_s32(v0_3, v4_3)
uint128_t v4_12 = vzip1q_s32(v6_3, v7_2)
uint128_t v6_17 = vzip2q_s32(v6_3, v7_2)
uint128_t v7_15 = vzip1q_s32(v5_3, v3_3)
uint128_t v3_7 = vzip2q_s32(v5_3, v3_3)
uint128_t v5_26 = vzip1q_s32(v16_22, v2_4)
uint128_t v2_5 = vzip2q_s32(v16_22, v2_4)
uint128_t v16_23 = vzip1q_s32(v1_4, v0_5)
uint128_t v0_6 = vzip2q_s32(v1_4, v0_5)
uint128_t v1_5 = vzip1q_s32(v4_12, v7_15)
uint128_t v4_13 = vzip2q_s32(v4_12, v7_15)
uint128_t v7_16 = vzip1q_s32(v6_17, v3_7)
uint128_t v3_8 = vzip2q_s32(v6_17, v3_7)
uint128_t v5_27 = vrev32q_s8(v5_26)
uint128_t v1_6 = vrev32q_s8(v1_5)
uint128_t v2_6 = vrev32q_s8(v2_5)
uint128_t v4_14 = vrev32q_s8(v4_13)
uint128_t v6_18 = vrev32q_s8(v16_23)
uint128_t v7_17 = vrev32q_s8(v7_16)
uint128_t v0_7 = vrev32q_s8(v0_6)
uint128_t v3_9 = vrev32q_s8(v3_8)
int128_t* entry_x2
*entry_x2 = v5_27
entry_x2[1] = v1_6
entry_x2[2] = v2_6
entry_x2[3] = v4_14
entry_x2[4] = v6_18
entry_x2[5] = v7_17
entry_x2[6] = v0_7
entry_x2[7] = v3_9
