// 函数: _ZNK5Botan7SHACAL214simd_decrypt_4EPKhPh
// 地址: 0xe22fd8
// 来自: E:\torrent\Cursor\Dominion_1.0.3315\split_config.arm64_v8a\lib\arm64-v8a\libTGGAndroid.so

uint128_t entry_v10
int64_t var_20 = entry_v10.q
uint128_t entry_v9
int64_t var_10 = entry_v9.q
uint128_t entry_v8
int64_t var_8 = entry_v8.q
uint128_t v1 = *(arg2 + 0x10)
uint128_t v2 = *(arg2 + 0x20)
uint128_t v3 = *(arg2 + 0x30)
uint128_t v4 = *(arg2 + 0x40)
uint128_t v5 = *(arg2 + 0x50)
uint128_t v6 = *(arg2 + 0x60)
uint128_t v7 = *(arg2 + 0x70)
void* x9 = *(arg1 + 8)
uint128_t v0_1 = vrev32q_s8(*arg2)
uint128_t v1_1 = vrev32q_s8(v1)
uint128_t v2_1 = vrev32q_s8(v2)
uint128_t v3_1 = vrev32q_s8(v3)
uint128_t v4_1 = vrev32q_s8(v4)
uint128_t v5_1 = vrev32q_s8(v5)
uint128_t v6_1 = vrev32q_s8(v6)
uint128_t v7_1 = vrev32q_s8(v7)
uint128_t v16 = vzip1q_s32(v0_1, v4_1)
uint128_t v0_2 = vzip2q_s32(v0_1, v4_1)
uint128_t v4_2 = vzip1q_s32(v2_1, v6_1)
uint128_t v2_2 = vzip2q_s32(v2_1, v6_1)
uint128_t v17 = vzip1q_s32(v1_1, v5_1)
uint128_t v18 = vzip2q_s32(v1_1, v5_1)
uint128_t v19 = vzip1q_s32(v3_1, v7_1)
uint128_t v20 = vzip2q_s32(v3_1, v7_1)
int64_t i = 0
uint128_t v1_2 = vzip1q_s32(v16, v4_2)
uint128_t v6_2 = vzip2q_s32(v16, v4_2)
uint128_t v7_2 = vzip1q_s32(v0_2, v2_2)
uint128_t v5_2 = vzip2q_s32(v0_2, v2_2)
uint128_t v4_3 = vzip1q_s32(v17, v19)
uint128_t v3_2 = vzip2q_s32(v17, v19)
uint128_t v2_3 = vzip1q_s32(v18, v20)
uint128_t v0_3 = vzip2q_s32(v18, v20)

do
    v16.d = v6_2.d u>> 2
    v16:4.d = v6_2:4.d u>> 2
    v16:8.d = v6_2:8.d u>> 2
    v16:0xc.d = v6_2:0xc.d u>> 2
    v17.d = v6_2.d << 0x1e
    v17:4.d = v6_2:4.d << 0x1e
    v17:8.d = v6_2:8.d << 0x1e
    v17:0xc.d = v6_2:0xc.d << 0x1e
    v18.d = v6_2.d u>> 0xd
    v18:4.d = v6_2:4.d u>> 0xd
    v18:8.d = v6_2:8.d u>> 0xd
    v18:0xc.d = v6_2:0xc.d u>> 0xd
    v19.d = v6_2.d << 0x13
    v19:4.d = v6_2:4.d << 0x13
    v19:8.d = v6_2:8.d << 0x13
    v19:0xc.d = v6_2:0xc.d << 0x13
    v20.d = v6_2.d u>> 0x16
    v20:4.d = v6_2:4.d u>> 0x16
    v20:8.d = v6_2:8.d u>> 0x16
    v20:0xc.d = v6_2:0xc.d u>> 0x16
    uint128_t v21
    v21.d = v6_2.d << 0xa
    v21:4.d = v6_2:4.d << 0xa
    v21:8.d = v6_2:8.d << 0xa
    v21:0xc.d = v6_2:0xc.d << 0xa
    uint128_t v22
    v22.d = v3_2.d u>> 6
    v22:4.d = v3_2:4.d u>> 6
    v22:8.d = v3_2:8.d u>> 6
    v22:0xc.d = v3_2:0xc.d u>> 6
    uint128_t v23
    v23.d = v3_2.d << 0x1a
    v23:4.d = v3_2:4.d << 0x1a
    v23:8.d = v3_2:8.d << 0x1a
    v23:0xc.d = v3_2:0xc.d << 0x1a
    uint128_t v24
    v24.d = v3_2.d u>> 0xb
    v24:4.d = v3_2:4.d u>> 0xb
    v24:8.d = v3_2:8.d u>> 0xb
    v24:0xc.d = v3_2:0xc.d u>> 0xb
    uint128_t v25
    v25.d = v3_2.d << 0x15
    v25:4.d = v3_2:4.d << 0x15
    v25:8.d = v3_2:8.d << 0x15
    v25:0xc.d = v3_2:0xc.d << 0x15
    uint128_t v26
    v26.d = v3_2.d u>> 0x19
    v26:4.d = v3_2:4.d u>> 0x19
    v26:8.d = v3_2:8.d u>> 0x19
    v26:0xc.d = v3_2:0xc.d u>> 0x19
    uint128_t v27
    v27.d = v3_2.d << 7
    v27:4.d = v3_2:4.d << 7
    v27:8.d = v3_2:8.d << 7
    v27:0xc.d = v3_2:0xc.d << 7
    uint128_t v28
    v28.d = v7_2.d u>> 2
    v28:4.d = v7_2:4.d u>> 2
    v28:8.d = v7_2:8.d u>> 2
    v28:0xc.d = v7_2:0xc.d u>> 2
    uint128_t v29
    v29.d = v7_2.d << 0x1e
    v29:4.d = v7_2:4.d << 0x1e
    v29:8.d = v7_2:8.d << 0x1e
    v29:0xc.d = v7_2:0xc.d << 0x1e
    uint128_t v30
    v30.d = v7_2.d u>> 0xd
    v30:4.d = v7_2:4.d u>> 0xd
    v30:8.d = v7_2:8.d u>> 0xd
    v30:0xc.d = v7_2:0xc.d u>> 0xd
    uint128_t v31
    v31.d = v7_2.d << 0x13
    v31:4.d = v7_2:4.d << 0x13
    v31:8.d = v7_2:8.d << 0x13
    v31:0xc.d = v7_2:0xc.d << 0x13
    entry_v8.d = v7_2.d u>> 0x16
    entry_v8:4.d = v7_2:4.d u>> 0x16
    entry_v8:8.d = v7_2:8.d u>> 0x16
    entry_v8:0xc.d = v7_2:0xc.d u>> 0x16
    entry_v9.d = v7_2.d << 0xa
    entry_v9:4.d = v7_2:4.d << 0xa
    entry_v9:8.d = v7_2:8.d << 0xa
    entry_v9:0xc.d = v7_2:0xc.d << 0xa
    entry_v10.d = v2_3.d u>> 6
    entry_v10:4.d = v2_3:4.d u>> 6
    entry_v10:8.d = v2_3:8.d u>> 6
    entry_v10:0xc.d = v2_3:0xc.d u>> 6
    uint128_t v16_1 = vorrq_s8(v17, v16)
    v17.d = v2_3.d << 0x1a
    v17:4.d = v2_3:4.d << 0x1a
    v17:8.d = v2_3:8.d << 0x1a
    v17:0xc.d = v2_3:0xc.d << 0x1a
    uint128_t v18_1 = vorrq_s8(v19, v18)
    v19.d = v2_3.d u>> 0xb
    v19:4.d = v2_3:4.d u>> 0xb
    v19:8.d = v2_3:8.d u>> 0xb
    v19:0xc.d = v2_3:0xc.d u>> 0xb
    uint128_t v20_1 = vorrq_s8(v21, v20)
    v21.d = v2_3.d << 0x15
    v21:4.d = v2_3:4.d << 0x15
    v21:8.d = v2_3:8.d << 0x15
    v21:0xc.d = v2_3:0xc.d << 0x15
    uint128_t v22_1 = vorrq_s8(v23, v22)
    v23.d = v2_3.d u>> 0x19
    v23:4.d = v2_3:4.d u>> 0x19
    v23:8.d = v2_3:8.d u>> 0x19
    v23:0xc.d = v2_3:0xc.d u>> 0x19
    uint128_t v24_1 = vorrq_s8(v25, v24)
    v25.d = v2_3.d << 7
    v25:4.d = v2_3:4.d << 7
    v25:8.d = v2_3:8.d << 7
    v25:0xc.d = v2_3:0xc.d << 7
    v26 = vorrq_s8(v27, v26)
    v27.d = v5_2.d u>> 2
    v27:4.d = v5_2:4.d u>> 2
    v27:8.d = v5_2:8.d u>> 2
    v27:0xc.d = v5_2:0xc.d u>> 2
    v28 = vorrq_s8(v29, v28)
    v29.d = v5_2.d << 0x1e
    v29:4.d = v5_2:4.d << 0x1e
    v29:8.d = v5_2:8.d << 0x1e
    v29:0xc.d = v5_2:0xc.d << 0x1e
    v30 = vorrq_s8(v31, v30)
    v31.d = v5_2.d u>> 0xd
    v31:4.d = v5_2:4.d u>> 0xd
    v31:8.d = v5_2:8.d u>> 0xd
    v31:0xc.d = v5_2:0xc.d u>> 0xd
    entry_v8 = vorrq_s8(entry_v9, entry_v8)
    entry_v9.d = v5_2.d << 0x13
    entry_v9:4.d = v5_2:4.d << 0x13
    entry_v9:8.d = v5_2:8.d << 0x13
    entry_v9:0xc.d = v5_2:0xc.d << 0x13
    uint128_t v17_1 = vorrq_s8(v17, entry_v10)
    entry_v10.d = v5_2.d u>> 0x16
    entry_v10:4.d = v5_2:4.d u>> 0x16
    entry_v10:8.d = v5_2:8.d u>> 0x16
    entry_v10:0xc.d = v5_2:0xc.d u>> 0x16
    uint128_t v19_1 = vorrq_s8(v21, v19)
    v21.d = v5_2.d << 0xa
    v21:4.d = v5_2:4.d << 0xa
    v21:8.d = v5_2:8.d << 0xa
    v21:0xc.d = v5_2:0xc.d << 0xa
    uint128_t v23_1 = vorrq_s8(v25, v23)
    v25.d = v0_3.d u>> 6
    v25:4.d = v0_3:4.d u>> 6
    v25:8.d = v0_3:8.d u>> 6
    v25:0xc.d = v0_3:0xc.d u>> 6
    v27 = vorrq_s8(v29, v27)
    v29.d = v0_3.d << 0x1a
    v29:4.d = v0_3:4.d << 0x1a
    v29:8.d = v0_3:8.d << 0x1a
    v29:0xc.d = v0_3:0xc.d << 0x1a
    v31 = vorrq_s8(entry_v9, v31)
    entry_v9.d = v0_3.d u>> 0xb
    entry_v9:4.d = v0_3:4.d u>> 0xb
    entry_v9:8.d = v0_3:8.d u>> 0xb
    entry_v9:0xc.d = v0_3:0xc.d u>> 0xb
    uint128_t v21_1 = vorrq_s8(v21, entry_v10)
    entry_v10.d = v0_3.d << 0x15
    entry_v10:4.d = v0_3:4.d << 0x15
    entry_v10:8.d = v0_3:8.d << 0x15
    entry_v10:0xc.d = v0_3:0xc.d << 0x15
    uint128_t v25_1 = vorrq_s8(v29, v25)
    v29.d = v0_3.d u>> 0x19
    v29:4.d = v0_3:4.d u>> 0x19
    v29:8.d = v0_3:8.d u>> 0x19
    v29:0xc.d = v0_3:0xc.d u>> 0x19
    entry_v9 = vorrq_s8(entry_v10, entry_v9)
    entry_v10.d = v0_3.d << 7
    entry_v10:4.d = v0_3:4.d << 7
    entry_v10:8.d = v0_3:8.d << 7
    entry_v10:0xc.d = v0_3:0xc.d << 7
    v29 = vorrq_s8(entry_v10, v29)
    uint128_t v18_2 = vorrq_s8(v7_2, v6_2)
    int32_t* x10_1 = x9 + 0xfc + i
    uint128_t v22_4
    v22_4.d = *x10_1
    uint128_t v1_4 = v1_2 - (v18_1 ^ v16_1 ^ v20_1) - vorrq_s8(v5_2 & v18_2, v7_2 & v6_2)
    uint128_t v18_5 = vnegq_s32(v22_4)
    uint128_t v7_3 = vorrq_s8(v5_2, v7_2)
    uint128_t v4_4 = v4_3 - v1_4
    uint128_t v1_6 = v1_4 - ((v24_1 ^ v22_1 ^ v26) + (v2_3 & v3_2) + (v0_3 & not.o(v3_2)))
        + vdupq_laneq_s32(v18_5, 0)
    uint128_t v16_5
    v16_5.d = v4_4.d u>> 2
    v16_5:4.d = v4_4:4.d u>> 2
    v16_5:8.d = v4_4:8.d u>> 2
    v16_5:0xc.d = v4_4:0xc.d u>> 2
    uint128_t v18_6
    v18_6.d = v4_4.d << 0x1e
    v18_6:4.d = v4_4:4.d << 0x1e
    v18_6:8.d = v4_4:8.d << 0x1e
    v18_6:0xc.d = v4_4:0xc.d << 0x1e
    int32_t x11_1 = x10_1[-1]
    uint128_t v20_3
    v20_3.d = v4_4.d u>> 0xd
    v20_3:4.d = v4_4:4.d u>> 0xd
    v20_3:8.d = v4_4:8.d u>> 0xd
    v20_3:0xc.d = v4_4:0xc.d u>> 0xd
    v22_4.d = v4_4.d << 0x13
    v22_4:4.d = v4_4:4.d << 0x13
    v22_4:8.d = v4_4:8.d << 0x13
    v22_4:0xc.d = v4_4:0xc.d << 0x13
    uint128_t v7_5 = vorrq_s8(v4_4 & v7_3, v5_2 & v7_2)
    uint128_t v23_3
    v23_3.d = v4_4.d u>> 0x16
    v23_3:4.d = v4_4:4.d u>> 0x16
    v23_3:8.d = v4_4:8.d u>> 0x16
    v23_3:0xc.d = v4_4:0xc.d u>> 0x16
    uint128_t v16_6 = vorrq_s8(v18_6, v16_5)
    v18_6.d = v4_4.d << 0xa
    v18_6:4.d = v4_4:4.d << 0xa
    v18_6:8.d = v4_4:8.d << 0xa
    v18_6:0xc.d = v4_4:0xc.d << 0xa
    uint128_t v20_4 = vorrq_s8(v22_4, v20_3)
    uint128_t v18_7 = vorrq_s8(v18_6, v23_3)
    uint128_t v7_6 = v6_2 - (v30 ^ v28 ^ entry_v8) - v7_5
    uint128_t v6_3
    v6_3.d = v1_6.d u>> 6
    v6_3:4.d = v1_6:4.d u>> 6
    v6_3:8.d = v1_6:8.d u>> 6
    v6_3:0xc.d = v1_6:0xc.d u>> 6
    uint128_t v22_5
    v22_5.d = v1_6.d << 0x1a
    v22_5:4.d = v1_6:4.d << 0x1a
    v22_5:8.d = v1_6:8.d << 0x1a
    v22_5:0xc.d = v1_6:0xc.d << 0x1a
    uint128_t v23_4
    v23_4.d = v1_6.d u>> 0xb
    v23_4:4.d = v1_6:4.d u>> 0xb
    v23_4:8.d = v1_6:8.d u>> 0xb
    v23_4:0xc.d = v1_6:0xc.d u>> 0xb
    uint128_t v16_7 = v20_4 ^ v16_6
    v20_4.d = v1_6.d << 0x15
    v20_4:4.d = v1_6:4.d << 0x15
    v20_4:8.d = v1_6:8.d << 0x15
    v20_4:0xc.d = v1_6:0xc.d << 0x15
    uint128_t v22_6 = vorrq_s8(v22_5, v6_3)
    v6_3.d = v1_6.d u>> 0x19
    v6_3:4.d = v1_6:4.d u>> 0x19
    v6_3:8.d = v1_6:8.d u>> 0x19
    v6_3:0xc.d = v1_6:0xc.d u>> 0x19
    uint128_t v20_5 = vorrq_s8(v20_4, v23_4)
    v23_4.d = v1_6.d << 7
    v23_4:4.d = v1_6:4.d << 7
    v23_4:8.d = v1_6:8.d << 7
    v23_4:0xc.d = v1_6:0xc.d << 7
    uint128_t v23_5 = vorrq_s8(v23_4, v6_3)
    v6_3.d = float.s(x11_1)
    uint128_t v6_5 = v3_2 - v7_6
    uint128_t v3_3 = v7_6 + vdupq_laneq_s32(vnegq_s32(v6_3), 0)
    uint128_t v18_8 = vorrq_s8(v4_4, v5_2)
    int32_t x12_1 = x10_1[-3]
    int32_t x11_2 = x10_1[-2]
    uint128_t v3_4 = v3_3 - ((v19_1 ^ v17_1 ^ v23_1) + (v0_3 & v2_3) + (v1_6 & not.o(v2_3)))
    uint128_t v17_6 = v6_5 & v18_8
    uint128_t v5_3 = v20_5 ^ v22_6 ^ v23_5
    v18_8.d = v6_5.d u>> 2
    v18_8:4.d = v6_5:4.d u>> 2
    v18_8:8.d = v6_5:8.d u>> 2
    v18_8:0xc.d = v6_5:0xc.d u>> 2
    uint128_t v20_6
    v20_6.d = v6_5.d << 0x1e
    v20_6:4.d = v6_5:4.d << 0x1e
    v20_6:8.d = v6_5:8.d << 0x1e
    v20_6:0xc.d = v6_5:0xc.d << 0x1e
    v22_6.d = v6_5.d u>> 0xd
    v22_6:4.d = v6_5:4.d u>> 0xd
    v22_6:8.d = v6_5:8.d u>> 0xd
    v22_6:0xc.d = v6_5:0xc.d u>> 0xd
    v23_5.d = v6_5.d << 0x13
    v23_5:4.d = v6_5:4.d << 0x13
    v23_5:8.d = v6_5:8.d << 0x13
    v23_5:0xc.d = v6_5:0xc.d << 0x13
    uint128_t v7_8 = vorrq_s8(v17_6, v4_4 & v5_2)
    v17_6.d = v6_5.d u>> 0x16
    v17_6:4.d = v6_5:4.d u>> 0x16
    v17_6:8.d = v6_5:8.d u>> 0x16
    v17_6:0xc.d = v6_5:0xc.d u>> 0x16
    uint128_t v18_9 = vorrq_s8(v20_6, v18_8)
    v20_6.d = v6_5.d << 0xa
    v20_6:4.d = v6_5:4.d << 0xa
    v20_6:8.d = v6_5:8.d << 0xa
    v20_6:0xc.d = v6_5:0xc.d << 0xa
    uint128_t v22_7 = vorrq_s8(v23_5, v22_6)
    uint128_t v17_7 = vorrq_s8(v20_6, v17_6)
    v20_6.d = v3_4.d u>> 6
    v20_6:4.d = v3_4:4.d u>> 6
    v20_6:8.d = v3_4:8.d u>> 6
    v20_6:0xc.d = v3_4:0xc.d u>> 6
    v23_5.d = v3_4.d << 0x1a
    v23_5:4.d = v3_4:4.d << 0x1a
    v23_5:8.d = v3_4:8.d << 0x1a
    v23_5:0xc.d = v3_4:0xc.d << 0x1a
    uint128_t v19_5 = v7_2 - (v31 ^ v27 ^ v21_1) - v7_8
    v7_8.d = v3_4.d u>> 0xb
    v7_8:4.d = v3_4:4.d u>> 0xb
    v7_8:8.d = v3_4:8.d u>> 0xb
    v7_8:0xc.d = v3_4:0xc.d u>> 0xb
    uint128_t v18_10 = v22_7 ^ v18_9
    v22_7.d = v3_4.d << 0x15
    v22_7:4.d = v3_4:4.d << 0x15
    v22_7:8.d = v3_4:8.d << 0x15
    v22_7:0xc.d = v3_4:0xc.d << 0x15
    uint128_t v20_7 = vorrq_s8(v23_5, v20_6)
    v23_5.d = v3_4.d u>> 0x19
    v23_5:4.d = v3_4:4.d u>> 0x19
    v23_5:8.d = v3_4:8.d u>> 0x19
    v23_5:0xc.d = v3_4:0xc.d u>> 0x19
    uint128_t v22_8 = vorrq_s8(v22_7, v7_8)
    v7_8.d = v3_4.d << 7
    v7_8:4.d = v3_4:4.d << 7
    v7_8:8.d = v3_4:8.d << 7
    v7_8:0xc.d = v3_4:0xc.d << 7
    uint128_t v23_6 = vorrq_s8(v7_8, v23_5)
    v7_8.d = float.s(x11_2)
    uint128_t v7_10 = v2_3 - v19_5
    uint128_t v2_4 = v19_5 + vdupq_laneq_s32(vnegq_s32(v7_8), 0)
    v19_5.d = float.s(x12_1)
    uint128_t v17_9 = v22_8 ^ v20_7 ^ v23_6
    uint128_t v20_8
    v20_8.d = v7_10.d u>> 2
    v20_8:4.d = v7_10:4.d u>> 2
    v20_8:8.d = v7_10:8.d u>> 2
    v20_8:0xc.d = v7_10:0xc.d u>> 2
    v23_6.d = v7_10.d << 0x1e
    v23_6:4.d = v7_10:4.d << 0x1e
    v23_6:8.d = v7_10:8.d << 0x1e
    v23_6:0xc.d = v7_10:0xc.d << 0x1e
    uint128_t v2_6 = v2_4 - ((entry_v9 ^ v25_1 ^ v29) + (v1_6 & v0_3)) - (v3_4 & not.o(v0_3))
    uint128_t v21_4
    v21_4.d = v7_10.d u>> 0xd
    v21_4:4.d = v7_10:4.d u>> 0xd
    v21_4:8.d = v7_10:8.d u>> 0xd
    v21_4:0xc.d = v7_10:0xc.d u>> 0xd
    uint128_t v18_12 = vorrq_s8(v7_10 & vorrq_s8(v6_5, v4_4), v6_5 & v4_4)
    uint128_t v22_10
    v22_10.d = v7_10.d << 0x13
    v22_10:4.d = v7_10:4.d << 0x13
    v22_10:8.d = v7_10:8.d << 0x13
    v22_10:0xc.d = v7_10:0xc.d << 0x13
    uint128_t v20_9 = vorrq_s8(v23_6, v20_8)
    v23_6.d = v7_10.d u>> 0x16
    v23_6:4.d = v7_10:4.d u>> 0x16
    v23_6:8.d = v7_10:8.d u>> 0x16
    v23_6:0xc.d = v7_10:0xc.d u>> 0x16
    uint128_t v21_5 = vorrq_s8(v22_10, v21_4)
    v22_10.d = v7_10.d << 0xa
    v22_10:4.d = v7_10:4.d << 0xa
    v22_10:8.d = v7_10:8.d << 0xa
    v22_10:0xc.d = v7_10:0xc.d << 0xa
    uint128_t v19_6 = vnegq_s32(v19_5)
    int32_t x11_3 = x10_1[-5]
    int32_t x12_2 = x10_1[-4]
    uint128_t v22_11 = vorrq_s8(v22_10, v23_6)
    uint128_t v16_10 = v5_2 - (v16_7 ^ v18_7) - v18_12
    uint128_t v18_13 = v21_5 ^ v20_9
    v20_9.d = v2_6.d u>> 6
    v20_9:4.d = v2_6:4.d u>> 6
    v20_9:8.d = v2_6:8.d u>> 6
    v20_9:0xc.d = v2_6:0xc.d u>> 6
    v21_5.d = v2_6.d << 0x1a
    v21_5:4.d = v2_6:4.d << 0x1a
    v21_5:8.d = v2_6:8.d << 0x1a
    v21_5:0xc.d = v2_6:0xc.d << 0x1a
    uint128_t v0_4 = v0_3 - v16_10
    uint128_t v16_11 = vdupq_laneq_s32(v19_6, 0) + v16_10
    uint128_t v19_7
    v19_7.d = v2_6.d u>> 0xb
    v19_7:4.d = v2_6:4.d u>> 0xb
    v19_7:8.d = v2_6:8.d u>> 0xb
    v19_7:0xc.d = v2_6:0xc.d u>> 0xb
    uint128_t v18_14 = v18_13 ^ v22_11
    v22_11.d = v2_6.d << 0x15
    v22_11:4.d = v2_6:4.d << 0x15
    v22_11:8.d = v2_6:8.d << 0x15
    v22_11:0xc.d = v2_6:0xc.d << 0x15
    uint128_t v20_10 = vorrq_s8(v21_5, v20_9)
    v21_5.d = v2_6.d u>> 0x19
    v21_5:4.d = v2_6:4.d u>> 0x19
    v21_5:8.d = v2_6:8.d u>> 0x19
    v21_5:0xc.d = v2_6:0xc.d u>> 0x19
    uint128_t v19_8 = vorrq_s8(v22_11, v19_7)
    v22_11.d = v2_6.d << 7
    v22_11:4.d = v2_6:4.d << 7
    v22_11:8.d = v2_6:8.d << 7
    v22_11:0xc.d = v2_6:0xc.d << 7
    uint128_t v6_6 = vorrq_s8(v7_10, v6_5)
    uint128_t v21_6 = vorrq_s8(v22_11, v21_5)
    uint128_t v19_9 = v19_8 ^ v20_10
    v23_6.d = float.s(x12_2)
    v20_10.d = v0_4.d u>> 2
    v20_10:4.d = v0_4:4.d u>> 2
    v20_10:8.d = v0_4:8.d u>> 2
    v20_10:0xc.d = v0_4:0xc.d u>> 2
    uint128_t v22_12
    v22_12.d = v0_4.d << 0x1e
    v22_12:4.d = v0_4:4.d << 0x1e
    v22_12:8.d = v0_4:8.d << 0x1e
    v22_12:0xc.d = v0_4:0xc.d << 0x1e
    uint128_t v6_8 = vorrq_s8(v0_4 & v6_6, v7_10 & v6_5)
    uint128_t v16_12
    v16_12.d = v0_4.d u>> 0xd
    v16_12:4.d = v0_4:4.d u>> 0xd
    v16_12:8.d = v0_4:8.d u>> 0xd
    v16_12:0xc.d = v0_4:0xc.d u>> 0xd
    uint128_t v19_10 = v19_9 ^ v21_6
    v21_6.d = v0_4.d << 0x13
    v21_6:4.d = v0_4:4.d << 0x13
    v21_6:8.d = v0_4:8.d << 0x13
    v21_6:0xc.d = v0_4:0xc.d << 0x13
    uint128_t v23_7 = vnegq_s32(v23_6)
    uint128_t v20_11 = vorrq_s8(v22_12, v20_10)
    v22_12.d = v0_4.d u>> 0x16
    v22_12:4.d = v0_4:4.d u>> 0x16
    v22_12:8.d = v0_4:8.d u>> 0x16
    v22_12:0xc.d = v0_4:0xc.d u>> 0x16
    uint128_t v16_13 = vorrq_s8(v21_6, v16_12)
    v21_6.d = v0_4.d << 0xa
    v21_6:4.d = v0_4:4.d << 0xa
    v21_6:8.d = v0_4:8.d << 0xa
    v21_6:0xc.d = v0_4:0xc.d << 0xa
    uint128_t v4_6 = v4_4 - (v18_10 ^ v17_7) - v6_8
    uint128_t v5_6 = v16_11 - v5_3 - (v3_4 & v1_6) - (v2_6 & not.o(v1_6))
    v1_2 = v1_6 - v4_6
    uint128_t v21_7
    v21_7.d = v5_6.d u>> 6
    v21_7:4.d = v5_6:4.d u>> 6
    v21_7:8.d = v5_6:8.d u>> 6
    v21_7:0xc.d = v5_6:0xc.d u>> 6
    uint128_t v23_8
    v23_8.d = v5_6.d << 0x1a
    v23_8:4.d = v5_6:4.d << 0x1a
    v23_8:8.d = v5_6:8.d << 0x1a
    v23_8:0xc.d = v5_6:0xc.d << 0x1a
    uint128_t v4_8 = vdupq_laneq_s32(v23_7, 0) + v4_6 - v17_9
    v17_9.d = v5_6.d u>> 0xb
    v17_9:4.d = v5_6:4.d u>> 0xb
    v17_9:8.d = v5_6:8.d u>> 0xb
    v17_9:0xc.d = v5_6:0xc.d u>> 0xb
    uint128_t v7_11 = v7_10 - (v16_13 ^ v20_11 ^ vorrq_s8(v21_6, v22_12))
    uint128_t v16_15
    v16_15.d = v5_6.d << 0x15
    v16_15:4.d = v5_6:4.d << 0x15
    v16_15:8.d = v5_6:8.d << 0x15
    v16_15:0xc.d = v5_6:0xc.d << 0x15
    uint128_t v20_13 = v1_2 & vorrq_s8(v0_4, v7_10)
    uint128_t v21_8 = vorrq_s8(v23_8, v21_7)
    v23_8.d = v5_6.d u>> 0x19
    v23_8:4.d = v5_6:4.d u>> 0x19
    v23_8:8.d = v5_6:8.d u>> 0x19
    v23_8:0xc.d = v5_6:0xc.d u>> 0x19
    uint128_t v16_16 = vorrq_s8(v16_15, v17_9)
    v17_9.d = v5_6.d << 7
    v17_9:4.d = v5_6:4.d << 7
    v17_9:8.d = v5_6:8.d << 7
    v17_9:0xc.d = v5_6:0xc.d << 7
    uint128_t v6_10 = vorrq_s8(v20_13, v0_4 & v7_10)
    uint128_t v17_10 = vorrq_s8(v17_9, v23_8)
    v23_8.d = v1_2.d u>> 2
    v23_8:4.d = v1_2:4.d u>> 2
    v23_8:8.d = v1_2:8.d u>> 2
    v23_8:0xc.d = v1_2:0xc.d u>> 2
    uint128_t v22_14
    v22_14.d = v1_2.d << 0x1e
    v22_14:4.d = v1_2:4.d << 0x1e
    v22_14:8.d = v1_2:8.d << 0x1e
    v22_14:0xc.d = v1_2:0xc.d << 0x1e
    v20_13.d = v1_2.d u>> 0xd
    v20_13:4.d = v1_2:4.d u>> 0xd
    v20_13:8.d = v1_2:8.d u>> 0xd
    v20_13:0xc.d = v1_2:0xc.d u>> 0xd
    uint128_t v16_17 = v16_16 ^ v21_8
    v21_8.d = v1_2.d << 0x13
    v21_8:4.d = v1_2:4.d << 0x13
    v21_8:8.d = v1_2:8.d << 0x13
    v21_8:0xc.d = v1_2:0xc.d << 0x13
    uint128_t v18_16 = v6_5 - v18_14 - v6_10
    v6_10.d = float.s(x11_3)
    v22 = vorrq_s8(v22_14, v23_8)
    v23_8.d = v1_2.d u>> 0x16
    v23_8:4.d = v1_2:4.d u>> 0x16
    v23_8:8.d = v1_2:8.d u>> 0x16
    v23_8:0xc.d = v1_2:0xc.d u>> 0x16
    uint128_t v20_14 = vorrq_s8(v21_8, v20_13)
    v21_8.d = v1_2.d << 0xa
    v21_8:4.d = v1_2:4.d << 0xa
    v21_8:8.d = v1_2:8.d << 0xa
    v21_8:0xc.d = v1_2:0xc.d << 0xa
    uint128_t v6_11 = vnegq_s32(v6_10)
    uint128_t v21_9 = vorrq_s8(v21_8, v23_8)
    v4_3 = v4_8 - (v2_6 & v3_4) - (v5_6 & not.o(v3_4))
    v6_2 = v3_4 - v18_16
    uint128_t v3_5 = v18_16 + vdupq_laneq_s32(v6_11, 0)
    uint128_t v17_12 = v20_14 ^ v22 ^ v21_9
    v18_16.d = v4_3.d u>> 6
    v18_16:4.d = v4_3:4.d u>> 6
    v18_16:8.d = v4_3:8.d u>> 6
    v18_16:0xc.d = v4_3:0xc.d u>> 6
    uint128_t v20_15
    v20_15.d = v4_3.d << 0x1a
    v20_15:4.d = v4_3:4.d << 0x1a
    v20_15:8.d = v4_3:8.d << 0x1a
    v20_15:0xc.d = v4_3:0xc.d << 0x1a
    v21_9.d = v4_3.d u>> 0xb
    v21_9:4.d = v4_3:4.d u>> 0xb
    v21_9:8.d = v4_3:8.d u>> 0xb
    v21_9:0xc.d = v4_3:0xc.d u>> 0xb
    uint128_t v3_6 = v3_5 - v19_10
    v19_10.d = v4_3.d << 0x15
    v19_10:4.d = v4_3:4.d << 0x15
    v19_10:8.d = v4_3:8.d << 0x15
    v19_10:0xc.d = v4_3:0xc.d << 0x15
    uint128_t v18_17 = vorrq_s8(v20_15, v18_16)
    v20_15.d = v4_3.d u>> 0x19
    v20_15:4.d = v4_3:4.d u>> 0x19
    v20_15:8.d = v4_3:8.d u>> 0x19
    v20_15:0xc.d = v4_3:0xc.d u>> 0x19
    uint128_t v19_11 = vorrq_s8(v19_10, v21_9)
    v21_9.d = v4_3.d << 7
    v21_9:4.d = v4_3:4.d << 7
    v21_9:8.d = v4_3:8.d << 7
    v21_9:0xc.d = v4_3:0xc.d << 7
    uint128_t v20_16 = vorrq_s8(v21_9, v20_15)
    uint64_t x10_2 = zx.q(x10_1[-7])
    int32_t x11_4 = *(x10_2 - 0x18)
    uint128_t v0_8 = v7_11 - vorrq_s8(v6_2 & vorrq_s8(v1_2, v0_4), v1_2 & v0_4)
    v7_11.d = float.s(x11_4)
    v7_2 = v2_6 - v0_8
    uint128_t v0_11 =
        v0_8 + vdupq_laneq_s32(vnegq_s32(v7_11), 0) - (v16_17 ^ v17_10) - (v4_3 & v5_6)
    v3_2 = v3_6 - (v5_6 & v2_6) - (v4_3 & not.o(v2_6))
    uint128_t v19_12
    v19_12.d = float.s(x10_2.d)
    uint128_t v2_9 = v7_2 & vorrq_s8(v6_2, v1_2)
    uint128_t v19_13 = vnegq_s32(v19_12)
    uint128_t v16_21 = v0_4 - v17_12 - vorrq_s8(v2_9, v6_2 & v1_2)
    v5_2 = v5_6 - v16_21
    v2_3 = v0_11 - (v3_2 & not.o(v5_6))
    i -= 0x20
    v0_3 = v16_21 + vdupq_laneq_s32(v19_13, 0) - (v19_11 ^ v18_17 ^ v20_16) - (v3_2 & v4_3)
        - (v2_3 & not.o(v4_3))
while (i != -0x100)

uint128_t v16_23 = vzip1q_s32(v1_2, v7_2)
uint128_t v1_7 = vzip2q_s32(v1_2, v7_2)
uint128_t v7_13 = vzip1q_s32(v6_2, v5_2)
uint128_t v5_7 = vzip2q_s32(v6_2, v5_2)
uint128_t v6_12 = vzip1q_s32(v4_3, v2_3)
uint128_t v2_11 = vzip2q_s32(v4_3, v2_3)
uint128_t v4_10 = vzip1q_s32(v3_2, v0_3)
uint128_t v0_14 = vzip2q_s32(v3_2, v0_3)
entry_v9.q = var_10
entry_v8.q = var_8
uint128_t v3_8 = vzip1q_s32(v16_23, v7_13)
uint128_t v7_14 = vzip2q_s32(v16_23, v7_13)
uint128_t v16_24 = vzip1q_s32(v1_7, v5_7)
uint128_t v1_8 = vzip2q_s32(v1_7, v5_7)
uint128_t v5_8 = vzip1q_s32(v6_12, v4_10)
uint128_t v4_11 = vzip2q_s32(v6_12, v4_10)
uint128_t v6_13 = vzip1q_s32(v2_11, v0_14)
uint128_t v0_15 = vzip2q_s32(v2_11, v0_14)
uint128_t v2_12 = vrev32q_s8(v3_8)
uint128_t v3_9 = vrev32q_s8(v5_8)
uint128_t v5_9 = vrev32q_s8(v7_14)
uint128_t v4_12 = vrev32q_s8(v4_11)
uint128_t v7_15 = vrev32q_s8(v16_24)
uint128_t v6_14 = vrev32q_s8(v6_13)
uint128_t v1_9 = vrev32q_s8(v1_8)
uint128_t v0_16 = vrev32q_s8(v0_15)
int128_t* entry_x2
*entry_x2 = v2_12
entry_x2[1] = v3_9
entry_x2[2] = v5_9
entry_x2[3] = v4_12
entry_x2[4] = v7_15
entry_x2[5] = v6_14
entry_x2[6] = v1_9
entry_x2[7] = v0_16
entry_v10.q = var_20
