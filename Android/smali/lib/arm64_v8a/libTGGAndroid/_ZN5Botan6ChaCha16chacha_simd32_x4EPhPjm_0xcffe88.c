// 函数: _ZN5Botan6ChaCha16chacha_simd32_x4EPhPjm
// 地址: 0xcffe88
// 来自: E:\torrent\Cursor\Dominion_1.0.3315\split_config.arm64_v8a\lib\arm64-v8a\libTGGAndroid.so

int64_t entry_x3

if ((entry_x3.d & 1) != 0)
    uint8_t* x0
    uint8_t* x1
    uint64_t x2
    x0, x1, x2 = Botan::assertion_failure("rounds % 2 == 0", "Valid rounds", "chacha_simd32_x4", 
        "..\..\ExternalCode\botan\botanAM\arm\botan_all_android.cpp", 0x55b4)
    return Botan::ChaCha::cipher(x0, x1, x2) __tailcall

int32_t x8 = *(arg3 + 0x30)
int32_t temp0 = *arg3
int128_t v12
v12.d = temp0
v12:4.d = temp0
v12:8.d = temp0
v12:0xc.d = temp0
int32_t temp0_1 = *(arg3 + 8)
int128_t v14
v14.d = temp0_1
v14:4.d = temp0_1
v14:8.d = temp0_1
v14:0xc.d = temp0_1
int32_t temp0_2 = *(arg3 + 0xc)
uint128_t v15
v15.d = temp0_2
v15:4.d = temp0_2
v15:8.d = temp0_2
v15:0xc.d = temp0_2
int32_t temp0_3 = *(arg3 + 0x10)
uint128_t v11
v11.d = temp0_3
v11:4.d = temp0_3
v11:8.d = temp0_3
v11:0xc.d = temp0_3
int32_t temp0_4 = *(arg3 + 0x14)
uint128_t v4
v4.d = temp0_4
v4:4.d = temp0_4
v4:8.d = temp0_4
v4:0xc.d = temp0_4
int32_t temp0_5 = *(arg3 + 0x18)
uint128_t v5
v5.d = temp0_5
v5:4.d = temp0_5
v5:8.d = temp0_5
v5:0xc.d = temp0_5
int32_t temp0_6 = *(arg3 + 0x1c)
uint128_t v6
v6.d = temp0_6
v6:4.d = temp0_6
v6:8.d = temp0_6
v6:0xc.d = temp0_6
int32_t temp0_7 = *(arg3 + 0x20)
int128_t v7
v7.d = temp0_7
v7:4.d = temp0_7
v7:8.d = temp0_7
v7:0xc.d = temp0_7
int32_t temp0_8 = *(arg3 + 0x24)
int128_t v16
v16.d = temp0_8
v16:4.d = temp0_8
v16:8.d = temp0_8
v16:0xc.d = temp0_8
int32_t temp0_9 = *(arg3 + 0x28)
int128_t v17
v17.d = temp0_9
v17:4.d = temp0_9
v17:8.d = temp0_9
v17:0xc.d = temp0_9
int32_t temp0_10 = *(arg3 + 0x2c)
int128_t v18
v18.d = temp0_10
v18:4.d = temp0_10
v18:8.d = temp0_10
v18:0xc.d = temp0_10
int128_t v22
v22.q = 0
v22:8.q = 0
int32_t temp0_11 = *(arg3 + 0x34)
int128_t v24
v24.d = temp0_11
v24:4.d = temp0_11
v24:8.d = temp0_11
v24:0xc.d = temp0_11
int32_t temp0_12 = *(arg3 + 0x38)
uint128_t v20
v20.d = temp0_12
v20:4.d = temp0_12
v20:8.d = temp0_12
v20:0xc.d = temp0_12
int32_t temp0_13 = *(arg3 + 0x3c)
uint128_t v19
v19.d = temp0_13
v19:4.d = temp0_13
v19:8.d = temp0_13
v19:0xc.d = temp0_13
int32_t temp0_14 = *arg3
uint128_t v23
v23.d = temp0_14
v23:4.d = temp0_14
v23:8.d = temp0_14
v23:0xc.d = temp0_14
v22:4.d = x8 == 0xffffffff ? 1 : 0
int128_t v25
v25.d = x8
v25:4.d = x8
v25:8.d = x8
v25:0xc.d = x8
v22:8.d = x8 != 0xfffffffd && x8 u>= 0xfffffffd ? 1 : 0
v22:0xc.d = x8 != 0xfffffffc && x8 u>= 0xfffffffc ? 1 : 0
uint128_t v21_1 = v25 + data_71c940
uint128_t v22_1 = v24 + v22
uint64_t i_1 = entry_x3 u>> 1
uint128_t v26 = v19
uint128_t v31 = v20
uint128_t v30 = v22_1
uint128_t v29 = v21_1
int128_t v24_1 = v18
int128_t v25_1 = v17
int128_t v27 = v16
int128_t v28 = v7
uint128_t v10 = v6
uint128_t v9 = v5
uint128_t v8 = v4
uint128_t v13 = v23
uint128_t var_90 = v11
int128_t var_80 = v12
uint128_t var_70 = v15
int128_t var_60 = v14

if (i_1 != 0)
    v12 = var_80
    v15 = var_70
    v14 = var_60
    v11 = var_90
    v13 = v23
    v8 = v4
    v9 = v5
    v10 = v6
    v28 = v7
    v27 = v16
    v25_1 = v17
    v24_1 = v18
    v29 = v21_1
    v30 = v22_1
    v31 = v20
    v26 = v19
    uint64_t i
    
    do
        int128_t v12_1 = v12 + v11
        uint128_t v13_1 = v13 + v8
        uint128_t v29_1 = v12_1 ^ v29
        int128_t v14_1 = v14 + v9
        uint128_t v30_1 = v13_1 ^ v30
        uint128_t v0
        v0.d = v29_1.d u>> 0x10
        v0:4.d = v29_1:4.d u>> 0x10
        v0:8.d = v29_1:8.d u>> 0x10
        v0:0xc.d = v29_1:0xc.d u>> 0x10
        v29_1.d <<= 0x10
        v29_1:4.d <<= 0x10
        v29_1:8.d <<= 0x10
        v29_1:0xc.d <<= 0x10
        uint128_t v15_1 = v15 + v10
        uint128_t v31_1 = v14_1 ^ v31
        uint128_t v0_1 = vorrq_s8(v29_1, v0)
        v29_1.d = v30_1.d u>> 0x10
        v29_1:4.d = v30_1:4.d u>> 0x10
        v29_1:8.d = v30_1:8.d u>> 0x10
        v29_1:0xc.d = v30_1:0xc.d u>> 0x10
        v30_1.d <<= 0x10
        v30_1:4.d <<= 0x10
        v30_1:8.d <<= 0x10
        v30_1:0xc.d <<= 0x10
        uint128_t v26_1 = v15_1 ^ v26
        uint128_t v29_2 = vorrq_s8(v30_1, v29_1)
        v30_1.d = v31_1.d u>> 0x10
        v30_1:4.d = v31_1:4.d u>> 0x10
        v30_1:8.d = v31_1:8.d u>> 0x10
        v30_1:0xc.d = v31_1:0xc.d u>> 0x10
        v31_1.d <<= 0x10
        v31_1:4.d <<= 0x10
        v31_1:8.d <<= 0x10
        v31_1:0xc.d <<= 0x10
        uint128_t v30_2 = vorrq_s8(v31_1, v30_1)
        v31_1.d = v26_1.d u>> 0x10
        v31_1:4.d = v26_1:4.d u>> 0x10
        v31_1:8.d = v26_1:8.d u>> 0x10
        v31_1:0xc.d = v26_1:0xc.d u>> 0x10
        v26_1.d <<= 0x10
        v26_1:4.d <<= 0x10
        v26_1:8.d <<= 0x10
        v26_1:0xc.d <<= 0x10
        int128_t v28_1 = v0_1 + v28
        uint128_t v26_2 = vorrq_s8(v26_1, v31_1)
        int128_t v27_1 = v29_2 + v27
        uint128_t v31_2 = v28_1 ^ v11
        int128_t v25_2 = v30_2 + v25_1
        uint128_t v8_1 = v27_1 ^ v8
        v11.d = v31_2.d u>> 0x14
        v11:4.d = v31_2:4.d u>> 0x14
        v11:8.d = v31_2:8.d u>> 0x14
        v11:0xc.d = v31_2:0xc.d u>> 0x14
        v31_2.d <<= 0xc
        v31_2:4.d <<= 0xc
        v31_2:8.d <<= 0xc
        v31_2:0xc.d <<= 0xc
        int128_t v24_2 = v26_2 + v24_1
        uint128_t v9_1 = v25_2 ^ v9
        uint128_t v31_3 = vorrq_s8(v31_2, v11)
        v11.d = v8_1.d u>> 0x14
        v11:4.d = v8_1:4.d u>> 0x14
        v11:8.d = v8_1:8.d u>> 0x14
        v11:0xc.d = v8_1:0xc.d u>> 0x14
        v8_1.d <<= 0xc
        v8_1:4.d <<= 0xc
        v8_1:8.d <<= 0xc
        v8_1:0xc.d <<= 0xc
        uint128_t v10_1 = v24_2 ^ v10
        uint128_t v8_2 = vorrq_s8(v8_1, v11)
        v11.d = v9_1.d u>> 0x14
        v11:4.d = v9_1:4.d u>> 0x14
        v11:8.d = v9_1:8.d u>> 0x14
        v11:0xc.d = v9_1:0xc.d u>> 0x14
        v9_1.d <<= 0xc
        v9_1:4.d <<= 0xc
        v9_1:8.d <<= 0xc
        v9_1:0xc.d <<= 0xc
        uint128_t v9_2 = vorrq_s8(v9_1, v11)
        v11.d = v10_1.d u>> 0x14
        v11:4.d = v10_1:4.d u>> 0x14
        v11:8.d = v10_1:8.d u>> 0x14
        v11:0xc.d = v10_1:0xc.d u>> 0x14
        v10_1.d <<= 0xc
        v10_1:4.d <<= 0xc
        v10_1:8.d <<= 0xc
        v10_1:0xc.d <<= 0xc
        uint128_t v10_2 = vorrq_s8(v10_1, v11)
        int128_t v11_1 = v31_3 + v12_1
        uint128_t v12_2 = v8_2 + v13_1
        uint128_t v0_2 = v11_1 ^ v0_1
        int128_t v13_2 = v9_2 + v14_1
        uint128_t v14_2 = v10_2 + v15_1
        uint128_t v29_3 = v12_2 ^ v29_2
        v15_1.d = v0_2.d u>> 0x18
        v15_1:4.d = v0_2:4.d u>> 0x18
        v15_1:8.d = v0_2:8.d u>> 0x18
        v15_1:0xc.d = v0_2:0xc.d u>> 0x18
        v0_2.d <<= 8
        v0_2:4.d <<= 8
        v0_2:8.d <<= 8
        v0_2:0xc.d <<= 8
        uint128_t v30_3 = v13_2 ^ v30_2
        uint128_t v0_3 = vorrq_s8(v0_2, v15_1)
        v15_1.d = v29_3.d u>> 0x18
        v15_1:4.d = v29_3:4.d u>> 0x18
        v15_1:8.d = v29_3:8.d u>> 0x18
        v15_1:0xc.d = v29_3:0xc.d u>> 0x18
        v29_3.d <<= 8
        v29_3:4.d <<= 8
        v29_3:8.d <<= 8
        v29_3:0xc.d <<= 8
        uint128_t v26_3 = v14_2 ^ v26_2
        uint128_t v29_4 = vorrq_s8(v29_3, v15_1)
        v15_1.d = v30_3.d u>> 0x18
        v15_1:4.d = v30_3:4.d u>> 0x18
        v15_1:8.d = v30_3:8.d u>> 0x18
        v15_1:0xc.d = v30_3:0xc.d u>> 0x18
        v30_3.d <<= 8
        v30_3:4.d <<= 8
        v30_3:8.d <<= 8
        v30_3:0xc.d <<= 8
        int128_t v28_2 = v0_3 + v28_1
        uint128_t v30_4 = vorrq_s8(v30_3, v15_1)
        v15_1.d = v26_3.d u>> 0x18
        v15_1:4.d = v26_3:4.d u>> 0x18
        v15_1:8.d = v26_3:8.d u>> 0x18
        v15_1:0xc.d = v26_3:0xc.d u>> 0x18
        v26_3.d <<= 8
        v26_3:4.d <<= 8
        v26_3:8.d <<= 8
        v26_3:0xc.d <<= 8
        int128_t v27_2 = v29_4 + v27_1
        uint128_t v31_4 = v28_2 ^ v31_3
        uint128_t v26_4 = vorrq_s8(v26_3, v15_1)
        int128_t v25_3 = v30_4 + v25_2
        uint128_t v8_3 = v27_2 ^ v8_2
        v15_1.d = v31_4.d u>> 0x19
        v15_1:4.d = v31_4:4.d u>> 0x19
        v15_1:8.d = v31_4:8.d u>> 0x19
        v15_1:0xc.d = v31_4:0xc.d u>> 0x19
        v31_4.d <<= 7
        v31_4:4.d <<= 7
        v31_4:8.d <<= 7
        v31_4:0xc.d <<= 7
        int128_t v24_3 = v26_4 + v24_2
        uint128_t v9_3 = v25_3 ^ v9_2
        uint128_t v31_5 = vorrq_s8(v31_4, v15_1)
        v15_1.d = v8_3.d u>> 0x19
        v15_1:4.d = v8_3:4.d u>> 0x19
        v15_1:8.d = v8_3:8.d u>> 0x19
        v15_1:0xc.d = v8_3:0xc.d u>> 0x19
        v8_3.d <<= 7
        v8_3:4.d <<= 7
        v8_3:8.d <<= 7
        v8_3:0xc.d <<= 7
        uint128_t v10_3 = v24_3 ^ v10_2
        uint128_t v8_4 = vorrq_s8(v8_3, v15_1)
        v15_1.d = v9_3.d u>> 0x19
        v15_1:4.d = v9_3:4.d u>> 0x19
        v15_1:8.d = v9_3:8.d u>> 0x19
        v15_1:0xc.d = v9_3:0xc.d u>> 0x19
        v9_3.d <<= 7
        v9_3:4.d <<= 7
        v9_3:8.d <<= 7
        v9_3:0xc.d <<= 7
        uint128_t v9_4 = vorrq_s8(v9_3, v15_1)
        v15_1.d = v10_3.d u>> 0x19
        v15_1:4.d = v10_3:4.d u>> 0x19
        v15_1:8.d = v10_3:8.d u>> 0x19
        v15_1:0xc.d = v10_3:0xc.d u>> 0x19
        v10_3.d <<= 7
        v10_3:4.d <<= 7
        v10_3:8.d <<= 7
        v10_3:0xc.d <<= 7
        int128_t v11_2 = v8_4 + v11_1
        uint128_t v10_4 = vorrq_s8(v10_3, v15_1)
        uint128_t v15_2 = v9_4 + v12_2
        uint128_t v26_5 = v11_2 ^ v26_4
        int128_t v1_1 = v10_4 + v13_2
        uint128_t v0_4 = v15_2 ^ v0_3
        v12_2.d = v26_5.d u>> 0x10
        v12_2:4.d = v26_5:4.d u>> 0x10
        v12_2:8.d = v26_5:8.d u>> 0x10
        v12_2:0xc.d = v26_5:0xc.d u>> 0x10
        v26_5.d <<= 0x10
        v26_5:4.d <<= 0x10
        v26_5:8.d <<= 0x10
        v26_5:0xc.d <<= 0x10
        uint128_t v2_1 = v31_5 + v14_2
        uint128_t v29_5 = v1_1 ^ v29_4
        uint128_t v26_6 = vorrq_s8(v26_5, v12_2)
        v12_2.d = v0_4.d u>> 0x10
        v12_2:4.d = v0_4:4.d u>> 0x10
        v12_2:8.d = v0_4:8.d u>> 0x10
        v12_2:0xc.d = v0_4:0xc.d u>> 0x10
        v0_4.d <<= 0x10
        v0_4:4.d <<= 0x10
        v0_4:8.d <<= 0x10
        v0_4:0xc.d <<= 0x10
        uint128_t v30_5 = v2_1 ^ v30_4
        uint128_t v0_5 = vorrq_s8(v0_4, v12_2)
        v12_2.d = v29_5.d u>> 0x10
        v12_2:4.d = v29_5:4.d u>> 0x10
        v12_2:8.d = v29_5:8.d u>> 0x10
        v12_2:0xc.d = v29_5:0xc.d u>> 0x10
        v29_5.d <<= 0x10
        v29_5:4.d <<= 0x10
        v29_5:8.d <<= 0x10
        v29_5:0xc.d <<= 0x10
        int128_t v25_4 = v26_6 + v25_3
        uint128_t v29_6 = vorrq_s8(v29_5, v12_2)
        v12_2.d = v30_5.d u>> 0x10
        v12_2:4.d = v30_5:4.d u>> 0x10
        v12_2:8.d = v30_5:8.d u>> 0x10
        v12_2:0xc.d = v30_5:0xc.d u>> 0x10
        v30_5.d <<= 0x10
        v30_5:4.d <<= 0x10
        v30_5:8.d <<= 0x10
        v30_5:0xc.d <<= 0x10
        int128_t v24_4 = v0_5 + v24_3
        uint128_t v8_5 = v25_4 ^ v8_4
        uint128_t v30_6 = vorrq_s8(v30_5, v12_2)
        int128_t v28_3 = v29_6 + v28_2
        uint128_t v9_5 = v24_4 ^ v9_4
        v12_2.d = v8_5.d u>> 0x14
        v12_2:4.d = v8_5:4.d u>> 0x14
        v12_2:8.d = v8_5:8.d u>> 0x14
        v12_2:0xc.d = v8_5:0xc.d u>> 0x14
        v8_5.d <<= 0xc
        v8_5:4.d <<= 0xc
        v8_5:8.d <<= 0xc
        v8_5:0xc.d <<= 0xc
        int128_t v27_3 = v30_6 + v27_2
        uint128_t v10_5 = v28_3 ^ v10_4
        uint128_t v8_6 = vorrq_s8(v8_5, v12_2)
        v12_2.d = v9_5.d u>> 0x14
        v12_2:4.d = v9_5:4.d u>> 0x14
        v12_2:8.d = v9_5:8.d u>> 0x14
        v12_2:0xc.d = v9_5:0xc.d u>> 0x14
        v9_5.d <<= 0xc
        v9_5:4.d <<= 0xc
        v9_5:8.d <<= 0xc
        v9_5:0xc.d <<= 0xc
        uint128_t v31_6 = v27_3 ^ v31_5
        uint128_t v9_6 = vorrq_s8(v9_5, v12_2)
        v12_2.d = v10_5.d u>> 0x14
        v12_2:4.d = v10_5:4.d u>> 0x14
        v12_2:8.d = v10_5:8.d u>> 0x14
        v12_2:0xc.d = v10_5:0xc.d u>> 0x14
        v10_5.d <<= 0xc
        v10_5:4.d <<= 0xc
        v10_5:8.d <<= 0xc
        v10_5:0xc.d <<= 0xc
        uint128_t v10_6 = vorrq_s8(v10_5, v12_2)
        v12_2.d = v31_6.d u>> 0x14
        v12_2:4.d = v31_6:4.d u>> 0x14
        v12_2:8.d = v31_6:8.d u>> 0x14
        v12_2:0xc.d = v31_6:0xc.d u>> 0x14
        v31_6.d <<= 0xc
        v31_6:4.d <<= 0xc
        v31_6:8.d <<= 0xc
        v31_6:0xc.d <<= 0xc
        uint128_t v3_1 = vorrq_s8(v31_6, v12_2)
        v12 = v8_6 + v11_2
        v13 = v9_6 + v15_2
        v14 = v10_6 + v1_1
        uint128_t v1_2 = v12 ^ v26_6
        uint128_t v0_6 = v13 ^ v0_5
        v26_6.d = v1_2.d u>> 0x18
        v26_6:4.d = v1_2:4.d u>> 0x18
        v26_6:8.d = v1_2:8.d u>> 0x18
        v26_6:0xc.d = v1_2:0xc.d u>> 0x18
        v1_2.d <<= 8
        v1_2:4.d <<= 8
        v1_2:8.d <<= 8
        v1_2:0xc.d <<= 8
        v15 = v3_1 + v2_1
        int128_t v2_2 = v14 ^ v29_6
        v26 = vorrq_s8(v1_2, v26_6)
        v1_2.d = v0_6.d u>> 0x18
        v1_2:4.d = v0_6:4.d u>> 0x18
        v1_2:8.d = v0_6:8.d u>> 0x18
        v1_2:0xc.d = v0_6:0xc.d u>> 0x18
        v0_6.d <<= 8
        v0_6:4.d <<= 8
        v0_6:8.d <<= 8
        v0_6:0xc.d <<= 8
        uint128_t v31_7 = v15 ^ v30_6
        v29 = vorrq_s8(v0_6, v1_2)
        v0_6.d = v2_2.d u>> 0x18
        v0_6:4.d = v2_2:4.d u>> 0x18
        v0_6:8.d = v2_2:8.d u>> 0x18
        v0_6:0xc.d = v2_2:0xc.d u>> 0x18
        v1_2.d = v2_2.d << 8
        v1_2:4.d = v2_2:4.d << 8
        v1_2:8.d = v2_2:8.d << 8
        v1_2:0xc.d = v2_2:0xc.d << 8
        v30 = vorrq_s8(v1_2, v0_6)
        v0_6.d = v31_7.d u>> 0x18
        v0_6:4.d = v31_7:4.d u>> 0x18
        v0_6:8.d = v31_7:8.d u>> 0x18
        v0_6:0xc.d = v31_7:0xc.d u>> 0x18
        v1_2.d = v31_7.d << 8
        v1_2:4.d = v31_7:4.d << 8
        v1_2:8.d = v31_7:8.d << 8
        v1_2:0xc.d = v31_7:0xc.d << 8
        v25_1 = v26 + v25_4
        v31 = vorrq_s8(v1_2, v0_6)
        v24_1 = v29 + v24_4
        v0 = v25_1 ^ v8_6
        v28 = v30 + v28_3
        uint128_t v1_3 = v24_1 ^ v9_6
        v8_6.d = v0.d u>> 0x19
        v8_6:4.d = v0:4.d u>> 0x19
        v8_6:8.d = v0:8.d u>> 0x19
        v8_6:0xc.d = v0:0xc.d u>> 0x19
        v0.d <<= 7
        v0:4.d <<= 7
        v0:8.d <<= 7
        v0:0xc.d <<= 7
        v27 = v31 + v27_3
        int128_t v2_3 = v28 ^ v10_6
        v8 = vorrq_s8(v0, v8_6)
        v0.d = v1_3.d u>> 0x19
        v0:4.d = v1_3:4.d u>> 0x19
        v0:8.d = v1_3:8.d u>> 0x19
        v0:0xc.d = v1_3:0xc.d u>> 0x19
        v1_3.d <<= 7
        v1_3:4.d <<= 7
        v1_3:8.d <<= 7
        v1_3:0xc.d <<= 7
        int128_t v3_2 = v27 ^ v3_1
        v9 = vorrq_s8(v1_3, v0)
        v0.d = v2_3.d u>> 0x19
        v0:4.d = v2_3:4.d u>> 0x19
        v0:8.d = v2_3:8.d u>> 0x19
        v0:0xc.d = v2_3:0xc.d u>> 0x19
        v1_3.d = v2_3.d << 7
        v1_3:4.d = v2_3:4.d << 7
        v1_3:8.d = v2_3:8.d << 7
        v1_3:0xc.d = v2_3:0xc.d << 7
        v10 = vorrq_s8(v1_3, v0)
        v0.d = v3_2.d u>> 0x19
        v0:4.d = v3_2:4.d u>> 0x19
        v0:8.d = v3_2:8.d u>> 0x19
        v0:0xc.d = v3_2:0xc.d u>> 0x19
        v1_3.d = v3_2.d << 7
        v1_3:4.d = v3_2:4.d << 7
        v1_3:8.d = v3_2:8.d << 7
        v1_3:0xc.d = v3_2:0xc.d << 7
        i = i_1
        i_1 -= 1
        v11 = vorrq_s8(v1_3, v0)
    while (i != 1)

uint128_t v13_3 = v13 + v23
uint128_t v8_7 = v8 + v4
uint128_t v9_7 = v9 + v5
uint128_t v12_3 = v12 + var_80
uint128_t v10_7 = v10 + v6
uint128_t v4_1 = v27 + v16
uint128_t v6_1 = v25_1 + v17
uint128_t v14_3 = v14 + var_60
uint128_t v16_1 = vzip1q_s32(v12_3, v14_3)
uint128_t v2_4 = v28 + v7
uint128_t v7_1 = v24_1 + v18
uint128_t v15_3 = v15 + var_70
uint128_t v5_1 = v26 + v19
int32_t* x8_2 = arg2
uint128_t v17_1 = vzip1q_s32(v13_3, v15_3)
uint128_t v23_1 = v11 + var_90
uint128_t v18_1 = vzip1q_s32(v23_1, v9_7)
uint128_t v0_7 = v29 + v21_1
uint128_t v3_3 = v31 + v20
uint128_t v19_1 = vzip1q_s32(v8_7, v10_7)
*x8_2 = v16_1.d
x8_2[1] = v17_1.d
x8_2[2] = v16_1:4.d
x8_2[3] = v17_1:4.d
uint128_t v20_1 = vzip1q_s32(v2_4, v6_1)
uint128_t v1_4 = v30 + v22_1
x8_2[4] = v18_1.d
x8_2[5] = v19_1.d
x8_2[6] = v18_1:4.d
x8_2[7] = v19_1:4.d
uint128_t v21_2 = vzip1q_s32(v4_1, v7_1)
uint128_t v24_5 = vzip1q_s32(v0_7, v3_3)
arg2[8] = v20_1.d
arg2[9] = v21_2.d
arg2[0xa] = v20_1:4.d
arg2[0xb] = v21_2:4.d
uint128_t v25_5 = vzip1q_s32(v1_4, v5_1)
int64_t v26_7 = vextq_s8(v16_1, v16_1, 8)
arg2[0xc] = v24_5.d
arg2[0xd] = v25_5.d
arg2[0xe] = v24_5:4.d
arg2[0xf] = v25_5:4.d
int64_t v27_4 = vextq_s8(v17_1, v17_1, 8)
int64_t v16_2 = vextq_s8(v18_1, v18_1, 8)
arg2[0x10] = v26_7.d
arg2[0x11] = v27_4.d
arg2[0x12] = v26_7:4.d
arg2[0x13] = v27_4:4.d
int64_t v17_2 = vextq_s8(v19_1, v19_1, 8)
arg2[0x14] = v16_2.d
arg2[0x15] = v17_2.d
arg2[0x16] = v16_2:4.d
arg2[0x17] = v17_2:4.d
int64_t v16_3 = vextq_s8(v20_1, v20_1, 8)
int64_t v17_3 = vextq_s8(v21_2, v21_2, 8)
arg2[0x18] = v16_3.d
arg2[0x19] = v17_3.d
arg2[0x1a] = v16_3:4.d
arg2[0x1b] = v17_3:4.d
int64_t v16_4 = vextq_s8(v24_5, v24_5, 8)
int64_t v17_4 = vextq_s8(v25_5, v25_5, 8)
arg2[0x1c] = v16_4.d
arg2[0x1d] = v17_4.d
arg2[0x1e] = v16_4:4.d
arg2[0x1f] = v17_4:4.d
uint128_t v16_5 = vzip2q_s32(v12_3, v14_3)
uint128_t v17_5 = vzip2q_s32(v13_3, v15_3)
uint128_t v18_2 = vzip2q_s32(v23_1, v9_7)
arg2[0x20] = v16_5.d
arg2[0x21] = v17_5.d
arg2[0x22] = v16_5:4.d
arg2[0x23] = v17_5:4.d
uint128_t v19_2 = vzip2q_s32(v8_7, v10_7)
uint128_t v20_2 = vzip2q_s32(v2_4, v6_1)
arg2[0x24] = v18_2.d
arg2[0x25] = v19_2.d
arg2[0x26] = v18_2:4.d
arg2[0x27] = v19_2:4.d
uint128_t v21_3 = vzip2q_s32(v4_1, v7_1)
uint128_t v2_5 = vzip2q_s32(v0_7, v3_3)
arg2[0x28] = v20_2.d
arg2[0x29] = v21_3.d
arg2[0x2a] = v20_2:4.d
arg2[0x2b] = v21_3:4.d
uint128_t v3_4 = vzip2q_s32(v1_4, v5_1)
int64_t v0_8 = vextq_s8(v16_5, v16_5, 8)
arg2[0x2c] = v2_5.d
arg2[0x2d] = v3_4.d
arg2[0x2e] = v2_5:4.d
arg2[0x2f] = v3_4:4.d
int64_t v1_5 = vextq_s8(v17_5, v17_5, 8)
arg2[0x30] = v0_8.d
arg2[0x31] = v1_5.d
arg2[0x32] = v0_8:4.d
arg2[0x33] = v1_5:4.d
int64_t v0_9 = vextq_s8(v18_2, v18_2, 8)
int64_t v1_6 = vextq_s8(v19_2, v19_2, 8)
arg2[0x34] = v0_9.d
arg2[0x35] = v1_6.d
arg2[0x36] = v0_9:4.d
arg2[0x37] = v1_6:4.d
int64_t v0_10 = vextq_s8(v20_2, v20_2, 8)
int64_t v4_2 = vextq_s8(v2_5, v2_5, 8)
int64_t v1_7 = vextq_s8(v21_3, v21_3, 8)
int64_t v5_2 = vextq_s8(v3_4, v3_4, 8)
arg2[0x38] = v0_10.d
arg2[0x39] = v1_7.d
arg2[0x3a] = v0_10:4.d
arg2[0x3b] = v1_7:4.d
arg2[0x3c] = v4_2.d
arg2[0x3d] = v5_2.d
arg2[0x3e] = v4_2:4.d
arg2[0x3f] = v5_2:4.d
int32_t x8_17 = *(arg3 + 0x30)
*(arg3 + 0x30) = x8_17 + 4

if (x8_17 != 0xfffffffb && x8_17 u>= 0xfffffffb)
    *(arg3 + 0x34) += 1
